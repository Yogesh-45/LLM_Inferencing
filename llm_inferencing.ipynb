{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7s8_4Hsdx7vB",
        "outputId": "1cfc3866-785e-4e25-cb54-e604d8761225"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'llama.cpp' already exists and is not an empty directory.\n",
            "/content/llama.cpp\n"
          ]
        }
      ],
      "source": [
        "#@title Clone the llama.cpp repository\n",
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "%cd llama.cpp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yx25xfbhHSJG",
        "outputId": "4708bfd2-ef61-46f9-d31e-07ce5f7eabbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'ggify'...\n",
            "remote: Enumerating objects: 67, done.\u001b[K\n",
            "remote: Counting objects: 100% (67/67), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 67 (delta 34), reused 54 (delta 25), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (67/67), 12.89 KiB | 6.44 MiB/s, done.\n",
            "Resolving deltas: 100% (34/34), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/akx/ggify.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOBbIMFZyG5O",
        "outputId": "4cafbed9-f45e-45ab-bb11-f23b90dfa58b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.125.190.83)] [1 InRele\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.125.190.83)] [Connecte\r                                                                                                    \rGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.83)] [Waiting for headers] [Connected to r2u.stat.\r                                                                                                    \rGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,172 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,501 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,616 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,452 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,323 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,223 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,512 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,734 kB]\n",
            "Fetched 23.9 MB in 3s (8,540 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "#@title Install cmake and build tools\n",
        "!apt-get update && apt-get install -y build-essential cmake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ddjlEH2yJ8n",
        "outputId": "75144bc6-922d-475c-91aa-1c5a1240f3d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP: TRUE (found version \"4.5\")\n",
            "-- OpenMP found\n",
            "-- Using llamafile\n",
            "-- x86 detected\n",
            "-- Using runtime weight conversion of Q4_0 to Q4_0_x_x to enable optimized GEMM/GEMV kernels\n",
            "-- Including CPU backend\n",
            "-- Using AMX\n",
            "-- Including AMX backend\n",
            "-- Configuring done (2.0s)\n",
            "-- Generating done (0.3s)\n",
            "-- Build files have been written to: /content/llama.cpp\n",
            "[  1%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-aarch64.c.o\u001b[0m\n",
            "[  5%] \u001b[32m\u001b[1mLinking CXX shared library libggml-base.so\u001b[0m\n",
            "[  5%] Built target ggml-base\n",
            "[  5%] \u001b[32mBuilding CXX object ggml/src/ggml-amx/CMakeFiles/ggml-amx.dir/ggml-amx.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object ggml/src/ggml-amx/CMakeFiles/ggml-amx.dir/mmq.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32m\u001b[1mLinking CXX shared library libggml-amx.so\u001b[0m\n",
            "[  6%] Built target ggml-amx\n",
            "[  6%] \u001b[32mBuilding C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu.c.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu-aarch64.c.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu-quants.c.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/llamafile/sgemm.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32m\u001b[1mLinking CXX shared library libggml-cpu.so\u001b[0m\n",
            "[  9%] Built target ggml-cpu\n",
            "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32m\u001b[1mLinking CXX shared library libggml.so\u001b[0m\n",
            "[ 10%] Built target ggml\n",
            "[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32m\u001b[1mLinking CXX shared library libllama.so\u001b[0m\n",
            "[ 14%] Built target llama\n",
            "[ 14%] \u001b[34m\u001b[1mGenerating build details from Git\u001b[0m\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "[ 15%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
            "[ 15%] Built target build_info\n",
            "[ 15%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
            "[ 20%] Built target common\n",
            "[ 20%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\n",
            "[ 21%] Built target test-tokenizer-0\n",
            "[ 21%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n",
            "[ 22%] Built target test-tokenizer-1-bpe\n",
            "[ 22%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-spm\u001b[0m\n",
            "[ 23%] Built target test-tokenizer-1-spm\n",
            "[ 23%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-log\u001b[0m\n",
            "[ 24%] Built target test-log\n",
            "[ 24%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-arg-parser\u001b[0m\n",
            "[ 25%] Built target test-arg-parser\n",
            "[ 26%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
            "[ 27%] Built target test-quantize-fns\n",
            "[ 27%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
            "[ 28%] Built target test-quantize-perf\n",
            "[ 29%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
            "[ 30%] Built target test-sampling\n",
            "[ 31%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-template\u001b[0m\n",
            "[ 32%] Built target test-chat-template\n",
            "[ 32%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n",
            "[ 33%] Built target test-grammar-parser\n",
            "[ 34%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-integration\u001b[0m\n",
            "[ 35%] Built target test-grammar-integration\n",
            "[ 36%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n",
            "[ 37%] Built target test-llama-grammar\n",
            "[ 37%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-barrier\u001b[0m\n",
            "[ 38%] Built target test-barrier\n",
            "[ 38%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n",
            "[ 40%] Built target test-backend-ops\n",
            "[ 41%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n",
            "[ 42%] Built target test-rope\n",
            "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\n",
            "[ 44%] Built target test-model-load-cancel\n",
            "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-autorelease\u001b[0m\n",
            "[ 46%] Built target test-autorelease\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-schema-to-grammar\u001b[0m\n",
            "[ 48%] Built target test-json-schema-to-grammar\n",
            "[ 49%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n",
            "[ 49%] \u001b[32m\u001b[1mLinking C executable ../bin/test-c\u001b[0m\n",
            "[ 49%] Built target test-c\n",
            "[ 49%] \u001b[32mBuilding CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cvector-generator\u001b[0m\n",
            "[ 50%] Built target llama-cvector-generator\n",
            "[ 51%] \u001b[32mBuilding CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched-bench\u001b[0m\n",
            "[ 51%] Built target llama-batched-bench\n",
            "[ 52%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched\u001b[0m\n",
            "[ 52%] Built target llama-batched\n",
            "[ 52%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-convert-llama2c-to-ggml\u001b[0m\n",
            "[ 53%] Built target llama-convert-llama2c-to-ggml\n",
            "[ 53%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-embedding\u001b[0m\n",
            "[ 54%] Built target llama-embedding\n",
            "[ 54%] \u001b[32mBuilding CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-eval-callback\u001b[0m\n",
            "[ 55%] Built target llama-eval-callback\n",
            "[ 56%] \u001b[32mBuilding CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-export-lora\u001b[0m\n",
            "[ 56%] Built target llama-export-lora\n",
            "[ 57%] \u001b[32mBuilding CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gbnf-validator\u001b[0m\n",
            "[ 57%] Built target llama-gbnf-validator\n",
            "[ 58%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\u001b[0m\n",
            "[ 58%] Built target sha256\n",
            "[ 59%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\u001b[0m\n",
            "[ 59%] Built target xxhash\n",
            "[ 60%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\u001b[0m\n",
            "[ 60%] Built target sha1\n",
            "[ 61%] \u001b[32mBuilding CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-hash\u001b[0m\n",
            "[ 61%] Built target llama-gguf-hash\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-split\u001b[0m\n",
            "[ 63%] Built target llama-gguf-split\n",
            "[ 64%] \u001b[32mBuilding CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf\u001b[0m\n",
            "[ 64%] Built target llama-gguf\n",
            "[ 64%] \u001b[32mBuilding CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gritlm\u001b[0m\n",
            "[ 65%] Built target llama-gritlm\n",
            "[ 65%] \u001b[32mBuilding CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-imatrix\u001b[0m\n",
            "[ 66%] Built target llama-imatrix\n",
            "[ 66%] \u001b[32mBuilding CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-infill\u001b[0m\n",
            "[ 67%] Built target llama-infill\n",
            "[ 68%] \u001b[32mBuilding CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n",
            "[ 68%] Built target llama-bench\n",
            "[ 69%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o\u001b[0m\n",
            "[ 69%] Built target llava\n",
            "[ 69%] \u001b[32m\u001b[1mLinking CXX static library libllava_static.a\u001b[0m\n",
            "[ 69%] Built target llava_static\n",
            "[ 70%] \u001b[32m\u001b[1mLinking CXX shared library libllava_shared.so\u001b[0m\n",
            "[ 70%] Built target llava_shared\n",
            "[ 70%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-cli\u001b[0m\n",
            "[ 71%] Built target llama-llava-cli\n",
            "[ 71%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-minicpmv-cli\u001b[0m\n",
            "[ 72%] Built target llama-minicpmv-cli\n",
            "[ 73%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookahead\u001b[0m\n",
            "[ 73%] Built target llama-lookahead\n",
            "[ 74%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup\u001b[0m\n",
            "[ 74%] Built target llama-lookup\n",
            "[ 75%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-create\u001b[0m\n",
            "[ 75%] Built target llama-lookup-create\n",
            "[ 76%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-merge\u001b[0m\n",
            "[ 76%] Built target llama-lookup-merge\n",
            "[ 77%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-stats\u001b[0m\n",
            "[ 78%] Built target llama-lookup-stats\n",
            "[ 79%] \u001b[32mBuilding CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cli\u001b[0m\n",
            "[ 80%] Built target llama-cli\n",
            "[ 80%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-parallel\u001b[0m\n",
            "[ 81%] Built target llama-parallel\n",
            "[ 81%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-passkey\u001b[0m\n",
            "[ 82%] Built target llama-passkey\n",
            "[ 82%] \u001b[32mBuilding CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-perplexity\u001b[0m\n",
            "[ 83%] Built target llama-perplexity\n",
            "[ 84%] \u001b[32mBuilding CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize-stats\u001b[0m\n",
            "[ 84%] Built target llama-quantize-stats\n",
            "[ 85%] \u001b[32mBuilding CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n",
            "[ 85%] Built target llama-quantize\n",
            "[ 86%] \u001b[32mBuilding CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-retrieval\u001b[0m\n",
            "[ 86%] Built target llama-retrieval\n",
            "[ 86%] \u001b[34m\u001b[1mGenerating loading.html.hpp\u001b[0m\n",
            "[ 87%] \u001b[34m\u001b[1mGenerating completion.js.hpp\u001b[0m\n",
            "[ 88%] \u001b[34m\u001b[1mGenerating deps_daisyui.min.css.hpp\u001b[0m\n",
            "[ 88%] \u001b[34m\u001b[1mGenerating deps_markdown-it.js.hpp\u001b[0m\n",
            "[ 89%] \u001b[34m\u001b[1mGenerating deps_tailwindcss.js.hpp\u001b[0m\n",
            "[ 89%] \u001b[34m\u001b[1mGenerating deps_vue.esm-browser.js.hpp\u001b[0m\n",
            "[ 89%] \u001b[34m\u001b[1mGenerating index.html.hpp\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-server\u001b[0m\n",
            "[ 91%] Built target llama-server\n",
            "[ 92%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-save-load-state\u001b[0m\n",
            "[ 93%] Built target llama-save-load-state\n",
            "[ 93%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple\u001b[0m\n",
            "[ 94%] Built target llama-simple\n",
            "[ 94%] \u001b[32mBuilding CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple-chat\u001b[0m\n",
            "[ 95%] Built target llama-simple-chat\n",
            "[ 95%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative\u001b[0m\n",
            "[ 96%] Built target llama-speculative\n",
            "[ 96%] \u001b[32mBuilding CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative-simple\u001b[0m\n",
            "[ 97%] Built target llama-speculative-simple\n",
            "[ 98%] \u001b[32mBuilding CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tokenize\u001b[0m\n",
            "[ 98%] Built target llama-tokenize\n",
            "[ 99%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o\u001b[0m\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-vdot\u001b[0m\n",
            "[ 99%] Built target llama-vdot\n",
            "[100%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-q8dot\u001b[0m\n",
            "[100%] Built target llama-q8dot\n"
          ]
        }
      ],
      "source": [
        "#@title Build llama.cpp\n",
        "!cmake . && make"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQfDJJKAr0CX",
        "outputId": "d7c11b7f-e1e4-4e37-8561-3e522e4c180b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "d9adcfec44c0496bb0c0faef565191b6",
            "078b5b0c593a409199f8cd2e29d7b873",
            "ad4e9b9a4171462cb46ffe7eeb5e0e79",
            "140f5e3ae1244f7a8feb04a190aad5eb",
            "18ee58a9fbed479a968d2eeaf83d6807",
            "a9cd2d3d705a48378c44e70cfaa6d824",
            "f5f4814728ac46779cec4ada224fdb28",
            "ae975cf9f0954c43baa477d9957e65fa",
            "88cb0b3a67d24c5686646f384a54bd1b",
            "db5a75a4f0f043c3b041272bc6c14a56",
            "a60c1fd202cc4d7ba89c526b9a1c3a06"
          ]
        },
        "id": "lGzv8pteCUDk",
        "outputId": "09e9265d-0ff3-42b6-9e40-06544d70df49"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d9adcfec44c0496bb0c0faef565191b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('vicuna_original/tokenizer_config.json',\n",
              " 'vicuna_original/special_tokens_map.json',\n",
              " 'vicuna_original/tokenizer.model',\n",
              " 'vicuna_original/added_tokens.json',\n",
              " 'vicuna_original/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "#@title loading vicuna-7b-v1.3 model from hugging-face & saving it\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Load the tokenizer and model\n",
        "model_name = \"lmsys/vicuna-7b-v1.3\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
        "\n",
        "# Save as GGML format using llama.cpp compatibility\n",
        "model.save_pretrained(\"vicuna_original\")\n",
        "tokenizer.save_pretrained(\"vicuna_original\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWflMryQxIXY"
      },
      "outputs": [],
      "source": [
        "!mkdir quantized_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHgzX04cHoAs",
        "outputId": "18e16693-5880-492e-ad18-9ff5873d9232"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:hf-to-gguf:Loading model: vicuna_original\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00003.safetensors'\n",
            "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> F16, shape = {4096, 32000}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00003.safetensors'\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00003.safetensors'\n",
            "INFO:hf-to-gguf:output.weight,               torch.float16 --> F16, shape = {4096, 32000}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 2048\n",
            "INFO:hf-to-gguf:gguf: embedding length = 4096\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 11008\n",
            "INFO:hf-to-gguf:gguf: head count = 32\n",
            "INFO:hf-to-gguf:gguf: key-value head count = 32\n",
            "INFO:hf-to-gguf:gguf: rope theta = 10000.0\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\n",
            "INFO:hf-to-gguf:gguf: file type = 1\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:gguf.vocab:Setting special token type bos to 1\n",
            "INFO:gguf.vocab:Setting special token type eos to 2\n",
            "INFO:gguf.vocab:Setting special token type unk to 0\n",
            "INFO:gguf.vocab:Setting special token type pad to 0\n",
            "INFO:gguf.vocab:Setting add_bos_token to True\n",
            "INFO:gguf.vocab:Setting add_eos_token to False\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:/content/llama.cpp/quantized_model/vicuna-7B-v1.3-F16.gguf: n_tensors = 291, total_size = 13.5G\n",
            "Writing: 100% 13.5G/13.5G [02:34<00:00, 87.1Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to /content/llama.cpp/quantized_model/vicuna-7B-v1.3-F16.gguf\n"
          ]
        }
      ],
      "source": [
        "#@title converting hugging-face model to gguf format\n",
        "!python3 convert_hf_to_gguf.py /content/llama.cpp/vicuna_original --model-name vicuna --outfile /content/llama.cpp/quantized_model/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title quantizing gguf fp-16 model to fp-4\n",
        "!/content/llama.cpp/bin/llama-quantize /content/llama.cpp/quantized_model/vicuna-7B-v1.3-F16.gguf /content/llama.cpp/quantized_model/vicuna-7B_Q4_K_M.gguf Q4_K_M"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1JdOsxWcOcO",
        "outputId": "52655705-78b3-4682-ee2f-31661c581a57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main: build = 4162 (f6d12e7d)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/llama.cpp/quantized_model/vicuna-7B-v1.3-F16.gguf' to '/content/llama.cpp/quantized_model/vicuna-7B_Q4_K_M.gguf' as Q4_K_M\n",
            "llama_model_loader: loaded meta data with 33 key-value pairs and 291 tensors from /content/llama.cpp/quantized_model/vicuna-7B-v1.3-F16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = vicuna\n",
            "llama_model_loader: - kv   3:                            general.version str              = v1.3\n",
            "llama_model_loader: - kv   4:                       general.organization str              = Lmsys\n",
            "llama_model_loader: - kv   5:                           general.basename str              = vicuna\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 7B\n",
            "llama_model_loader: - kv   7:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   8:                       llama.context_length u32              = 2048\n",
            "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  31:            tokenizer.ggml.add_space_prefix bool             = true\n",
            "llama_model_loader: - kv  32:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type  f16:  226 tensors\n",
            "[   1/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q6_K .. size =   250.00 MiB ->   102.54 MiB\n",
            "[   2/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   3/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q4_K .. size =   250.00 MiB ->    70.31 MiB\n",
            "[   4/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[   5/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   6/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[   7/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[   8/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[   9/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[  10/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  11/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  12/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  13/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  14/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  15/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  16/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  17/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  18/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[  19/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  20/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  21/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  22/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  23/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  24/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  25/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  26/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  27/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[  28/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  29/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  30/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  31/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  32/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  33/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  34/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  35/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  36/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[  37/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  38/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  39/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  40/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  41/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  42/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  43/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  44/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  45/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  46/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  47/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  48/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  49/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  50/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  51/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  52/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  53/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  54/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  55/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  56/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  57/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  58/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  59/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  60/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  61/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  62/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  63/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[  64/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  65/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  66/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  67/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  68/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  69/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  70/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  71/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  72/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  73/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  74/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  75/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  76/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  77/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  78/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  79/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  80/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  81/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  82/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  83/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  84/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  85/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  86/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  87/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  88/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  89/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  90/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[  91/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  92/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  93/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  94/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  95/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  96/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  97/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  98/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  99/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 100/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 101/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 102/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 103/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 104/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 105/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 106/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 107/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 108/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 109/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 110/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 111/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 112/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 113/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 114/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 115/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 116/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 117/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[ 118/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 119/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 120/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 121/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 122/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 123/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 124/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 125/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 126/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 127/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 128/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 129/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 130/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 131/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 132/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 133/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 134/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 135/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 136/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 137/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 138/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 139/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 140/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 141/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 142/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 143/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 144/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[ 145/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 146/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 147/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 148/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 149/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 150/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 151/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 152/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 153/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 154/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 155/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 156/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 157/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 158/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 159/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 160/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 161/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 162/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 163/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 164/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 165/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 166/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 167/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 168/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 169/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 170/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 171/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[ 172/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 173/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 174/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 175/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 176/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 177/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 178/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 179/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 180/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 181/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 182/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 183/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 184/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 185/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 186/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 187/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 188/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 189/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 190/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 191/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 192/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 193/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 194/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 195/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 196/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 197/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 198/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[ 199/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 200/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 201/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 202/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 203/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 204/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 205/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 206/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 207/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 208/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 209/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 210/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 211/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 212/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 213/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 214/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 215/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 216/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 217/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 218/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 219/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 220/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 221/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 222/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 223/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 224/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 225/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[ 226/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 227/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 228/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 229/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 230/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 231/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 232/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 233/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 234/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 235/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 236/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 237/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 238/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 239/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 240/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 241/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 242/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 243/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 244/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 245/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 246/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 247/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 248/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 249/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 250/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 251/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 252/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[ 253/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 254/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 255/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 256/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 257/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 258/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 259/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 260/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 261/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[ 262/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 263/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 264/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 265/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 266/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 267/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 268/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 269/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 270/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[ 271/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 272/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 273/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 274/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 275/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 276/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 277/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 278/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 279/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[ 280/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 281/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 282/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 283/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 284/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 285/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 286/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 287/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 288/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[ 289/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 290/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 291/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "llama_model_quantize_internal: model size  = 12853.02 MB\n",
            "llama_model_quantize_internal: quant size  =  3891.24 MB\n",
            "\n",
            "main: quantize time = 931372.68 ms\n",
            "main:    total time = 931372.68 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YRrGDksDrrc",
        "outputId": "13d04403-10a2-4c2e-80fc-bc5fdd8c47ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build: 4162 (f6d12e7d) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: llama backend init\n",
            "main: load the model and apply lora adapter, if any\n",
            "llama_model_loader: loaded meta data with 33 key-value pairs and 291 tensors from /content/llama.cpp/quantized_model/vicuna-7B_Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = vicuna\n",
            "llama_model_loader: - kv   3:                            general.version str              = v1.3\n",
            "llama_model_loader: - kv   4:                       general.organization str              = Lmsys\n",
            "llama_model_loader: - kv   5:                           general.basename str              = vicuna\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 7B\n",
            "llama_model_loader: - kv   7:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   8:                       llama.context_length u32              = 2048\n",
            "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  31:            tokenizer.ggml.add_space_prefix bool             = true\n",
            "llama_model_loader: - kv  32:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "llm_load_vocab: special tokens cache size = 3\n",
            "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 2048\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
            "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 11008\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 2048\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 6.74 B\n",
            "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
            "llm_load_print_meta: general.name     = vicuna\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_print_meta: EOG token        = 2 '</s>'\n",
            "llm_load_print_meta: max token length = 48\n",
            "llm_load_tensors:   CPU_Mapped model buffer size =  3891.24 MiB\n",
            "..................................................................................................\n",
            "llama_new_context_with_model: n_seq_max     = 1\n",
            "llama_new_context_with_model: n_ctx         = 4096\n",
            "llama_new_context_with_model: n_ctx_per_seq = 4096\n",
            "llama_new_context_with_model: n_batch       = 2048\n",
            "llama_new_context_with_model: n_ubatch      = 512\n",
            "llama_new_context_with_model: flash_attn    = 0\n",
            "llama_new_context_with_model: freq_base     = 10000.0\n",
            "llama_new_context_with_model: freq_scale    = 1\n",
            "llama_new_context_with_model: n_ctx_pre_seq (4096) > n_ctx_train (2048) -- possible training context overflow\n",
            "llama_kv_cache_init:        CPU KV buffer size =  2048.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
            "main: llama threadpool init, n_threads = 1\n",
            "main: model was trained on only 2048 context tokens (4096 specified)\n",
            "\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 2 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "\n",
            "sampler seed: 438112237\n",
            "sampler params: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1\n",
            "\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \n",
            "generate: n_ctx = 4096, n_batch = 2048, n_predict = 128, n_keep = 1\n",
            "\n",
            " I believe the meaning of life is to fulfill your purpose. And your purpose is to fulfill the will of God.\n",
            "In Matthew 7:21, Jesus says, Not everyone who says to Me, Lord, Lord, will enter the kingdom of heaven, but he who does the will of My Father who is in heaven will enter.\n",
            "Your purpose is not found in what you do, but in who you are. It is not found in what you have, but in what you have been given. And it is not found in what you do, but in what you do for others.\n",
            "You were created to be a child of\n",
            "\n",
            "llama_perf_sampler_print:    sampling time =       7.08 ms /   136 runs   (    0.05 ms per token, 19211.75 tokens per second)\n",
            "llama_perf_context_print:        load time =   10068.73 ms\n",
            "llama_perf_context_print: prompt eval time =    3523.90 ms /     8 tokens (  440.49 ms per token,     2.27 tokens per second)\n",
            "llama_perf_context_print:        eval time =   92530.44 ms /   127 runs   (  728.59 ms per token,     1.37 tokens per second)\n",
            "llama_perf_context_print:       total time =   96091.35 ms /   135 tokens\n"
          ]
        }
      ],
      "source": [
        "#@title inferencing of the quantized model\n",
        "!/content/llama.cpp/bin/llama-cli -m /content/llama.cpp/quantized_model/vicuna-7B_Q4_K_M.gguf -p \"I believe the meaning of life is\" -n 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzkdYGdVCTy3"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzdXi07MX-ki"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25AOXePHlAIO"
      },
      "outputs": [],
      "source": [
        "# Initialize the custom model\n",
        "base_model_name = \"lmsys/vicuna-7b-v1.3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168,
          "referenced_widgets": [
            "ba9d9445939549edad0b966b0134a2eb",
            "5a0e293ae81e4cc396cae305e03da53d",
            "062c4b9e0b0e44b7b641a56f0a25511f",
            "0810e015fded44d8a658e758f58660c5",
            "e67ef75d4db94903925369798b31ef19",
            "61b021d7287c4b18997d345c1f6f21d4",
            "8c5e75898c8e47789244733bcd542184",
            "237f220a84674a5098c59cbf37ec8051",
            "48836409a0cc4b9a98b04e50fd681b4a",
            "27598e42d9c5435a85e60a5a09e4600c",
            "78b0c915d4f9401d9137e6009570432d",
            "5ee928322e3940908e31e40893ebd669",
            "67221f3660bd4b418deb0cbcfe224dd1",
            "61a6b4e762b3446c81362b8f5b64f087",
            "40b050919c2748f4b4e99e185130d475",
            "1ab9d9ec535145779a1c7227ddeba0fe",
            "eee423fb7b5c49d68a83190e080300ed",
            "bf1f783678054990b126dd23904ce12b",
            "2b878d4cfa514cba919a816479ceccca",
            "0ce6a77ac385450e868b6d2acb49fe12",
            "4d7f19988e354e0ea5452e160ee168de",
            "2b08a26f935443778276b96c3139f767",
            "af5975bc529d4fc2a3b653dd163675a7",
            "bf3a9ca4adeb46c8ba71f14566be638f",
            "9de5a1c92448493f94dfa3297096b5ee",
            "0caf8032f77e40c09446a9c286209984",
            "145218bc4cb248a2ba7eb74d2cb653aa",
            "090c5fcfd66c441c87cd7c87e7fca1b8",
            "549b574902e64d46a214bbc9498e987b",
            "af35a094d69e45c1ba8018e8247408fb",
            "2649c4029b764eb6b2c9fac6216e0b00",
            "308126d94173419a908df9a07547583f",
            "576f8640862e4d11bb70898c7ed48511"
          ]
        },
        "id": "uR5UqA4w4t5-",
        "outputId": "6eb92854-5dd9-4b6c-8f9b-1547c14b4db6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba9d9445939549edad0b966b0134a2eb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ee928322e3940908e31e40893ebd669",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af5975bc529d4fc2a3b653dd163675a7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241,
          "referenced_widgets": [
            "617da13d7cbf48578c92a3713bec6729",
            "aa8281c267de4d61996e28163729279d",
            "c6057f63294349ddb02cbd529b03dafd",
            "cad96e9bd8934f53bfa889951c9bc424",
            "515d1e5f1adf44c8a8c8f9b75e218a44",
            "27601138e49e42258cf46d430e8f62ba",
            "3adc6c676f1744659b0cd12df1912bbb",
            "ce12870d1132474baeeaf4b996e532b4",
            "6624bc1fec094e90b1c2dc2bda4953ce",
            "a917b4272f164a608940bc40a1fa6c5e",
            "333cb64bdc64429cafa80b8f4a8f0f04",
            "feddde061f6e4444bd480c0f4379b8bb",
            "381c7d3eeac74234823ab424003e1b7c",
            "d2ee8fc079a94e4dbff6e07a7c2adf60",
            "120eb0ec30df4ae3a5907ab5b903cf36",
            "a10c93d91da741b3a17ef3a354e75e44",
            "4bf9068d17ce4b198c26f740afda9b77",
            "feea812abca34574991da2ab9f48b0f1",
            "a64e50d2fb844e9d834c4445c3e8628f",
            "a040492de74d432e922ef534da4dcf73",
            "d1a29a6452d54d5e9af8afd2bdd1c8ec",
            "1b5125d083714f9b87d8835490261cd5",
            "6528386896dc4f9bb0b079cb54349f2f",
            "1dd69b66c3b94bf68baca39342849915",
            "1fbe8adced5f453ba34097769a7390c4",
            "3605c399a39342ae9268279e26cac924",
            "7e4ea410128d49fca59e1a6bbcd72713",
            "d21d2300ab7c4967a22ba945d1408f3e",
            "8e6619d407914d5e980ea78e8ccf140b",
            "f8ee854311844698b0f00a08c421af78",
            "9de84eba817446c9ac3a5a87d3770c11",
            "6a195136c21c4f1cb767328ecc5fe6f5",
            "6bbcc022b1fe486491d89cec64a7dff5",
            "be393ebf21204cba8984d8440dd2c278",
            "0ec43020a81d44a2ae48b9a33f18cb4d",
            "d70befb903544a8595d518453f90f37e",
            "fff7d5783fcf452e9c3d52340eda7837",
            "05916bcd5e124ca1ac77a9656bbcbdbf",
            "65dfaa007eea4f27ae4880a00135fee6",
            "1ee7836b82aa4f3788eaa62d9d334312",
            "868194f48b7b4cbc9ee3e7eaef7cc147",
            "9843dfa7b5e7490984f2651ef5e75089",
            "6ce835a154314fbe994f74c94622b07e",
            "22e950a7254f49c79ca370aab8a55952",
            "3d4efc726a9849398a359f5a50b56939",
            "4774e70eab0a4afeb1d3374238e7443d",
            "a33179dd44d94803ae0ca4e42cb7c27f",
            "4068dd95d0434c83bc90f19622a9a250",
            "57c2ed7f03c4471fbb09d7de54d6e1b0",
            "c2fcbae0f9c44b71bb7bd90bc68fa236",
            "dc5557a5e060491093ab28ec5a4e798e",
            "8838563bbeb94aee94ee61354d4b1250",
            "9833fa4ca8364dada389fd8ef3928b5b",
            "779cdc03441a492f9b8e5ac23edcc556",
            "f7c522bd85ab43b39fb81f122c616f10",
            "7df7616123e6410283f641b713014beb",
            "77d165540dad40bfb9b4cb9dbb39d0c5",
            "83c145bfb3be418d81297b7c662f291d",
            "0b20f818873847f994226ada22639acd",
            "9fa3b81e61b04a82895d045693c73b58",
            "28dc1388ad6541088b5619dc4e0c42c1",
            "562160f2776642d5a7f77888c5b2ca57",
            "06b94e77ea3c4e06975de15051bcc0d7",
            "1f6b1aa67142475ea4789da20ee9973c",
            "41968ef504fe4ffdabc08f90df8c1b77",
            "8350886080994024b04008e14e8da30b",
            "e637082f7a8a482383a4180277a69e35",
            "9a407a9a6ab743c1b7c53042868fbb2f",
            "89e03b7e3705408ea157b8d4a8d4304d",
            "c0f0ad400f9d4593b3569d3498e650ad",
            "0232f8cfad6a4751a89b83e0788e2372",
            "0612c8ea7cd54dab8d440cd5b55e1bb2",
            "87a29ca98b664d678a989c795f86f895",
            "a96ac47828b54fa4a1d0e99782fe4475",
            "464024664de140c882fb81e239650848",
            "6f096c7e4091448ca0af5d0ae4b2a960",
            "22bf260c22414b6f852f611b9ecdcbc0"
          ]
        },
        "id": "k8BU-7R7lEpJ",
        "outputId": "a9798bcb-2d93-4ba5-c97f-edc631e6087d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "617da13d7cbf48578c92a3713bec6729",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/566 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "feddde061f6e4444bd480c0f4379b8bb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6528386896dc4f9bb0b079cb54349f2f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be393ebf21204cba8984d8440dd2c278",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d4efc726a9849398a359f5a50b56939",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7df7616123e6410283f641b713014beb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e637082f7a8a482383a4180277a69e35",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model= AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=torch.float16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGuM2nEco8XL"
      },
      "outputs": [],
      "source": [
        "#@title Medusa Implementation\n",
        "\n",
        "class medusahead(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim, weights_data):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(input_dim, input_dim)\n",
        "    self.linear2 = nn.Linear(input_dim, output_dim)\n",
        "    self.silu   = nn.SiLU()\n",
        "\n",
        "    self._initialize_weights_to_zero(self.linear1)\n",
        "    # Ensure the weights of linear1 are in float16\n",
        "    self.linear1.weight.data = self.linear1.weight.data.half()\n",
        "    if self.linear1.bias is not None:\n",
        "        self.linear1.bias.data = self.linear1.bias.data.half()\n",
        "\n",
        "\n",
        "    self.linear2.weight.data.copy_(weights_data)\n",
        "    self.linear2.weight.data = self.linear2.weight.data.half()\n",
        "    if self.linear2.bias is not None:\n",
        "        self.linear2.bias.data = self.linear2.bias.data.half()\n",
        "\n",
        "\n",
        "  def _initialize_weights_to_zero(self, layer):\n",
        "    if hasattr(layer, \"weight\") and layer.weight is not None:\n",
        "      nn.init.constant_(layer.weight, 0.0)  # Set weights to zero\n",
        "    if hasattr(layer, \"bias\") and layer.bias is not None:\n",
        "      nn.init.constant_(layer.bias, 0.0)  # Set biases to zero\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual_x = x\n",
        "    x= self.linear1(x)\n",
        "    x= self.silu(x)\n",
        "    x= x + residual_x\n",
        "    output= self.linear2(x)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IODRMh4qUlZo"
      },
      "outputs": [],
      "source": [
        "#@title Model with Medusa\n",
        "\n",
        "import json\n",
        "\n",
        "class model_with_medusa(nn.Module):\n",
        "  def __init__(self, base_model_name):\n",
        "    super(model_with_medusa, self).__init__()\n",
        "    self.model= AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=torch.float16)\n",
        "    self.weights_data= None\n",
        "\n",
        "    if self.model.lm_head.weight is not None:\n",
        "      self.weights_data= self.model.lm_head.weight.data\n",
        "\n",
        "    if self.model.lm_head.bias is not None:\n",
        "      self.bias_data= self.model.lm_head.bias.data\n",
        "\n",
        "\n",
        "    # Freeze the base model (optional, if you don't want to fine-tune it)\n",
        "    for param in self.model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Add a new head (custom linear layer)\n",
        "    hidden_size = self.model.config.hidden_size\n",
        "    new_head_out_dim = self.model.config.vocab_size\n",
        "\n",
        "    self.medusa_head1 = medusahead(hidden_size, new_head_out_dim, self.weights_data)\n",
        "    self.medusa_head2 = medusahead(hidden_size, new_head_out_dim, self.weights_data)\n",
        "    self.medusa_head3 = medusahead(hidden_size, new_head_out_dim, self.weights_data)\n",
        "    self.medusa_head4 = medusahead(hidden_size, new_head_out_dim, self.weights_data)\n",
        "    self.medusa_head5 = medusahead(hidden_size, new_head_out_dim, self.weights_data)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask=None):\n",
        "      # Forward pass through the base model\n",
        "      outputs = self.model(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
        "\n",
        "      # Extract the final hidden states\n",
        "      hidden_states = outputs.hidden_states[-1]  # Shape: (batch_size, seq_length, hidden_size)\n",
        "      # print(f'hidden state data type: {hidden_states.dtype}')\n",
        "\n",
        "      # Compute the output from the new head\n",
        "      medusa_head1_output = self.medusa_head1(hidden_states)  # Shape: (batch_size, seq_length, new_head_out_dim)\n",
        "      medusa_head2_output = self.medusa_head2(hidden_states)\n",
        "      medusa_head3_output = self.medusa_head3(hidden_states)\n",
        "      medusa_head4_output = self.medusa_head4(hidden_states)\n",
        "      medusa_head5_output = self.medusa_head5(hidden_states)\n",
        "\n",
        "      return {\n",
        "          \"logits\": outputs.logits,  # Original language modeling logits\n",
        "          \"medusa_head1_output\": medusa_head1_output,  # Output from the new head\n",
        "          \"medusa_head2_output\": medusa_head2_output,\n",
        "          \"medusa_head3_output\": medusa_head3_output,\n",
        "          \"medusa_head4_output\": medusa_head4_output,\n",
        "          \"medusa_head5_output\": medusa_head5_output\n",
        "      }\n",
        "\n",
        "  def save_pretrained(self, save_directory):\n",
        "      \"\"\"Save the model to a directory in Hugging Face format.\"\"\"\n",
        "      # Save model weights\n",
        "      torch.save(self.state_dict(), f\"{save_directory}/pytorch_model.bin\")\n",
        "\n",
        "      # Save config\n",
        "      config = self.model.config.to_dict()\n",
        "      config[\"custom_heads\"] = [\"medusa_head1\", \"medusa_head2\", \"medusa_head3\", \"medusa_head4\", \"medusa_head5\"]\n",
        "      with open(f\"{save_directory}/config.json\", \"w\") as f:\n",
        "          json.dump(config, f)\n",
        "\n",
        "      print(f\"Model saved to {save_directory}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "2c1a290037b746d3bf0918277f2d747a",
            "b9a589eab0514cbdb8a559df6ca959a9",
            "7ac586ca67da4d7eb4278bd7ef2f4178",
            "a4ba36d6eec34ffa8a1a712d5945188a",
            "f292cfc21b2047d2b728d4a815f7d29b",
            "034326c2cc954e0695be3d3bf2a54938",
            "b8523549bc2f477eb0caa125d5eec9c4",
            "ad1d0b8cf9db4f429c0a0dc94986ec10",
            "15908b6d39bd46d69920ad0ba0ed3d80",
            "ddbcbb97d4804b3b84db6641b8257b1a",
            "3b62392368bd4c2dbb374e3e67d4cd21"
          ]
        },
        "id": "hEw-wsIDk-uJ",
        "outputId": "89a1c7e1-79c3-4119-c4c6-f351474c40b5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c1a290037b746d3bf0918277f2d747a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = model_with_medusa(base_model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "numpnu9H6BI0",
        "outputId": "52f47d44-248c-43da-a146-08a23fad04af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OrderedDict([('model.model.embed_tokens.weight',\n",
              "              tensor([[ 9.8884e-05, -2.3329e-04,  5.8460e-04,  ..., -3.4237e-04,\n",
              "                        5.9724e-05, -1.1957e-04],\n",
              "                      [ 1.5579e-02, -1.3435e-02,  1.2840e-02,  ...,  1.2833e-02,\n",
              "                        7.8659e-03, -6.3133e-04],\n",
              "                      [ 1.6689e-03,  1.6994e-03, -1.4938e-02,  ..., -1.1467e-02,\n",
              "                       -1.2245e-02, -4.2200e-04],\n",
              "                      ...,\n",
              "                      [-9.2163e-03,  2.9800e-02, -1.8066e-02,  ..., -1.1650e-02,\n",
              "                        1.0414e-02,  4.2305e-03],\n",
              "                      [-1.2108e-02, -1.4572e-02, -5.6343e-03,  ...,  5.9357e-03,\n",
              "                       -2.2247e-02, -3.9330e-03],\n",
              "                      [-1.5976e-02,  9.3412e-04, -2.1225e-02,  ...,  1.3420e-02,\n",
              "                        2.9282e-02, -8.4610e-03]], dtype=torch.float16)),\n",
              "             ('model.model.layers.0.self_attn.q_proj.weight',\n",
              "              tensor([[-0.0089, -0.0302,  0.0089,  ...,  0.0183, -0.0058, -0.0359],\n",
              "                      [-0.0011, -0.0100,  0.0097,  ...,  0.0154,  0.0048, -0.0108],\n",
              "                      [-0.0019,  0.0132, -0.0085,  ..., -0.0088, -0.0063,  0.0130],\n",
              "                      ...,\n",
              "                      [-0.0103, -0.0065,  0.0307,  ..., -0.0051, -0.0157,  0.0245],\n",
              "                      [-0.0101, -0.0215, -0.0127,  ..., -0.0196, -0.0147, -0.0094],\n",
              "                      [ 0.0187,  0.0116,  0.0190,  ...,  0.0341, -0.0317, -0.0380]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.0.self_attn.k_proj.weight',\n",
              "              tensor([[-0.0320,  0.0256, -0.0025,  ...,  0.0240, -0.0177,  0.0285],\n",
              "                      [-0.0061, -0.0042,  0.0016,  ...,  0.0003, -0.0123,  0.0161],\n",
              "                      [ 0.0016,  0.0125, -0.0137,  ..., -0.0091,  0.0090, -0.0107],\n",
              "                      ...,\n",
              "                      [-0.0322,  0.0383, -0.0548,  ...,  0.0149, -0.0359, -0.0409],\n",
              "                      [-0.0389,  0.0018, -0.0435,  ...,  0.0339, -0.0484, -0.0126],\n",
              "                      [ 0.0197,  0.0387,  0.0402,  ..., -0.0139,  0.0093, -0.0241]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.0.self_attn.v_proj.weight',\n",
              "              tensor([[ 0.0061,  0.0036,  0.0036,  ..., -0.0076,  0.0112,  0.0147],\n",
              "                      [-0.0026, -0.0103,  0.0002,  ...,  0.0077, -0.0093,  0.0059],\n",
              "                      [-0.0079, -0.0063, -0.0082,  ...,  0.0091, -0.0009,  0.0047],\n",
              "                      ...,\n",
              "                      [ 0.0011,  0.0005,  0.0178,  ..., -0.0231, -0.0161, -0.0349],\n",
              "                      [-0.0099,  0.0015, -0.0043,  ..., -0.0080, -0.0046, -0.0221],\n",
              "                      [ 0.0103, -0.0025, -0.0014,  ..., -0.0085,  0.0156,  0.0029]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.0.self_attn.o_proj.weight',\n",
              "              tensor([[ 0.0061,  0.0076,  0.0054,  ...,  0.0065,  0.0013, -0.0068],\n",
              "                      [ 0.0078,  0.0040,  0.0093,  ..., -0.0021,  0.0101,  0.0045],\n",
              "                      [-0.0077,  0.0035,  0.0004,  ..., -0.0181, -0.0134,  0.0068],\n",
              "                      ...,\n",
              "                      [-0.0021,  0.0040, -0.0004,  ...,  0.0170,  0.0054, -0.0034],\n",
              "                      [-0.0017, -0.0064,  0.0090,  ..., -0.0190, -0.0085,  0.0028],\n",
              "                      [ 0.0056,  0.0010,  0.0061,  ..., -0.0312,  0.0025,  0.0040]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.0.mlp.gate_proj.weight',\n",
              "              tensor([[-0.0137, -0.0107, -0.0002,  ...,  0.0123,  0.0066,  0.0091],\n",
              "                      [-0.0183, -0.0208,  0.0315,  ..., -0.0144, -0.0119,  0.0200],\n",
              "                      [ 0.0035, -0.0044,  0.0225,  ...,  0.0198, -0.0042, -0.0261],\n",
              "                      ...,\n",
              "                      [ 0.0001,  0.0039, -0.0063,  ..., -0.0219,  0.0103,  0.0038],\n",
              "                      [-0.0153, -0.0057,  0.0163,  ..., -0.0199, -0.0271,  0.0209],\n",
              "                      [-0.0018, -0.0034,  0.0232,  ...,  0.0252, -0.0001,  0.0044]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.0.mlp.up_proj.weight',\n",
              "              tensor([[ 0.0052,  0.0086, -0.0104,  ..., -0.0095, -0.0005, -0.0262],\n",
              "                      [-0.0100,  0.0052, -0.0143,  ..., -0.0071, -0.0269, -0.0089],\n",
              "                      [-0.0019,  0.0113, -0.0276,  ...,  0.0269, -0.0068, -0.0272],\n",
              "                      ...,\n",
              "                      [-0.0110,  0.0287, -0.0062,  ..., -0.0040, -0.0005,  0.0519],\n",
              "                      [ 0.0075, -0.0036,  0.0195,  ...,  0.0139, -0.0041, -0.0020],\n",
              "                      [-0.0061,  0.0223,  0.0044,  ..., -0.0059, -0.0050, -0.0046]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.0.mlp.down_proj.weight',\n",
              "              tensor([[-0.0057, -0.0078,  0.0525,  ..., -0.0031,  0.0243,  0.0004],\n",
              "                      [-0.0022, -0.0167, -0.0066,  ...,  0.0019, -0.0442,  0.0358],\n",
              "                      [-0.0077, -0.0068,  0.0027,  ...,  0.0149,  0.0360,  0.0126],\n",
              "                      ...,\n",
              "                      [-0.0279,  0.0359,  0.0062,  ...,  0.0182,  0.0021,  0.0253],\n",
              "                      [-0.0124, -0.0269, -0.0172,  ...,  0.0282,  0.0344, -0.0174],\n",
              "                      [-0.0682,  0.0087, -0.0029,  ...,  0.0394,  0.0108, -0.0042]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.0.input_layernorm.weight',\n",
              "              tensor([0.0442, 0.0186, 0.0179,  ..., 0.0307, 0.0244, 0.0277],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.0.post_attention_layernorm.weight',\n",
              "              tensor([0.0582, 0.0822, 0.0836,  ..., 0.0697, 0.0828, 0.0732],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.1.self_attn.q_proj.weight',\n",
              "              tensor([[-0.0143, -0.0006,  0.0097,  ...,  0.0012, -0.0023, -0.0064],\n",
              "                      [ 0.0068,  0.0015,  0.0007,  ..., -0.0071, -0.0008, -0.0165],\n",
              "                      [-0.0193,  0.0026, -0.0005,  ..., -0.0097,  0.0060,  0.0165],\n",
              "                      ...,\n",
              "                      [ 0.0087,  0.0229, -0.0428,  ..., -0.0012,  0.0406,  0.0036],\n",
              "                      [ 0.0148,  0.0183,  0.0051,  ...,  0.0127,  0.0277, -0.0500],\n",
              "                      [ 0.0009, -0.0299, -0.0285,  ..., -0.0128,  0.0244, -0.0123]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.1.self_attn.k_proj.weight',\n",
              "              tensor([[-0.0087, -0.0190,  0.0028,  ...,  0.0179,  0.0088, -0.0002],\n",
              "                      [-0.0026, -0.0202,  0.0037,  ..., -0.0117, -0.0061, -0.0118],\n",
              "                      [ 0.0040, -0.0025,  0.0188,  ...,  0.0016,  0.0038, -0.0009],\n",
              "                      ...,\n",
              "                      [ 0.0146, -0.0224,  0.0332,  ...,  0.0025, -0.0206, -0.0187],\n",
              "                      [-0.0506, -0.0569, -0.0251,  ..., -0.0140, -0.0004,  0.0163],\n",
              "                      [-0.0181,  0.0191, -0.0066,  ..., -0.0090,  0.0280, -0.0200]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.1.self_attn.v_proj.weight',\n",
              "              tensor([[-0.0148, -0.0143, -0.0188,  ...,  0.0158,  0.0100,  0.0204],\n",
              "                      [ 0.0061, -0.0043, -0.0064,  ...,  0.0054, -0.0110,  0.0043],\n",
              "                      [-0.0019,  0.0032, -0.0070,  ...,  0.0090,  0.0111, -0.0080],\n",
              "                      ...,\n",
              "                      [-0.0036, -0.0124, -0.0003,  ..., -0.0100,  0.0033, -0.0132],\n",
              "                      [-0.0050, -0.0045,  0.0021,  ..., -0.0037, -0.0046,  0.0006],\n",
              "                      [-0.0069, -0.0129,  0.0022,  ...,  0.0094, -0.0374,  0.0139]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.1.self_attn.o_proj.weight',\n",
              "              tensor([[-0.0003,  0.0030,  0.0040,  ...,  0.0003,  0.0007, -0.0063],\n",
              "                      [ 0.0161, -0.0046, -0.0015,  ..., -0.0046, -0.0004,  0.0141],\n",
              "                      [ 0.0173, -0.0016, -0.0001,  ...,  0.0045, -0.0049, -0.0144],\n",
              "                      ...,\n",
              "                      [-0.0053,  0.0063, -0.0005,  ...,  0.0051,  0.0056, -0.0053],\n",
              "                      [-0.0184,  0.0091,  0.0093,  ..., -0.0017,  0.0090,  0.0139],\n",
              "                      [ 0.0016, -0.0044,  0.0059,  ...,  0.0035,  0.0046, -0.0119]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.1.mlp.gate_proj.weight',\n",
              "              tensor([[-0.0112, -0.0117,  0.0254,  ...,  0.0119,  0.0283, -0.0036],\n",
              "                      [ 0.0114, -0.0228, -0.0005,  ..., -0.0267, -0.0348, -0.0029],\n",
              "                      [ 0.0153,  0.0018, -0.0153,  ..., -0.0156, -0.0075,  0.0070],\n",
              "                      ...,\n",
              "                      [ 0.0149,  0.0130, -0.0135,  ..., -0.0145,  0.0087, -0.0049],\n",
              "                      [-0.0171, -0.0035,  0.0138,  ..., -0.0069, -0.0384, -0.0065],\n",
              "                      [ 0.0138,  0.0180,  0.0151,  ..., -0.0207, -0.0088,  0.0501]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.1.mlp.up_proj.weight',\n",
              "              tensor([[-0.0152,  0.0068, -0.0153,  ..., -0.0156, -0.0091,  0.0228],\n",
              "                      [-0.0199,  0.0352, -0.0560,  ..., -0.0100,  0.0065, -0.0043],\n",
              "                      [-0.0195,  0.0231,  0.0232,  ...,  0.0178,  0.0153,  0.0264],\n",
              "                      ...,\n",
              "                      [ 0.0108, -0.0023, -0.0034,  ..., -0.0185, -0.0104, -0.0113],\n",
              "                      [ 0.0206,  0.0145, -0.0407,  ..., -0.0009,  0.0018, -0.0031],\n",
              "                      [-0.0343, -0.0062,  0.0035,  ...,  0.0176,  0.0038,  0.0117]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.1.mlp.down_proj.weight',\n",
              "              tensor([[-0.0022, -0.0596,  0.0154,  ..., -0.0007,  0.0284, -0.0032],\n",
              "                      [ 0.0086,  0.0295,  0.0207,  ...,  0.0145,  0.0050,  0.0107],\n",
              "                      [-0.0295, -0.0290, -0.0119,  ...,  0.0258, -0.0119, -0.0013],\n",
              "                      ...,\n",
              "                      [ 0.0179, -0.0326,  0.0489,  ...,  0.0002, -0.0348, -0.0151],\n",
              "                      [-0.0368, -0.0038,  0.0031,  ...,  0.0294, -0.0061,  0.0069],\n",
              "                      [ 0.0220, -0.0052,  0.0249,  ...,  0.0166, -0.0425,  0.0139]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.1.input_layernorm.weight',\n",
              "              tensor([0.1486, 0.0727, 0.0809,  ..., 0.0630, 0.0739, 0.0858],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.1.post_attention_layernorm.weight',\n",
              "              tensor([0.0930, 0.1068, 0.1132,  ..., 0.1044, 0.1074, 0.1088],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.2.self_attn.q_proj.weight',\n",
              "              tensor([[ 0.0024,  0.0494, -0.0230,  ..., -0.0094,  0.0030,  0.0003],\n",
              "                      [ 0.0080, -0.0348, -0.0251,  ...,  0.0138, -0.0037,  0.0707],\n",
              "                      [-0.0165,  0.0602, -0.0410,  ...,  0.0001,  0.0173,  0.0004],\n",
              "                      ...,\n",
              "                      [ 0.0542,  0.0045,  0.0713,  ...,  0.0386,  0.0254, -0.0513],\n",
              "                      [-0.0231, -0.0169, -0.0818,  ...,  0.0251,  0.0173,  0.0559],\n",
              "                      [-0.0731,  0.0564, -0.0063,  ...,  0.0197, -0.0115,  0.0348]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.2.self_attn.k_proj.weight',\n",
              "              tensor([[ 0.0488,  0.0301,  0.0323,  ...,  0.0324, -0.0037, -0.0543],\n",
              "                      [-0.0528,  0.0406,  0.0124,  ..., -0.0061,  0.0225, -0.0092],\n",
              "                      [ 0.0143, -0.0360,  0.0129,  ...,  0.0120,  0.0152, -0.0186],\n",
              "                      ...,\n",
              "                      [ 0.0223, -0.0289, -0.0900,  ..., -0.0058, -0.0764, -0.0246],\n",
              "                      [-0.0242, -0.0137,  0.0503,  ..., -0.0079, -0.0114,  0.0313],\n",
              "                      [ 0.0292, -0.0182,  0.0149,  ...,  0.0224, -0.0146, -0.0332]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.2.self_attn.v_proj.weight',\n",
              "              tensor([[ 0.0132,  0.0119, -0.0008,  ...,  0.0022,  0.0112,  0.0090],\n",
              "                      [ 0.0207,  0.0069, -0.0156,  ...,  0.0100,  0.0032,  0.0014],\n",
              "                      [ 0.0104, -0.0007, -0.0092,  ..., -0.0042, -0.0060,  0.0006],\n",
              "                      ...,\n",
              "                      [ 0.0312,  0.0074,  0.0154,  ..., -0.0123,  0.0076, -0.0181],\n",
              "                      [ 0.0102,  0.0023,  0.0139,  ...,  0.0154,  0.0116,  0.0040],\n",
              "                      [ 0.0218,  0.0094, -0.0097,  ...,  0.0168, -0.0056, -0.0002]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.2.self_attn.o_proj.weight',\n",
              "              tensor([[-0.0087, -0.0079, -0.0027,  ..., -0.0148, -0.0122,  0.0053],\n",
              "                      [-0.0127,  0.0002, -0.0034,  ..., -0.0129,  0.0011,  0.0076],\n",
              "                      [ 0.0230,  0.0112,  0.0035,  ..., -0.0040,  0.0101,  0.0083],\n",
              "                      ...,\n",
              "                      [-0.0111,  0.0042, -0.0289,  ...,  0.0103, -0.0056, -0.0115],\n",
              "                      [-0.0230, -0.0100,  0.0064,  ...,  0.0078, -0.0276,  0.0101],\n",
              "                      [-0.0133, -0.0038, -0.0082,  ..., -0.0161, -0.0070,  0.0139]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.2.mlp.gate_proj.weight',\n",
              "              tensor([[-0.0083, -0.0094, -0.0050,  ...,  0.0163, -0.0479, -0.0274],\n",
              "                      [ 0.0542,  0.0034,  0.0432,  ..., -0.0207,  0.0072, -0.0047],\n",
              "                      [ 0.0215,  0.0032,  0.0202,  ...,  0.0218, -0.0022,  0.0397],\n",
              "                      ...,\n",
              "                      [-0.0022, -0.0445, -0.0724,  ..., -0.0115,  0.0382,  0.0044],\n",
              "                      [ 0.0309, -0.0021, -0.0092,  ..., -0.0055,  0.0060, -0.0097],\n",
              "                      [ 0.0500,  0.0222, -0.0038,  ..., -0.0068,  0.0164, -0.0110]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.2.mlp.up_proj.weight',\n",
              "              tensor([[-4.9629e-03, -6.0539e-03,  1.5656e-02,  ...,  1.3039e-02,\n",
              "                       -2.3972e-02, -3.5583e-02],\n",
              "                      [ 4.1103e-04, -8.7891e-03,  3.3150e-03,  ..., -3.7659e-02,\n",
              "                        2.1591e-02,  1.7822e-02],\n",
              "                      [ 8.7595e-04,  2.7481e-02,  1.0254e-02,  ..., -1.7691e-03,\n",
              "                        1.9312e-03,  2.0721e-02],\n",
              "                      ...,\n",
              "                      [ 1.3981e-03,  8.3542e-03,  2.4216e-02,  ..., -1.0773e-02,\n",
              "                       -2.2614e-02,  8.8310e-04],\n",
              "                      [-3.4088e-02,  1.5869e-02,  4.2450e-02,  ..., -2.8114e-03,\n",
              "                       -5.8889e-04,  7.0343e-03],\n",
              "                      [ 1.4183e-02,  8.1778e-05, -4.3755e-03,  ..., -1.7044e-02,\n",
              "                        6.1188e-03,  2.3468e-02]], dtype=torch.float16)),\n",
              "             ('model.model.layers.2.mlp.down_proj.weight',\n",
              "              tensor([[-0.0151,  0.0111,  0.0209,  ..., -0.0068, -0.0012,  0.0107],\n",
              "                      [-0.0153, -0.0027,  0.0201,  ...,  0.0151,  0.0308, -0.0055],\n",
              "                      [-0.0135, -0.0258,  0.0032,  ..., -0.0332,  0.0331, -0.0163],\n",
              "                      ...,\n",
              "                      [ 0.0372, -0.0270, -0.0284,  ...,  0.0152,  0.0219, -0.0008],\n",
              "                      [ 0.0124,  0.0232,  0.0080,  ..., -0.0294, -0.0109,  0.0261],\n",
              "                      [ 0.0107,  0.0098,  0.0071,  ..., -0.0111,  0.0083, -0.0220]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.2.input_layernorm.weight',\n",
              "              tensor([0.1417, 0.1210, 0.1383,  ..., 0.1238, 0.1191, 0.1460],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.2.post_attention_layernorm.weight',\n",
              "              tensor([0.1375, 0.1406, 0.1381,  ..., 0.1327, 0.1371, 0.1390],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.3.self_attn.q_proj.weight',\n",
              "              tensor([[ 0.0416,  0.0106,  0.0018,  ..., -0.0007, -0.0160,  0.0148],\n",
              "                      [ 0.0202,  0.0037, -0.0155,  ...,  0.0050,  0.0136, -0.0437],\n",
              "                      [-0.0015,  0.0129,  0.0041,  ..., -0.0288, -0.0314, -0.0047],\n",
              "                      ...,\n",
              "                      [ 0.0302,  0.0046,  0.0189,  ..., -0.0312, -0.0216, -0.0011],\n",
              "                      [ 0.0482, -0.0124, -0.0288,  ..., -0.0089, -0.0203,  0.0174],\n",
              "                      [-0.0178,  0.0358,  0.0270,  ..., -0.0156, -0.0107, -0.0208]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.3.self_attn.k_proj.weight',\n",
              "              tensor([[ 6.1572e-05,  6.3095e-03, -4.0131e-02,  ...,  9.4986e-03,\n",
              "                        1.4603e-02, -5.1918e-03],\n",
              "                      [ 7.0534e-03,  7.7171e-03,  7.5951e-03,  ..., -2.1835e-02,\n",
              "                        8.8425e-03,  1.0063e-02],\n",
              "                      [-2.1088e-02, -2.6566e-02, -1.9974e-02,  ...,  1.7059e-02,\n",
              "                       -3.6011e-03, -3.0716e-02],\n",
              "                      ...,\n",
              "                      [-2.1988e-02, -1.6937e-03,  4.5380e-02,  ..., -2.3209e-02,\n",
              "                       -4.8065e-02, -1.5762e-02],\n",
              "                      [-2.1759e-02, -2.3518e-03, -3.8338e-03,  ...,  1.4160e-02,\n",
              "                       -1.3344e-02, -2.7428e-03],\n",
              "                      [-8.9050e-02, -6.8176e-02,  4.5624e-02,  ..., -1.4359e-02,\n",
              "                        1.7029e-02,  1.2405e-02]], dtype=torch.float16)),\n",
              "             ('model.model.layers.3.self_attn.v_proj.weight',\n",
              "              tensor([[-0.0458, -0.0046, -0.0041,  ...,  0.0111,  0.0121, -0.0072],\n",
              "                      [-0.0011,  0.0164, -0.0042,  ...,  0.0043,  0.0101, -0.0431],\n",
              "                      [ 0.0072,  0.0130,  0.0115,  ..., -0.0291,  0.0139, -0.0038],\n",
              "                      ...,\n",
              "                      [-0.0078,  0.0008,  0.0257,  ..., -0.0380,  0.0236,  0.0085],\n",
              "                      [-0.0058,  0.0027, -0.0089,  ...,  0.0105, -0.0062,  0.0174],\n",
              "                      [ 0.0052,  0.0221, -0.0146,  ...,  0.0100, -0.0068,  0.0265]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.3.self_attn.o_proj.weight',\n",
              "              tensor([[ 0.0401, -0.0034, -0.0148,  ..., -0.0292,  0.0116,  0.0102],\n",
              "                      [-0.0085,  0.0016, -0.0018,  ..., -0.0080, -0.0114,  0.0072],\n",
              "                      [-0.0010,  0.0062,  0.0136,  ...,  0.0207, -0.0102,  0.0086],\n",
              "                      ...,\n",
              "                      [-0.0114,  0.0111,  0.0065,  ...,  0.0061, -0.0140, -0.0057],\n",
              "                      [-0.0215,  0.0085,  0.0102,  ..., -0.0112, -0.0140, -0.0126],\n",
              "                      [ 0.0210,  0.0053,  0.0193,  ...,  0.0093,  0.0001, -0.0069]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.3.mlp.gate_proj.weight',\n",
              "              tensor([[-0.0015, -0.0291, -0.0173,  ...,  0.0260, -0.0100, -0.0013],\n",
              "                      [-0.0428,  0.0266,  0.0047,  ..., -0.0547,  0.0187,  0.0274],\n",
              "                      [-0.0304,  0.0055, -0.0476,  ...,  0.0599,  0.0445, -0.0630],\n",
              "                      ...,\n",
              "                      [ 0.0600,  0.0025, -0.0202,  ..., -0.0589,  0.0209, -0.0146],\n",
              "                      [ 0.0431,  0.0075, -0.0376,  ..., -0.0297, -0.0084,  0.0245],\n",
              "                      [-0.0258, -0.0249, -0.0140,  ...,  0.0016, -0.0258, -0.0140]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.3.mlp.up_proj.weight',\n",
              "              tensor([[ 0.0285, -0.0045,  0.0021,  ..., -0.0049, -0.0194,  0.0333],\n",
              "                      [ 0.0268,  0.0069, -0.0151,  ..., -0.0019, -0.0219, -0.0359],\n",
              "                      [ 0.0117, -0.0063, -0.0179,  ..., -0.0016,  0.0050, -0.0058],\n",
              "                      ...,\n",
              "                      [-0.0029,  0.0122,  0.0234,  ...,  0.0141,  0.0177,  0.0180],\n",
              "                      [ 0.0012, -0.0105, -0.0044,  ..., -0.0148, -0.0283,  0.0195],\n",
              "                      [-0.0299, -0.0022,  0.0234,  ...,  0.0188,  0.0195,  0.0265]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.3.mlp.down_proj.weight',\n",
              "              tensor([[ 0.0029,  0.0192,  0.0017,  ...,  0.0477,  0.0072,  0.0044],\n",
              "                      [-0.0175,  0.0086, -0.0077,  ..., -0.0024, -0.0166, -0.0242],\n",
              "                      [-0.0346, -0.0249, -0.0193,  ...,  0.0211,  0.0198, -0.0144],\n",
              "                      ...,\n",
              "                      [ 0.0163,  0.0075, -0.0185,  ..., -0.0104,  0.0191,  0.0188],\n",
              "                      [ 0.0294, -0.0076,  0.0038,  ..., -0.0293, -0.0103,  0.0149],\n",
              "                      [ 0.0324, -0.0529, -0.0239,  ...,  0.0139,  0.0096,  0.0078]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.3.input_layernorm.weight',\n",
              "              tensor([0.2042, 0.2045, 0.2184,  ..., 0.2035, 0.2072, 0.2052],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.3.post_attention_layernorm.weight',\n",
              "              tensor([0.1674, 0.1733, 0.1647,  ..., 0.1675, 0.1696, 0.1642],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.4.self_attn.q_proj.weight',\n",
              "              tensor([[ 0.0080,  0.0157,  0.0299,  ...,  0.0248,  0.0171,  0.0127],\n",
              "                      [-0.0069, -0.0370, -0.0247,  ...,  0.0295,  0.0019, -0.0114],\n",
              "                      [-0.0074, -0.0253,  0.0260,  ...,  0.0532, -0.0146, -0.0286],\n",
              "                      ...,\n",
              "                      [-0.0191,  0.0072, -0.0291,  ...,  0.0048,  0.0553,  0.0411],\n",
              "                      [ 0.0173, -0.0050, -0.0214,  ..., -0.0158,  0.0013,  0.0194],\n",
              "                      [-0.0001,  0.0485,  0.0056,  ..., -0.0134,  0.0444,  0.0112]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.4.self_attn.k_proj.weight',\n",
              "              tensor([[-0.0197, -0.0182, -0.0188,  ..., -0.0272,  0.0353, -0.0046],\n",
              "                      [ 0.0097,  0.0462, -0.0109,  ..., -0.0123,  0.0001, -0.0089],\n",
              "                      [ 0.0150,  0.0137,  0.0229,  ...,  0.0040,  0.0009,  0.0167],\n",
              "                      ...,\n",
              "                      [-0.0400,  0.0847, -0.0015,  ..., -0.0778, -0.0243, -0.0134],\n",
              "                      [-0.0142,  0.0167, -0.0101,  ...,  0.0233,  0.0421,  0.0460],\n",
              "                      [-0.0372, -0.0459,  0.0266,  ...,  0.0048,  0.0522,  0.0562]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.4.self_attn.v_proj.weight',\n",
              "              tensor([[-0.0182,  0.0206,  0.0083,  ...,  0.0059,  0.0029,  0.0036],\n",
              "                      [-0.0121,  0.0060,  0.0255,  ..., -0.0006,  0.0210, -0.0315],\n",
              "                      [ 0.0048, -0.0136,  0.0098,  ..., -0.0149,  0.0076,  0.0152],\n",
              "                      ...,\n",
              "                      [ 0.0166, -0.0310, -0.0100,  ..., -0.0113,  0.0063,  0.0253],\n",
              "                      [-0.0131,  0.0093,  0.0036,  ...,  0.0320, -0.0290, -0.0034],\n",
              "                      [ 0.0089, -0.0110,  0.0370,  ..., -0.0079, -0.0311, -0.0079]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.4.self_attn.o_proj.weight',\n",
              "              tensor([[ 0.0282,  0.0317,  0.0071,  ..., -0.0048, -0.0132, -0.0192],\n",
              "                      [-0.0046, -0.0040, -0.0066,  ...,  0.0003, -0.0316, -0.0235],\n",
              "                      [-0.0336, -0.0107, -0.0158,  ...,  0.0222, -0.0103,  0.0140],\n",
              "                      ...,\n",
              "                      [ 0.0107,  0.0067, -0.0012,  ..., -0.0013,  0.0250, -0.0150],\n",
              "                      [-0.0337,  0.0120,  0.0065,  ...,  0.0139,  0.0178, -0.0267],\n",
              "                      [-0.0312,  0.0194, -0.0287,  ..., -0.0326, -0.0219, -0.0178]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.4.mlp.gate_proj.weight',\n",
              "              tensor([[-0.0111,  0.0006,  0.0351,  ..., -0.0087, -0.0383,  0.0155],\n",
              "                      [ 0.0022,  0.0148, -0.0403,  ..., -0.0030,  0.0201,  0.0149],\n",
              "                      [ 0.0208,  0.0183, -0.0202,  ..., -0.0023, -0.0086, -0.0021],\n",
              "                      ...,\n",
              "                      [ 0.0069, -0.0105, -0.0064,  ...,  0.0024, -0.0193, -0.0020],\n",
              "                      [ 0.0275,  0.0017,  0.0119,  ..., -0.0149,  0.0289,  0.0053],\n",
              "                      [-0.0158,  0.0137, -0.0367,  ..., -0.0163,  0.0464,  0.0095]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.4.mlp.up_proj.weight',\n",
              "              tensor([[ 1.8661e-02,  4.7028e-02, -8.2550e-03,  ..., -2.4124e-02,\n",
              "                        1.9464e-03, -5.3940e-03],\n",
              "                      [ 2.9327e-02,  1.1444e-02,  3.3081e-02,  ..., -2.8931e-02,\n",
              "                        4.5563e-02,  1.2207e-02],\n",
              "                      [ 3.3325e-02,  1.6846e-02,  3.2288e-02,  ...,  1.8585e-02,\n",
              "                       -2.6306e-02,  5.6267e-03],\n",
              "                      ...,\n",
              "                      [ 1.7120e-02,  7.4997e-03, -2.1530e-02,  ...,  7.2527e-04,\n",
              "                       -3.4393e-02, -5.8556e-03],\n",
              "                      [-1.3298e-02,  5.2986e-03,  1.7914e-02,  ..., -5.0476e-02,\n",
              "                        2.3453e-02, -4.0192e-02],\n",
              "                      [ 9.2850e-03,  7.7057e-03,  6.3121e-05,  ..., -4.2076e-03,\n",
              "                       -2.7634e-02, -2.4734e-02]], dtype=torch.float16)),\n",
              "             ('model.model.layers.4.mlp.down_proj.weight',\n",
              "              tensor([[ 0.0271,  0.0238,  0.0225,  ..., -0.0140,  0.0007, -0.0058],\n",
              "                      [ 0.0373,  0.0213,  0.0398,  ..., -0.0135, -0.0215,  0.0031],\n",
              "                      [-0.0105, -0.0291,  0.0285,  ...,  0.0105,  0.0067, -0.0408],\n",
              "                      ...,\n",
              "                      [ 0.0098, -0.0415,  0.0312,  ..., -0.0125, -0.0494,  0.0132],\n",
              "                      [ 0.0040,  0.0213, -0.0176,  ..., -0.0271, -0.0151,  0.0241],\n",
              "                      [-0.0067,  0.0163,  0.0159,  ..., -0.0127, -0.0243, -0.0083]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.4.input_layernorm.weight',\n",
              "              tensor([0.2301, 0.2357, 0.2314,  ..., 0.2407, 0.2261, 0.2473],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.4.post_attention_layernorm.weight',\n",
              "              tensor([0.1799, 0.1783, 0.1794,  ..., 0.1780, 0.1875, 0.1738],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.5.self_attn.q_proj.weight',\n",
              "              tensor([[ 8.0338e-03,  8.0261e-03,  1.3283e-02,  ...,  2.4643e-02,\n",
              "                        2.0523e-02,  1.7227e-02],\n",
              "                      [ 3.0785e-03,  1.3588e-02,  1.1627e-02,  ...,  2.0309e-02,\n",
              "                       -5.5695e-04, -2.7191e-02],\n",
              "                      [-1.0857e-02, -2.8152e-03, -1.2383e-02,  ..., -1.6296e-02,\n",
              "                       -2.2720e-02, -1.5480e-02],\n",
              "                      ...,\n",
              "                      [-1.8797e-03,  6.4087e-03, -1.5221e-02,  ...,  4.3755e-03,\n",
              "                       -1.8021e-02, -4.3274e-02],\n",
              "                      [ 7.2098e-03, -4.3259e-03,  5.7861e-02,  ..., -1.2718e-02,\n",
              "                        6.3354e-02, -4.1485e-05],\n",
              "                      [-6.7627e-02, -2.7939e-02, -2.5299e-02,  ..., -2.6459e-02,\n",
              "                       -2.5208e-02, -2.2797e-02]], dtype=torch.float16)),\n",
              "             ('model.model.layers.5.self_attn.k_proj.weight',\n",
              "              tensor([[ 0.0132, -0.0040,  0.0116,  ..., -0.0193,  0.0212, -0.0124],\n",
              "                      [ 0.0012, -0.0027, -0.0159,  ...,  0.0124,  0.0012, -0.0140],\n",
              "                      [-0.0351,  0.0147,  0.0349,  ..., -0.0021,  0.0132, -0.0330],\n",
              "                      ...,\n",
              "                      [ 0.0068, -0.0416, -0.0186,  ..., -0.0299,  0.0086,  0.0592],\n",
              "                      [ 0.0505, -0.0131, -0.0405,  ..., -0.0520,  0.0250,  0.0513],\n",
              "                      [ 0.0272, -0.0263, -0.0179,  ...,  0.0127,  0.0323,  0.0642]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.5.self_attn.v_proj.weight',\n",
              "              tensor([[ 0.0050,  0.0231, -0.0097,  ..., -0.0228,  0.0095,  0.0151],\n",
              "                      [ 0.0385,  0.0039, -0.0359,  ...,  0.0070, -0.0215,  0.0006],\n",
              "                      [ 0.0208,  0.0293,  0.0200,  ..., -0.0109, -0.0092,  0.0371],\n",
              "                      ...,\n",
              "                      [-0.0389, -0.0049, -0.0006,  ...,  0.0082,  0.0080, -0.0050],\n",
              "                      [-0.0436,  0.0079, -0.0002,  ...,  0.0018, -0.0198,  0.0031],\n",
              "                      [-0.0065, -0.0107, -0.0041,  ...,  0.0136, -0.0009, -0.0079]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.5.self_attn.o_proj.weight',\n",
              "              tensor([[ 0.0157,  0.0032, -0.0029,  ..., -0.0107, -0.0339,  0.0019],\n",
              "                      [ 0.0222,  0.0214,  0.0124,  ..., -0.0319, -0.0066, -0.0058],\n",
              "                      [ 0.0010,  0.0011, -0.0083,  ..., -0.0114,  0.0177, -0.0070],\n",
              "                      ...,\n",
              "                      [ 0.0136, -0.0086, -0.0115,  ..., -0.0005, -0.0412,  0.0002],\n",
              "                      [ 0.0050, -0.0270, -0.0311,  ..., -0.0015,  0.0069, -0.0169],\n",
              "                      [-0.0171,  0.0239, -0.0233,  ...,  0.0248,  0.0088, -0.0017]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.5.mlp.gate_proj.weight',\n",
              "              tensor([[-0.0233,  0.0030,  0.0348,  ...,  0.0138, -0.0374,  0.0254],\n",
              "                      [-0.0350, -0.0283, -0.0024,  ...,  0.0066,  0.0324,  0.0125],\n",
              "                      [-0.0066, -0.0108, -0.0271,  ...,  0.0174, -0.0189, -0.0046],\n",
              "                      ...,\n",
              "                      [-0.0184,  0.0457,  0.0564,  ...,  0.0008, -0.0124,  0.0017],\n",
              "                      [ 0.0150,  0.0170,  0.0223,  ..., -0.0360, -0.0276,  0.0174],\n",
              "                      [ 0.0003, -0.0012,  0.0345,  ...,  0.0331, -0.0281,  0.0083]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.5.mlp.up_proj.weight',\n",
              "              tensor([[ 0.0266, -0.0279,  0.0401,  ...,  0.0155,  0.0163, -0.0039],\n",
              "                      [-0.0207, -0.0078, -0.0013,  ..., -0.0114, -0.0224,  0.0057],\n",
              "                      [-0.0204,  0.0204,  0.0067,  ...,  0.0389,  0.0002,  0.0065],\n",
              "                      ...,\n",
              "                      [-0.0129, -0.0140, -0.0038,  ..., -0.0171, -0.0236,  0.0202],\n",
              "                      [-0.0120, -0.0128,  0.0192,  ...,  0.0358, -0.0119, -0.0265],\n",
              "                      [-0.0008, -0.0099, -0.0033,  ...,  0.0032,  0.0164, -0.0078]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.5.mlp.down_proj.weight',\n",
              "              tensor([[-0.0088, -0.0078,  0.0057,  ..., -0.0104, -0.0332,  0.0031],\n",
              "                      [-0.0402,  0.0044, -0.0085,  ..., -0.0297, -0.0083, -0.0096],\n",
              "                      [ 0.0342, -0.0009,  0.0113,  ...,  0.0171, -0.0279, -0.0121],\n",
              "                      ...,\n",
              "                      [ 0.0015,  0.0004,  0.0213,  ..., -0.0219,  0.0268,  0.0263],\n",
              "                      [ 0.0213, -0.0018,  0.0286,  ..., -0.0047,  0.0286, -0.0170],\n",
              "                      [ 0.0303,  0.0153, -0.0096,  ...,  0.0009,  0.0129,  0.0044]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.5.input_layernorm.weight',\n",
              "              tensor([0.2817, 0.2791, 0.2817,  ..., 0.2847, 0.2725, 0.2825],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.5.post_attention_layernorm.weight',\n",
              "              tensor([0.1891, 0.1923, 0.1890,  ..., 0.1880, 0.1941, 0.1824],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.6.self_attn.q_proj.weight',\n",
              "              tensor([[-1.9089e-02,  9.4833e-03, -1.1108e-02,  ...,  1.2726e-02,\n",
              "                        6.8092e-04, -1.6266e-02],\n",
              "                      [ 7.1347e-05,  1.1017e-02, -9.7733e-03,  ..., -3.8574e-02,\n",
              "                        5.2757e-03,  1.3954e-02],\n",
              "                      [ 1.5774e-03, -8.7662e-03,  1.9821e-02,  ..., -1.0729e-03,\n",
              "                        3.6812e-03,  2.7603e-02],\n",
              "                      ...,\n",
              "                      [-4.3427e-02, -4.4250e-02, -3.5217e-02,  ..., -7.7019e-03,\n",
              "                        1.8311e-02,  3.3245e-03],\n",
              "                      [ 3.6835e-02, -2.4094e-02, -3.6957e-02,  ...,  5.8258e-02,\n",
              "                        2.2171e-02, -2.1454e-02],\n",
              "                      [-1.0307e-02,  2.4216e-02, -2.9556e-02,  ..., -3.4088e-02,\n",
              "                       -3.4393e-02, -2.4536e-02]], dtype=torch.float16)),\n",
              "             ('model.model.layers.6.self_attn.k_proj.weight',\n",
              "              tensor([[ 0.0045, -0.0243, -0.0347,  ...,  0.0246,  0.0224,  0.0291],\n",
              "                      [-0.0066,  0.0074, -0.0009,  ...,  0.0129,  0.0074, -0.0180],\n",
              "                      [-0.0303,  0.0028,  0.0047,  ..., -0.0076, -0.0081, -0.0308],\n",
              "                      ...,\n",
              "                      [-0.0300, -0.0206, -0.0436,  ..., -0.0108, -0.0269, -0.0184],\n",
              "                      [ 0.0202, -0.0469, -0.0076,  ...,  0.0410,  0.0541,  0.0304],\n",
              "                      [ 0.0220,  0.0249, -0.0344,  ..., -0.0078, -0.0157, -0.0206]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.6.self_attn.v_proj.weight',\n",
              "              tensor([[ 0.0038, -0.0173,  0.0088,  ..., -0.0052,  0.0149, -0.0432],\n",
              "                      [-0.0138, -0.0259, -0.0222,  ..., -0.0229,  0.0073,  0.0096],\n",
              "                      [ 0.0017, -0.0225,  0.0003,  ...,  0.0044,  0.0088,  0.0265],\n",
              "                      ...,\n",
              "                      [ 0.0048,  0.0245,  0.0272,  ..., -0.0019, -0.0144, -0.0209],\n",
              "                      [ 0.0065, -0.0076, -0.0183,  ..., -0.0248, -0.0249,  0.0065],\n",
              "                      [-0.0167, -0.0136, -0.0268,  ...,  0.0172,  0.0077, -0.0045]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.6.self_attn.o_proj.weight',\n",
              "              tensor([[ 0.0312,  0.0266, -0.0082,  ..., -0.0101, -0.0100,  0.0152],\n",
              "                      [ 0.0113,  0.0020,  0.0113,  ..., -0.0317,  0.0203, -0.0062],\n",
              "                      [ 0.0234,  0.0038, -0.0048,  ..., -0.0050,  0.0110,  0.0350],\n",
              "                      ...,\n",
              "                      [-0.0122,  0.0085,  0.0227,  ...,  0.0051,  0.0451, -0.0019],\n",
              "                      [-0.0028, -0.0104, -0.0113,  ...,  0.0081,  0.0086,  0.0109],\n",
              "                      [ 0.0096, -0.0208, -0.0266,  ...,  0.0208, -0.0031,  0.0015]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.6.mlp.gate_proj.weight',\n",
              "              tensor([[-0.0194, -0.0199,  0.0196,  ...,  0.0147,  0.0006,  0.0040],\n",
              "                      [ 0.0247, -0.0113,  0.0072,  ...,  0.0168, -0.0195, -0.0184],\n",
              "                      [ 0.0360, -0.0095, -0.0255,  ..., -0.0064, -0.0028, -0.0111],\n",
              "                      ...,\n",
              "                      [ 0.0362, -0.0032,  0.0105,  ..., -0.0101, -0.0241,  0.0294],\n",
              "                      [ 0.0199, -0.0125,  0.0106,  ...,  0.0220, -0.0049, -0.0201],\n",
              "                      [-0.0015,  0.0242, -0.0116,  ...,  0.0097,  0.0515, -0.0418]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.6.mlp.up_proj.weight',\n",
              "              tensor([[ 2.3975e-03, -2.2552e-02, -8.6517e-03,  ...,  6.4697e-02,\n",
              "                        1.0429e-02,  4.3365e-02],\n",
              "                      [ 2.0370e-03, -1.8143e-02,  1.8311e-02,  ...,  1.6907e-02,\n",
              "                        3.8940e-02,  4.2076e-03],\n",
              "                      [ 3.3569e-03,  2.6169e-02,  2.1957e-02,  ..., -1.3763e-02,\n",
              "                        4.5872e-04, -1.3474e-02],\n",
              "                      ...,\n",
              "                      [ 1.1551e-02, -1.7883e-02,  1.0582e-02,  ...,  1.5581e-04,\n",
              "                        3.6697e-03,  2.3560e-02],\n",
              "                      [-4.0531e-06,  1.1185e-02,  3.0731e-02,  ..., -1.6449e-02,\n",
              "                        1.2619e-02, -3.7994e-02],\n",
              "                      [-1.1604e-02,  1.0483e-02, -1.1795e-02,  ...,  1.0201e-02,\n",
              "                       -2.1286e-02, -1.5808e-02]], dtype=torch.float16)),\n",
              "             ('model.model.layers.6.mlp.down_proj.weight',\n",
              "              tensor([[ 0.0089, -0.0132, -0.0364,  ..., -0.0094,  0.0089,  0.0168],\n",
              "                      [ 0.0258,  0.0400,  0.0207,  ..., -0.0374, -0.0001, -0.0139],\n",
              "                      [ 0.0019,  0.0190,  0.0116,  ..., -0.0045,  0.0142, -0.0016],\n",
              "                      ...,\n",
              "                      [ 0.0196, -0.0186,  0.0272,  ..., -0.0084,  0.0262,  0.0200],\n",
              "                      [-0.0020, -0.0150,  0.0229,  ...,  0.0228,  0.0157, -0.0182],\n",
              "                      [ 0.0255,  0.0006,  0.0005,  ...,  0.0306,  0.0156,  0.0294]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.6.input_layernorm.weight',\n",
              "              tensor([0.2878, 0.2756, 0.2825,  ..., 0.2847, 0.2783, 0.2783],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.6.post_attention_layernorm.weight',\n",
              "              tensor([0.2017, 0.1987, 0.1951,  ..., 0.1918, 0.2040, 0.1879],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.7.self_attn.q_proj.weight',\n",
              "              tensor([[ 1.1375e-02,  1.2226e-03,  3.3569e-02,  ...,  8.8882e-03,\n",
              "                       -2.9968e-02, -4.9210e-04],\n",
              "                      [-7.3700e-03,  1.3794e-02,  4.0169e-03,  ...,  3.5095e-02,\n",
              "                        4.9770e-05, -1.7380e-02],\n",
              "                      [-2.9449e-03,  3.5477e-03,  2.0294e-02,  ..., -1.1658e-02,\n",
              "                        6.4049e-03,  7.2479e-03],\n",
              "                      ...,\n",
              "                      [-7.3669e-02,  6.4636e-02,  2.5146e-02,  ...,  7.1945e-03,\n",
              "                       -6.6711e-02, -2.1332e-02],\n",
              "                      [ 3.4027e-02, -1.7548e-02,  3.2684e-02,  ...,  2.3178e-02,\n",
              "                       -6.3553e-03,  3.9139e-03],\n",
              "                      [ 2.6398e-02, -3.9864e-04,  3.4912e-02,  ...,  3.1235e-02,\n",
              "                        2.3468e-02,  1.7944e-02]], dtype=torch.float16)),\n",
              "             ('model.model.layers.7.self_attn.k_proj.weight',\n",
              "              tensor([[-0.0048,  0.0004, -0.0227,  ..., -0.0420,  0.0105, -0.0014],\n",
              "                      [ 0.0227, -0.0168,  0.0027,  ...,  0.0256, -0.0083, -0.0262],\n",
              "                      [-0.0183, -0.0090,  0.0091,  ...,  0.0193,  0.0124, -0.0126],\n",
              "                      ...,\n",
              "                      [ 0.0424, -0.0292,  0.0030,  ...,  0.0221,  0.0050,  0.0088],\n",
              "                      [ 0.0039, -0.0114,  0.0110,  ...,  0.0086,  0.0200,  0.0800],\n",
              "                      [-0.0448,  0.0423,  0.0191,  ...,  0.0281, -0.0501,  0.0051]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.7.self_attn.v_proj.weight',\n",
              "              tensor([[ 0.0116, -0.0065, -0.0017,  ..., -0.0263,  0.0014, -0.0131],\n",
              "                      [-0.0205, -0.0256,  0.0005,  ..., -0.0118,  0.0007,  0.0111],\n",
              "                      [ 0.0146,  0.0086,  0.0287,  ..., -0.0069,  0.0119,  0.0017],\n",
              "                      ...,\n",
              "                      [ 0.0071, -0.0187,  0.0089,  ...,  0.0071,  0.0307, -0.0209],\n",
              "                      [ 0.0018,  0.0048,  0.0181,  ..., -0.0273,  0.0027,  0.0160],\n",
              "                      [-0.0080,  0.0129,  0.0253,  ...,  0.0032,  0.0128, -0.0070]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.7.self_attn.o_proj.weight',\n",
              "              tensor([[ 0.0191,  0.0299, -0.0005,  ...,  0.0099,  0.0012, -0.0046],\n",
              "                      [ 0.0022, -0.0034, -0.0017,  ..., -0.0057,  0.0157,  0.0191],\n",
              "                      [ 0.0029,  0.0310,  0.0214,  ..., -0.0035, -0.0066,  0.0154],\n",
              "                      ...,\n",
              "                      [ 0.0317,  0.0077, -0.0010,  ..., -0.0099, -0.0095, -0.0265],\n",
              "                      [-0.0067, -0.0022,  0.0130,  ...,  0.0363, -0.0058,  0.0092],\n",
              "                      [ 0.0034, -0.0383,  0.0063,  ...,  0.0163,  0.0059, -0.0098]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.7.mlp.gate_proj.weight',\n",
              "              tensor([[-0.0210,  0.0096,  0.0095,  ...,  0.0116,  0.0421, -0.0078],\n",
              "                      [-0.0140, -0.0171,  0.0324,  ..., -0.0008,  0.0292, -0.0171],\n",
              "                      [-0.0126,  0.0056,  0.0033,  ..., -0.0205,  0.0148, -0.0326],\n",
              "                      ...,\n",
              "                      [ 0.0165, -0.0281,  0.0005,  ...,  0.0303,  0.0243, -0.0008],\n",
              "                      [ 0.0086, -0.0104, -0.0110,  ..., -0.0316,  0.0088, -0.0055],\n",
              "                      [-0.0139,  0.0144, -0.0376,  ..., -0.0216, -0.0268,  0.0052]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.7.mlp.up_proj.weight',\n",
              "              tensor([[ 2.1576e-02, -1.7410e-02,  1.3030e-04,  ..., -9.6416e-04,\n",
              "                        8.8577e-03, -2.1652e-02],\n",
              "                      [ 2.5558e-02, -8.0338e-03,  4.7684e-03,  ..., -1.1742e-05,\n",
              "                        8.7280e-03, -7.3357e-03],\n",
              "                      [ 1.7605e-03, -5.8899e-03, -1.4679e-02,  ..., -2.1763e-03,\n",
              "                       -3.6907e-03,  1.4801e-02],\n",
              "                      ...,\n",
              "                      [ 3.2990e-02, -1.7319e-02, -1.2238e-02,  ...,  9.5215e-03,\n",
              "                        9.7351e-03,  2.8244e-02],\n",
              "                      [-1.4214e-02, -3.5076e-03, -1.7120e-02,  ..., -3.0384e-03,\n",
              "                       -1.4532e-04,  5.8823e-03],\n",
              "                      [ 1.9531e-02, -9.6703e-04,  5.4855e-03,  ..., -3.3905e-02,\n",
              "                        1.3512e-02, -1.3771e-03]], dtype=torch.float16)),\n",
              "             ('model.model.layers.7.mlp.down_proj.weight',\n",
              "              tensor([[ 0.0455,  0.0066,  0.0152,  ..., -0.0009, -0.0145,  0.0020],\n",
              "                      [ 0.0116, -0.0138,  0.0181,  ..., -0.0199,  0.0161, -0.0119],\n",
              "                      [-0.0334, -0.0522,  0.0097,  ..., -0.0258,  0.0280,  0.0147],\n",
              "                      ...,\n",
              "                      [ 0.0261,  0.0090, -0.0362,  ..., -0.0072, -0.0049, -0.0106],\n",
              "                      [ 0.0463,  0.0006,  0.0079,  ..., -0.0054,  0.0027,  0.0424],\n",
              "                      [-0.0020,  0.0246, -0.0060,  ..., -0.0009, -0.0524,  0.0025]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.7.input_layernorm.weight',\n",
              "              tensor([0.2947, 0.2791, 0.2805,  ..., 0.2922, 0.2820, 0.2939],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.7.post_attention_layernorm.weight',\n",
              "              tensor([0.2050, 0.2057, 0.2009,  ..., 0.2025, 0.2096, 0.1926],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.8.self_attn.q_proj.weight',\n",
              "              tensor([[-0.0102, -0.0023, -0.0170,  ..., -0.0130,  0.0006,  0.0007],\n",
              "                      [ 0.0079,  0.0095,  0.0357,  ..., -0.0272,  0.0121,  0.0027],\n",
              "                      [ 0.0106, -0.0155, -0.0111,  ...,  0.0210, -0.0063, -0.0047],\n",
              "                      ...,\n",
              "                      [-0.0242,  0.0523, -0.0341,  ..., -0.0163, -0.0135, -0.0287],\n",
              "                      [ 0.0073, -0.0316, -0.0437,  ..., -0.0641,  0.0361,  0.0058],\n",
              "                      [ 0.0356, -0.0329,  0.0212,  ...,  0.0082, -0.0221, -0.0479]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.8.self_attn.k_proj.weight',\n",
              "              tensor([[-0.0074,  0.0490,  0.0229,  ..., -0.0142,  0.0024,  0.0118],\n",
              "                      [-0.0082,  0.0027,  0.0117,  ...,  0.0173,  0.0173,  0.0065],\n",
              "                      [ 0.0050,  0.0135,  0.0034,  ..., -0.0084,  0.0098,  0.0007],\n",
              "                      ...,\n",
              "                      [-0.0344, -0.0414, -0.0498,  ..., -0.0332, -0.0666,  0.0261],\n",
              "                      [ 0.0104, -0.0124, -0.0695,  ...,  0.0037,  0.0253, -0.0433],\n",
              "                      [ 0.0343, -0.0017,  0.0297,  ...,  0.0620, -0.0597, -0.0153]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.8.self_attn.v_proj.weight',\n",
              "              tensor([[ 0.0109,  0.0021, -0.0057,  ..., -0.0261,  0.0244, -0.0160],\n",
              "                      [ 0.0075, -0.0148, -0.0321,  ...,  0.0222, -0.0210,  0.0031],\n",
              "                      [ 0.0283,  0.0081,  0.0173,  ..., -0.0080, -0.0145,  0.0250],\n",
              "                      ...,\n",
              "                      [-0.0015,  0.0105,  0.0108,  ..., -0.0094, -0.0195,  0.0057],\n",
              "                      [ 0.0181, -0.0095, -0.0069,  ..., -0.0009, -0.0132, -0.0093],\n",
              "                      [-0.0156,  0.0145,  0.0069,  ..., -0.0159,  0.0319,  0.0126]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.8.self_attn.o_proj.weight',\n",
              "              tensor([[-5.2223e-03, -1.7899e-02, -1.3412e-02,  ..., -1.1650e-02,\n",
              "                        9.5825e-03, -1.9165e-02],\n",
              "                      [-1.8707e-02,  1.0010e-02,  1.1429e-02,  ...,  7.0953e-03,\n",
              "                       -1.2772e-02,  2.9259e-03],\n",
              "                      [ 9.7504e-03, -4.0245e-03, -6.4850e-03,  ..., -1.1940e-02,\n",
              "                       -8.5592e-04,  2.3453e-02],\n",
              "                      ...,\n",
              "                      [-2.3782e-05,  1.8463e-02,  8.1787e-03,  ..., -5.0688e-04,\n",
              "                       -3.9787e-03,  5.9090e-03],\n",
              "                      [-1.7605e-03, -8.0643e-03, -3.1464e-02,  ..., -5.7144e-03,\n",
              "                       -2.0218e-02, -3.5362e-03],\n",
              "                      [-4.0009e-02,  1.4069e-02,  6.4354e-03,  ..., -1.3535e-02,\n",
              "                        9.9564e-03,  1.4893e-02]], dtype=torch.float16)),\n",
              "             ('model.model.layers.8.mlp.gate_proj.weight',\n",
              "              tensor([[-0.0070,  0.0146, -0.0006,  ..., -0.0151,  0.0097,  0.0119],\n",
              "                      [-0.0236,  0.0115,  0.0381,  ..., -0.0513,  0.0009, -0.0335],\n",
              "                      [ 0.0007, -0.0078, -0.0008,  ..., -0.0086,  0.0090, -0.0089],\n",
              "                      ...,\n",
              "                      [ 0.0011,  0.0136,  0.0339,  ..., -0.0125,  0.0059,  0.0143],\n",
              "                      [-0.0070,  0.0162, -0.0798,  ..., -0.0021,  0.0202,  0.0038],\n",
              "                      [ 0.0217, -0.0596,  0.0060,  ...,  0.0067,  0.0190, -0.0075]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.8.mlp.up_proj.weight',\n",
              "              tensor([[-0.0106,  0.0033, -0.0182,  ...,  0.0164,  0.0202,  0.0181],\n",
              "                      [ 0.0049,  0.0015, -0.0189,  ...,  0.0264,  0.0033,  0.0225],\n",
              "                      [-0.0069, -0.0207,  0.0297,  ..., -0.0295,  0.0004, -0.0165],\n",
              "                      ...,\n",
              "                      [ 0.0024, -0.0156,  0.0232,  ...,  0.0174, -0.0163,  0.0176],\n",
              "                      [ 0.0197,  0.0162, -0.0126,  ...,  0.0033, -0.0196,  0.0108],\n",
              "                      [-0.0048, -0.0070, -0.0112,  ..., -0.0061, -0.0626,  0.0239]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.8.mlp.down_proj.weight',\n",
              "              tensor([[-0.0051, -0.0046,  0.0130,  ...,  0.0093, -0.0076,  0.0049],\n",
              "                      [-0.0195, -0.0013, -0.0235,  ..., -0.0027, -0.0225, -0.0092],\n",
              "                      [-0.0186, -0.0037,  0.0246,  ..., -0.0042,  0.0559, -0.0350],\n",
              "                      ...,\n",
              "                      [-0.0040,  0.0215, -0.0106,  ..., -0.0048,  0.0230,  0.0160],\n",
              "                      [-0.0068, -0.0122, -0.0337,  ..., -0.0368, -0.0080, -0.0151],\n",
              "                      [-0.0166, -0.0008,  0.0057,  ...,  0.0211,  0.0212,  0.0374]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.8.input_layernorm.weight',\n",
              "              tensor([0.3052, 0.2974, 0.3022,  ..., 0.3120, 0.3140, 0.3101],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.8.post_attention_layernorm.weight',\n",
              "              tensor([0.2078, 0.2084, 0.2075,  ..., 0.2032, 0.2131, 0.1937],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.9.self_attn.q_proj.weight',\n",
              "              tensor([[ 0.0072,  0.0058,  0.0036,  ..., -0.0193, -0.0028, -0.0489],\n",
              "                      [ 0.0200,  0.0058,  0.0067,  ..., -0.0258, -0.0120,  0.0104],\n",
              "                      [-0.0130,  0.0001, -0.0043,  ..., -0.0063,  0.0091,  0.0060],\n",
              "                      ...,\n",
              "                      [-0.0311, -0.0337, -0.0202,  ...,  0.0760,  0.0093,  0.0220],\n",
              "                      [-0.0340,  0.0716,  0.0061,  ..., -0.0186,  0.0077,  0.0420],\n",
              "                      [-0.0270, -0.0526, -0.0596,  ...,  0.0502,  0.0222,  0.0468]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.9.self_attn.k_proj.weight',\n",
              "              tensor([[-0.0218, -0.0238,  0.0038,  ..., -0.0051,  0.0345,  0.0303],\n",
              "                      [-0.0137,  0.0175, -0.0156,  ..., -0.0016,  0.0131,  0.0211],\n",
              "                      [-0.0012,  0.0060, -0.0076,  ...,  0.0306,  0.0059,  0.0114],\n",
              "                      ...,\n",
              "                      [ 0.0447,  0.0530,  0.0900,  ..., -0.0388, -0.0354,  0.1246],\n",
              "                      [ 0.0211,  0.0554,  0.0095,  ...,  0.0071, -0.0463,  0.1467],\n",
              "                      [-0.0323, -0.0709, -0.0264,  ..., -0.0096, -0.0039,  0.0261]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.9.self_attn.v_proj.weight',\n",
              "              tensor([[ 0.0042, -0.0027, -0.0304,  ..., -0.0062,  0.0081,  0.0263],\n",
              "                      [-0.0186, -0.0121, -0.0293,  ..., -0.0087, -0.0158, -0.0174],\n",
              "                      [-0.0071,  0.0255, -0.0376,  ..., -0.0177,  0.0025,  0.0098],\n",
              "                      ...,\n",
              "                      [ 0.0220, -0.0303,  0.0130,  ...,  0.0234,  0.0163,  0.0210],\n",
              "                      [ 0.0068, -0.0054, -0.0010,  ...,  0.0145, -0.0194,  0.0017],\n",
              "                      [-0.0037,  0.0041,  0.0152,  ..., -0.0034, -0.0413,  0.0048]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.9.self_attn.o_proj.weight',\n",
              "              tensor([[ 7.6447e-03,  1.1940e-02,  1.9516e-02,  ..., -3.2928e-02,\n",
              "                        9.1219e-04,  3.3020e-02],\n",
              "                      [ 1.8448e-02,  9.9869e-03, -2.1805e-02,  ...,  1.1040e-02,\n",
              "                       -1.5930e-02, -1.2054e-02],\n",
              "                      [-2.2293e-02, -6.5193e-03,  8.2932e-03,  ...,  1.8311e-02,\n",
              "                       -3.4302e-02, -8.6546e-04],\n",
              "                      ...,\n",
              "                      [ 4.9438e-03,  2.1347e-02, -1.8646e-02,  ..., -1.6769e-02,\n",
              "                       -2.3361e-02, -2.3651e-02],\n",
              "                      [-2.3544e-02, -3.4103e-03, -2.9083e-02,  ...,  5.1384e-03,\n",
              "                        4.6909e-05,  3.2711e-03],\n",
              "                      [ 6.4564e-04,  2.8671e-02,  1.5671e-02,  ..., -6.7024e-03,\n",
              "                        4.6082e-03,  8.2016e-03]], dtype=torch.float16)),\n",
              "             ('model.model.layers.9.mlp.gate_proj.weight',\n",
              "              tensor([[ 2.6596e-02,  8.6060e-03, -1.8875e-02,  ...,  1.9485e-02,\n",
              "                        6.4468e-03,  1.2047e-02],\n",
              "                      [ 9.5520e-03,  5.9128e-03,  8.9765e-05,  ...,  1.6388e-02,\n",
              "                       -3.6652e-02, -1.0284e-02],\n",
              "                      [-2.1637e-02,  7.0496e-03,  6.4621e-03,  ...,  4.1687e-02,\n",
              "                        2.4307e-02, -1.7883e-02],\n",
              "                      ...,\n",
              "                      [ 3.3508e-02, -2.7664e-02,  1.4229e-02,  ...,  6.4087e-03,\n",
              "                       -2.8503e-02,  1.6129e-02],\n",
              "                      [-3.4866e-03,  1.8799e-02, -4.9103e-02,  ...,  3.7155e-03,\n",
              "                       -1.0544e-02,  3.7460e-03],\n",
              "                      [-1.6373e-02,  8.7967e-03, -4.9347e-02,  ..., -2.1683e-02,\n",
              "                        2.7496e-02,  2.3376e-02]], dtype=torch.float16)),\n",
              "             ('model.model.layers.9.mlp.up_proj.weight',\n",
              "              tensor([[-0.0090,  0.0353, -0.0104,  ...,  0.0173, -0.0175, -0.0335],\n",
              "                      [-0.0104, -0.0024, -0.0026,  ..., -0.0214, -0.0192,  0.0534],\n",
              "                      [ 0.0007,  0.0138, -0.0081,  ..., -0.0367, -0.0173,  0.0075],\n",
              "                      ...,\n",
              "                      [ 0.0044,  0.0175,  0.0292,  ..., -0.0144, -0.0173, -0.0154],\n",
              "                      [ 0.0210,  0.0002,  0.0069,  ..., -0.0060, -0.0118, -0.0007],\n",
              "                      [-0.0214,  0.0077,  0.0001,  ..., -0.0482,  0.0159,  0.0036]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.9.mlp.down_proj.weight',\n",
              "              tensor([[-0.0014, -0.0085,  0.0027,  ...,  0.0010,  0.0256,  0.0058],\n",
              "                      [-0.0124, -0.0122,  0.0120,  ...,  0.0126,  0.0234,  0.0233],\n",
              "                      [-0.0166, -0.0033,  0.0138,  ...,  0.0063,  0.0288, -0.0319],\n",
              "                      ...,\n",
              "                      [-0.0166, -0.0119, -0.0155,  ..., -0.0025,  0.0226,  0.0362],\n",
              "                      [-0.0309,  0.0542,  0.0350,  ..., -0.0084,  0.0407,  0.0121],\n",
              "                      [-0.0085,  0.0268, -0.0070,  ..., -0.0074,  0.0141,  0.0357]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.9.input_layernorm.weight',\n",
              "              tensor([0.3145, 0.3186, 0.3120,  ..., 0.3347, 0.3162, 0.3213],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.9.post_attention_layernorm.weight',\n",
              "              tensor([0.2085, 0.2084, 0.2152,  ..., 0.2051, 0.2172, 0.1943],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.10.self_attn.q_proj.weight',\n",
              "              tensor([[ 0.0129, -0.0006,  0.0094,  ...,  0.0053, -0.0187, -0.0136],\n",
              "                      [ 0.0116, -0.0312,  0.0014,  ..., -0.0041,  0.0140, -0.0078],\n",
              "                      [-0.0325,  0.0286,  0.0122,  ..., -0.0032,  0.0123,  0.0255],\n",
              "                      ...,\n",
              "                      [-0.0707,  0.0224,  0.0155,  ..., -0.0579,  0.0127,  0.0102],\n",
              "                      [ 0.0095,  0.0057,  0.0310,  ...,  0.0124, -0.0518,  0.0186],\n",
              "                      [ 0.0268, -0.0221,  0.0209,  ..., -0.0222,  0.0013,  0.0303]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.10.self_attn.k_proj.weight',\n",
              "              tensor([[-0.0172,  0.0211,  0.0239,  ...,  0.0231, -0.0084, -0.0013],\n",
              "                      [ 0.0117,  0.0213, -0.0220,  ...,  0.0004, -0.0128,  0.0137],\n",
              "                      [ 0.0093, -0.0077,  0.0175,  ..., -0.0073,  0.0088,  0.0018],\n",
              "                      ...,\n",
              "                      [ 0.0063,  0.0724,  0.0630,  ..., -0.0167,  0.0107, -0.0280],\n",
              "                      [ 0.0090,  0.0205,  0.0243,  ..., -0.0188, -0.0652,  0.0222],\n",
              "                      [ 0.0151,  0.0093,  0.0069,  ..., -0.0176,  0.0475, -0.0129]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.10.self_attn.v_proj.weight',\n",
              "              tensor([[ 0.0242, -0.0028,  0.0138,  ...,  0.0071,  0.0066,  0.0239],\n",
              "                      [-0.0009, -0.0187,  0.0100,  ..., -0.0072, -0.0057, -0.0298],\n",
              "                      [-0.0406,  0.0007, -0.0277,  ..., -0.0045,  0.0063,  0.0045],\n",
              "                      ...,\n",
              "                      [ 0.0388, -0.0054, -0.0427,  ...,  0.0063, -0.0062, -0.0078],\n",
              "                      [-0.0013, -0.0244, -0.0116,  ...,  0.0024,  0.0204, -0.0177],\n",
              "                      [ 0.0177, -0.0074,  0.0138,  ..., -0.0065,  0.0305,  0.0248]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.10.self_attn.o_proj.weight',\n",
              "              tensor([[ 0.0038, -0.0120,  0.0113,  ..., -0.0182,  0.0046, -0.0180],\n",
              "                      [ 0.0289, -0.0041,  0.0099,  ...,  0.0350,  0.0020, -0.0126],\n",
              "                      [-0.0043, -0.0052, -0.0208,  ..., -0.0257, -0.0216, -0.0240],\n",
              "                      ...,\n",
              "                      [ 0.0256, -0.0081, -0.0001,  ..., -0.0042,  0.0100,  0.0129],\n",
              "                      [-0.0092,  0.0004, -0.0042,  ..., -0.0112, -0.0038, -0.0312],\n",
              "                      [ 0.0048,  0.0102, -0.0204,  ...,  0.0145, -0.0245,  0.0005]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.10.mlp.gate_proj.weight',\n",
              "              tensor([[-0.0198, -0.0119, -0.0174,  ..., -0.0202,  0.0011,  0.0050],\n",
              "                      [-0.0271, -0.0004, -0.0728,  ...,  0.0022,  0.0292,  0.0105],\n",
              "                      [ 0.0019, -0.0219, -0.0570,  ...,  0.0350,  0.0501,  0.0446],\n",
              "                      ...,\n",
              "                      [-0.0281, -0.0055, -0.0261,  ..., -0.0125,  0.0414,  0.0142],\n",
              "                      [-0.0008,  0.0124,  0.0015,  ..., -0.0096,  0.0087,  0.0384],\n",
              "                      [-0.0014,  0.0098, -0.0396,  ..., -0.0055,  0.0392, -0.0065]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.10.mlp.up_proj.weight',\n",
              "              tensor([[-0.0155,  0.0091,  0.0303,  ..., -0.0155,  0.0268,  0.0093],\n",
              "                      [ 0.0007, -0.0143,  0.0102,  ...,  0.0285, -0.0048,  0.0439],\n",
              "                      [-0.0393, -0.0098, -0.0090,  ...,  0.0188,  0.0342, -0.0177],\n",
              "                      ...,\n",
              "                      [-0.0141, -0.0186,  0.0053,  ...,  0.0117,  0.0013, -0.0306],\n",
              "                      [ 0.0144, -0.0171, -0.0315,  ...,  0.0200,  0.0206, -0.0234],\n",
              "                      [-0.0115, -0.0210,  0.0418,  ..., -0.0156,  0.0134,  0.0069]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.10.mlp.down_proj.weight',\n",
              "              tensor([[-0.0330,  0.0038, -0.0354,  ..., -0.0081,  0.0427, -0.0085],\n",
              "                      [ 0.0199,  0.0087,  0.0150,  ..., -0.0148,  0.0135, -0.0287],\n",
              "                      [ 0.0413,  0.0337,  0.0198,  ..., -0.0083,  0.0120,  0.0378],\n",
              "                      ...,\n",
              "                      [ 0.0055,  0.0056,  0.0059,  ...,  0.0042,  0.0046, -0.0230],\n",
              "                      [ 0.0266, -0.0350, -0.0136,  ..., -0.0137,  0.0240,  0.0281],\n",
              "                      [ 0.0095,  0.0084, -0.0019,  ..., -0.0302,  0.0031, -0.0136]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.10.input_layernorm.weight',\n",
              "              tensor([0.3367, 0.3318, 0.3337,  ..., 0.3306, 0.3333, 0.3240],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.10.post_attention_layernorm.weight',\n",
              "              tensor([0.2166, 0.2158, 0.2203,  ..., 0.2158, 0.2240, 0.2029],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.11.self_attn.q_proj.weight',\n",
              "              tensor([[-0.0184, -0.0097,  0.0016,  ...,  0.0311,  0.0400, -0.0009],\n",
              "                      [ 0.0032,  0.0077,  0.0194,  ..., -0.0076, -0.0055,  0.0372],\n",
              "                      [ 0.0474,  0.0112, -0.0159,  ...,  0.0083,  0.0220,  0.0104],\n",
              "                      ...,\n",
              "                      [-0.0112, -0.0161,  0.0253,  ...,  0.0278,  0.0420, -0.0182],\n",
              "                      [ 0.0491,  0.0521,  0.0555,  ..., -0.0396, -0.0501, -0.0168],\n",
              "                      [-0.0437, -0.0401, -0.0593,  ..., -0.0480,  0.0202, -0.0008]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.11.self_attn.k_proj.weight',\n",
              "              tensor([[-0.0044,  0.0093,  0.0100,  ...,  0.0061,  0.0489,  0.0022],\n",
              "                      [-0.0032, -0.0153,  0.0346,  ...,  0.0062, -0.0064,  0.0289],\n",
              "                      [ 0.0184,  0.0107, -0.0336,  ..., -0.0008, -0.0149,  0.0168],\n",
              "                      ...,\n",
              "                      [-0.0021, -0.0112, -0.0236,  ..., -0.0039,  0.0094, -0.0005],\n",
              "                      [ 0.0577,  0.0105, -0.0161,  ..., -0.0683,  0.0133, -0.0100],\n",
              "                      [-0.0527, -0.0231, -0.0227,  ..., -0.0553,  0.0062,  0.0199]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.11.self_attn.v_proj.weight',\n",
              "              tensor([[ 0.0042,  0.0317,  0.0192,  ..., -0.0116,  0.0025,  0.0215],\n",
              "                      [ 0.0015,  0.0226,  0.0415,  ..., -0.0241, -0.0179, -0.0188],\n",
              "                      [-0.0185, -0.0196,  0.0163,  ..., -0.0114, -0.0032,  0.0187],\n",
              "                      ...,\n",
              "                      [-0.0088,  0.0118, -0.0254,  ..., -0.0019, -0.0041,  0.0149],\n",
              "                      [-0.0012, -0.0098, -0.0011,  ...,  0.0092, -0.0039,  0.0090],\n",
              "                      [-0.0101, -0.0003,  0.0297,  ...,  0.0138, -0.0175,  0.0175]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.11.self_attn.o_proj.weight',\n",
              "              tensor([[ 0.0016, -0.0058,  0.0284,  ...,  0.0201, -0.0192, -0.0016],\n",
              "                      [-0.0360, -0.0202,  0.0216,  ..., -0.0083,  0.0197, -0.0178],\n",
              "                      [-0.0022, -0.0052, -0.0238,  ...,  0.0122,  0.0052, -0.0274],\n",
              "                      ...,\n",
              "                      [ 0.0136,  0.0066, -0.0057,  ..., -0.0055,  0.0053, -0.0369],\n",
              "                      [ 0.0188, -0.0050,  0.0188,  ...,  0.0042, -0.0044,  0.0113],\n",
              "                      [-0.0251,  0.0276, -0.0192,  ...,  0.0029, -0.0166,  0.0154]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.11.mlp.gate_proj.weight',\n",
              "              tensor([[-0.0110, -0.0053,  0.0127,  ..., -0.0264, -0.0283, -0.0065],\n",
              "                      [ 0.0058,  0.0146,  0.0117,  ...,  0.0792,  0.0211,  0.0418],\n",
              "                      [ 0.0238,  0.0219,  0.0233,  ...,  0.0546,  0.0034,  0.0213],\n",
              "                      ...,\n",
              "                      [ 0.0673, -0.0235,  0.0607,  ...,  0.0072,  0.0379, -0.0109],\n",
              "                      [-0.0139,  0.0056, -0.0173,  ...,  0.0268, -0.0591, -0.0531],\n",
              "                      [-0.0102,  0.0123, -0.0047,  ...,  0.0097,  0.0232,  0.0196]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.11.mlp.up_proj.weight',\n",
              "              tensor([[-0.0030,  0.0008, -0.0190,  ...,  0.0166, -0.0343, -0.0028],\n",
              "                      [ 0.0058,  0.0408,  0.0270,  ..., -0.0032,  0.0093, -0.0071],\n",
              "                      [ 0.0150,  0.0126, -0.0034,  ...,  0.0075,  0.0370,  0.0159],\n",
              "                      ...,\n",
              "                      [ 0.0328, -0.0419,  0.0459,  ..., -0.0267,  0.0389, -0.0338],\n",
              "                      [ 0.0166,  0.0240,  0.0312,  ..., -0.0043, -0.0310, -0.0194],\n",
              "                      [-0.0039,  0.0062, -0.0090,  ..., -0.0205,  0.0487,  0.0047]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.11.mlp.down_proj.weight',\n",
              "              tensor([[-2.4673e-02, -4.6959e-03,  1.2947e-02,  ...,  2.1530e-02,\n",
              "                        9.6207e-03,  2.4078e-02],\n",
              "                      [ 1.5144e-02,  2.0752e-02,  3.2196e-02,  ..., -4.2000e-03,\n",
              "                       -1.0063e-02, -2.8961e-02],\n",
              "                      [ 3.2177e-03, -1.3680e-02,  1.8799e-02,  ...,  2.3727e-02,\n",
              "                        2.9877e-02,  8.1940e-03],\n",
              "                      ...,\n",
              "                      [-3.7136e-03, -1.3596e-02,  2.2858e-02,  ...,  7.1411e-03,\n",
              "                        9.7504e-03, -1.9257e-02],\n",
              "                      [ 5.0125e-03, -4.2816e-02,  3.3913e-03,  ..., -1.2871e-02,\n",
              "                        5.0140e-02,  7.2241e-04],\n",
              "                      [ 2.1790e-02,  1.3268e-02, -5.0664e-05,  ..., -1.5556e-02,\n",
              "                       -2.0103e-03,  3.5339e-02]], dtype=torch.float16)),\n",
              "             ('model.model.layers.11.input_layernorm.weight',\n",
              "              tensor([0.3081, 0.2896, 0.3054,  ..., 0.3059, 0.3091, 0.2942],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.11.post_attention_layernorm.weight',\n",
              "              tensor([0.2225, 0.2184, 0.2230,  ..., 0.2202, 0.2314, 0.2075],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.12.self_attn.q_proj.weight',\n",
              "              tensor([[-0.0058,  0.0169, -0.0003,  ..., -0.0099, -0.0243, -0.0165],\n",
              "                      [-0.0017, -0.0081, -0.0151,  ..., -0.0156, -0.0015, -0.0223],\n",
              "                      [ 0.0144, -0.0076,  0.0113,  ..., -0.0225,  0.0326,  0.0104],\n",
              "                      ...,\n",
              "                      [-0.0125,  0.0889, -0.0257,  ..., -0.0119, -0.0269,  0.0487],\n",
              "                      [-0.0201, -0.0611, -0.0088,  ..., -0.0623,  0.0257, -0.1130],\n",
              "                      [ 0.0290,  0.0341, -0.0331,  ..., -0.0228,  0.0173, -0.0041]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.12.self_attn.k_proj.weight',\n",
              "              tensor([[-0.0088,  0.0115,  0.0153,  ...,  0.0203,  0.0129, -0.0202],\n",
              "                      [-0.0084,  0.0141,  0.0096,  ...,  0.0212,  0.0115,  0.0121],\n",
              "                      [-0.0438, -0.0049, -0.0077,  ...,  0.0310, -0.0268,  0.0025],\n",
              "                      ...,\n",
              "                      [-0.0474,  0.0318,  0.0046,  ...,  0.0392, -0.0388,  0.0479],\n",
              "                      [-0.0092,  0.0059,  0.0118,  ..., -0.0550,  0.0079,  0.0056],\n",
              "                      [-0.0341,  0.0502, -0.0100,  ...,  0.0015,  0.0770, -0.0267]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.12.self_attn.v_proj.weight',\n",
              "              tensor([[-0.0050, -0.0045,  0.0063,  ..., -0.0172,  0.0076, -0.0107],\n",
              "                      [ 0.0012, -0.0047,  0.0003,  ..., -0.0046, -0.0109, -0.0216],\n",
              "                      [-0.0104,  0.0315, -0.0043,  ..., -0.0009,  0.0113, -0.0053],\n",
              "                      ...,\n",
              "                      [ 0.0157, -0.0215,  0.0053,  ...,  0.0381,  0.0094,  0.0213],\n",
              "                      [ 0.0112,  0.0147, -0.0059,  ..., -0.0113,  0.0120, -0.0080],\n",
              "                      [-0.0227, -0.0117, -0.0097,  ...,  0.0333,  0.0183, -0.0117]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.12.self_attn.o_proj.weight',\n",
              "              tensor([[-0.0150, -0.0114,  0.0108,  ..., -0.0043,  0.0229, -0.0388],\n",
              "                      [ 0.0185, -0.0013, -0.0212,  ..., -0.0039,  0.0053, -0.0123],\n",
              "                      [-0.0393, -0.0056, -0.0082,  ..., -0.0067, -0.0276,  0.0074],\n",
              "                      ...,\n",
              "                      [-0.0006, -0.0031,  0.0045,  ..., -0.0195, -0.0151,  0.0222],\n",
              "                      [-0.0202, -0.0069, -0.0080,  ..., -0.0113,  0.0081,  0.0048],\n",
              "                      [ 0.0226, -0.0070,  0.0139,  ..., -0.0092, -0.0044, -0.0248]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.12.mlp.gate_proj.weight',\n",
              "              tensor([[-1.6846e-02, -4.9782e-03, -9.2239e-03,  ..., -2.6230e-02,\n",
              "                        2.8019e-03, -3.9764e-02],\n",
              "                      [-1.2894e-02,  2.4658e-02, -7.6027e-03,  ..., -5.6686e-03,\n",
              "                       -1.2283e-02,  7.1487e-03],\n",
              "                      [ 1.0582e-02,  3.7842e-02, -8.4534e-03,  ...,  1.6388e-02,\n",
              "                        1.6373e-02,  1.8677e-02],\n",
              "                      ...,\n",
              "                      [ 2.6108e-02,  9.3603e-04, -2.0615e-02,  ...,  9.1791e-05,\n",
              "                       -1.9684e-02, -1.2484e-03],\n",
              "                      [ 8.6670e-03, -7.6675e-03, -3.5496e-03,  ...,  4.4937e-03,\n",
              "                        4.1046e-03, -1.7197e-02],\n",
              "                      [-3.9459e-02, -3.1128e-02,  1.2238e-02,  ..., -2.7283e-02,\n",
              "                        1.9974e-02,  1.5701e-02]], dtype=torch.float16)),\n",
              "             ('model.model.layers.12.mlp.up_proj.weight',\n",
              "              tensor([[ 0.0169,  0.0148, -0.0043,  ...,  0.0120,  0.0107, -0.0030],\n",
              "                      [-0.0027,  0.0036, -0.0049,  ..., -0.0094,  0.0004, -0.0027],\n",
              "                      [ 0.0046, -0.0120, -0.0087,  ..., -0.0310,  0.0251,  0.0259],\n",
              "                      ...,\n",
              "                      [ 0.0409,  0.0098, -0.0108,  ..., -0.0180, -0.0169,  0.0074],\n",
              "                      [-0.0119,  0.0198, -0.0079,  ..., -0.0185, -0.0116,  0.0002],\n",
              "                      [ 0.0039,  0.0261, -0.0388,  ..., -0.0338, -0.0215,  0.0282]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.12.mlp.down_proj.weight',\n",
              "              tensor([[-0.0133,  0.0152, -0.0066,  ..., -0.0075,  0.0267,  0.0337],\n",
              "                      [-0.0274, -0.0021, -0.0064,  ..., -0.0158, -0.0221,  0.0377],\n",
              "                      [-0.0117, -0.0196, -0.0163,  ...,  0.0237,  0.0027, -0.0346],\n",
              "                      ...,\n",
              "                      [ 0.0088, -0.0179, -0.0257,  ...,  0.0065,  0.0065, -0.0217],\n",
              "                      [-0.0281,  0.0091,  0.0065,  ...,  0.0026, -0.0249,  0.0027],\n",
              "                      [ 0.0088, -0.0051,  0.0221,  ..., -0.0138,  0.0273,  0.0233]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.12.input_layernorm.weight',\n",
              "              tensor([0.3569, 0.3562, 0.3484,  ..., 0.3613, 0.3567, 0.3508],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.12.post_attention_layernorm.weight',\n",
              "              tensor([0.2322, 0.2355, 0.2321,  ..., 0.2284, 0.2394, 0.2172],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.13.self_attn.q_proj.weight',\n",
              "              tensor([[-0.0037, -0.0125, -0.0283,  ..., -0.0083,  0.0107,  0.0049],\n",
              "                      [-0.0127,  0.0031, -0.0004,  ..., -0.0085,  0.0165,  0.0061],\n",
              "                      [-0.0052, -0.0125,  0.0148,  ...,  0.0079, -0.0016, -0.0074],\n",
              "                      ...,\n",
              "                      [-0.0266, -0.0659, -0.0150,  ...,  0.0218, -0.0208,  0.0086],\n",
              "                      [-0.0090,  0.0562,  0.0035,  ...,  0.0319,  0.0350, -0.0215],\n",
              "                      [ 0.0291,  0.0076, -0.0177,  ..., -0.0209,  0.0084,  0.0081]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.13.self_attn.k_proj.weight',\n",
              "              tensor([[ 0.0184, -0.0153,  0.0010,  ..., -0.0072, -0.0134, -0.0008],\n",
              "                      [-0.0177,  0.0077,  0.0068,  ..., -0.0106, -0.0020, -0.0036],\n",
              "                      [-0.0152,  0.0147,  0.0062,  ...,  0.0236,  0.0097, -0.0001],\n",
              "                      ...,\n",
              "                      [ 0.0180, -0.0403,  0.0055,  ...,  0.0224,  0.0036, -0.0333],\n",
              "                      [ 0.0444, -0.0013, -0.0611,  ..., -0.0390,  0.0208, -0.0201],\n",
              "                      [ 0.0490,  0.0057,  0.0271,  ..., -0.0341,  0.0608,  0.0549]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.13.self_attn.v_proj.weight',\n",
              "              tensor([[ 1.1871e-02, -6.0081e-03, -5.0812e-03,  ..., -3.6583e-03,\n",
              "                        7.1297e-03, -4.8943e-03],\n",
              "                      [ 7.0610e-03,  2.0218e-02, -1.6809e-05,  ...,  4.6082e-03,\n",
              "                       -1.6846e-02, -4.2633e-02],\n",
              "                      [ 2.9068e-02,  1.0277e-02,  2.4063e-02,  ...,  1.7258e-02,\n",
              "                        8.9569e-03, -8.3847e-03],\n",
              "                      ...,\n",
              "                      [ 1.9623e-02,  2.8381e-02, -3.7354e-02,  ...,  8.8196e-03,\n",
              "                        7.4234e-03,  4.1542e-03],\n",
              "                      [-2.5597e-03,  4.8485e-03, -2.0035e-02,  ..., -6.1531e-03,\n",
              "                       -1.8066e-02,  4.5586e-03],\n",
              "                      [-6.4087e-03, -7.0343e-03, -1.4877e-03,  ..., -2.2690e-02,\n",
              "                       -1.2993e-02, -1.3565e-02]], dtype=torch.float16)),\n",
              "             ('model.model.layers.13.self_attn.o_proj.weight',\n",
              "              tensor([[-0.0080, -0.0105, -0.0057,  ...,  0.0017, -0.0019,  0.0085],\n",
              "                      [ 0.0189, -0.0127, -0.0117,  ..., -0.0062, -0.0073, -0.0072],\n",
              "                      [ 0.0134, -0.0302,  0.0068,  ...,  0.0219,  0.0424, -0.0108],\n",
              "                      ...,\n",
              "                      [-0.0199,  0.0102, -0.0021,  ...,  0.0153,  0.0251,  0.0243],\n",
              "                      [-0.0134,  0.0074, -0.0215,  ...,  0.0139, -0.0088, -0.0118],\n",
              "                      [-0.0076, -0.0324,  0.0023,  ..., -0.0115, -0.0030,  0.0207]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.13.mlp.gate_proj.weight',\n",
              "              tensor([[-0.0244, -0.0315,  0.0179,  ..., -0.0242,  0.0596, -0.0111],\n",
              "                      [-0.0025, -0.0597,  0.0039,  ..., -0.0233, -0.0105, -0.0114],\n",
              "                      [ 0.0037,  0.0273,  0.0551,  ...,  0.0185,  0.0019, -0.0428],\n",
              "                      ...,\n",
              "                      [-0.0256,  0.0417,  0.0027,  ..., -0.0096, -0.0149, -0.0350],\n",
              "                      [ 0.0252, -0.0031, -0.0220,  ...,  0.0035, -0.0080,  0.0025],\n",
              "                      [ 0.0122, -0.0016, -0.0250,  ...,  0.0041,  0.0016,  0.0037]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.13.mlp.up_proj.weight',\n",
              "              tensor([[-0.0104,  0.0160,  0.0044,  ...,  0.0072, -0.0108, -0.0020],\n",
              "                      [-0.0136, -0.0267, -0.0331,  ...,  0.0320,  0.0078,  0.0022],\n",
              "                      [-0.0001,  0.0003, -0.0025,  ..., -0.0425, -0.0349,  0.0144],\n",
              "                      ...,\n",
              "                      [-0.0082, -0.0213,  0.0246,  ...,  0.0234,  0.0377,  0.0021],\n",
              "                      [-0.0183, -0.0168,  0.0123,  ..., -0.0111, -0.0414, -0.0146],\n",
              "                      [-0.0008, -0.0074,  0.0152,  ...,  0.0044,  0.0178,  0.0091]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.13.mlp.down_proj.weight',\n",
              "              tensor([[ 0.0059,  0.0293,  0.0175,  ..., -0.0080, -0.0108, -0.0121],\n",
              "                      [-0.0036,  0.0327, -0.0291,  ..., -0.0042, -0.0306,  0.0097],\n",
              "                      [-0.0084,  0.0388, -0.0119,  ..., -0.0107,  0.0047, -0.0054],\n",
              "                      ...,\n",
              "                      [ 0.0148, -0.0101, -0.0281,  ..., -0.0163, -0.0181,  0.0358],\n",
              "                      [ 0.0125, -0.0376, -0.0327,  ..., -0.0228, -0.0108,  0.0040],\n",
              "                      [-0.0194,  0.0461,  0.0166,  ...,  0.0082, -0.0167,  0.0020]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.13.input_layernorm.weight',\n",
              "              tensor([0.3706, 0.3767, 0.3694,  ..., 0.3762, 0.3811, 0.3672],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.13.post_attention_layernorm.weight',\n",
              "              tensor([0.2410, 0.2407, 0.2424,  ..., 0.2362, 0.2466, 0.2294],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.14.self_attn.q_proj.weight',\n",
              "              tensor([[-0.0119,  0.0112, -0.0066,  ...,  0.0096, -0.0097,  0.0139],\n",
              "                      [ 0.0055,  0.0231,  0.0077,  ..., -0.0008, -0.0037,  0.0023],\n",
              "                      [-0.0077, -0.0014, -0.0076,  ..., -0.0005, -0.0157, -0.0038],\n",
              "                      ...,\n",
              "                      [-0.0484, -0.0551, -0.0671,  ...,  0.0581, -0.0064, -0.0608],\n",
              "                      [-0.0605,  0.0348,  0.0215,  ...,  0.1209,  0.0515, -0.0003],\n",
              "                      [ 0.0624,  0.0024,  0.0051,  ...,  0.0108, -0.0098, -0.0495]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.14.self_attn.k_proj.weight',\n",
              "              tensor([[-0.0046,  0.0057,  0.0018,  ..., -0.0080, -0.0076, -0.0231],\n",
              "                      [-0.0112,  0.0074,  0.0287,  ...,  0.0065, -0.0021,  0.0168],\n",
              "                      [-0.0136,  0.0048, -0.0161,  ...,  0.0140, -0.0049,  0.0066],\n",
              "                      ...,\n",
              "                      [ 0.0479, -0.0296, -0.0415,  ...,  0.0116, -0.0336, -0.0764],\n",
              "                      [-0.0005,  0.0173, -0.0075,  ..., -0.0048,  0.0039, -0.0464],\n",
              "                      [-0.0179,  0.0214, -0.0174,  ..., -0.0164, -0.0005,  0.0017]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.14.self_attn.v_proj.weight',\n",
              "              tensor([[-0.0143, -0.0234, -0.0156,  ..., -0.0035, -0.0096, -0.0109],\n",
              "                      [-0.0427,  0.0033, -0.0133,  ..., -0.0125, -0.0056,  0.0303],\n",
              "                      [ 0.0188,  0.0073, -0.0088,  ...,  0.0335,  0.0469,  0.0143],\n",
              "                      ...,\n",
              "                      [ 0.0196, -0.0256, -0.0045,  ..., -0.0028, -0.0017,  0.0165],\n",
              "                      [ 0.0169,  0.0215, -0.0009,  ..., -0.0248,  0.0051, -0.0013],\n",
              "                      [ 0.0297, -0.0061,  0.0260,  ...,  0.0128,  0.0244, -0.0166]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.14.self_attn.o_proj.weight',\n",
              "              tensor([[ 0.0382,  0.0012,  0.0292,  ...,  0.0107, -0.0005,  0.0035],\n",
              "                      [-0.0244, -0.0147, -0.0092,  ..., -0.0020,  0.0078,  0.0001],\n",
              "                      [-0.0076,  0.0021, -0.0012,  ...,  0.0151, -0.0073, -0.0021],\n",
              "                      ...,\n",
              "                      [ 0.0181, -0.0272, -0.0061,  ...,  0.0023, -0.0106, -0.0128],\n",
              "                      [-0.0116,  0.0341,  0.0389,  ..., -0.0130,  0.0115,  0.0075],\n",
              "                      [ 0.0224,  0.0083,  0.0164,  ...,  0.0048,  0.0118,  0.0092]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.14.mlp.gate_proj.weight',\n",
              "              tensor([[-0.0556,  0.0275, -0.0209,  ..., -0.0263,  0.0351,  0.0084],\n",
              "                      [ 0.0455,  0.0172,  0.0153,  ...,  0.0023,  0.0073, -0.0282],\n",
              "                      [ 0.0120,  0.0245, -0.0204,  ...,  0.0069, -0.0226, -0.0166],\n",
              "                      ...,\n",
              "                      [-0.0058,  0.0118,  0.0083,  ...,  0.0064, -0.0040,  0.0004],\n",
              "                      [-0.0167, -0.0031,  0.0003,  ..., -0.0184, -0.0246,  0.0902],\n",
              "                      [ 0.0062,  0.0111, -0.0008,  ...,  0.0552,  0.0432,  0.0028]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.14.mlp.up_proj.weight',\n",
              "              tensor([[ 0.0059,  0.0146, -0.0220,  ..., -0.0447, -0.0012, -0.0227],\n",
              "                      [ 0.0011, -0.0128, -0.0202,  ...,  0.0098, -0.0029, -0.0183],\n",
              "                      [-0.0127, -0.0086,  0.0335,  ...,  0.0244, -0.0177, -0.0074],\n",
              "                      ...,\n",
              "                      [ 0.0035, -0.0106, -0.0235,  ...,  0.0303, -0.0144, -0.0247],\n",
              "                      [ 0.0066, -0.0079,  0.0139,  ...,  0.0135, -0.0295,  0.0147],\n",
              "                      [ 0.0323,  0.0062, -0.0093,  ...,  0.0125, -0.0323, -0.0012]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.14.mlp.down_proj.weight',\n",
              "              tensor([[-0.0328,  0.0392,  0.0173,  ..., -0.0064,  0.0260,  0.0264],\n",
              "                      [ 0.0099,  0.0522,  0.0314,  ...,  0.0193,  0.0068,  0.0248],\n",
              "                      [-0.0293, -0.0076, -0.0023,  ...,  0.0291, -0.0069, -0.0097],\n",
              "                      ...,\n",
              "                      [ 0.0037,  0.0010, -0.0211,  ..., -0.0305, -0.0296,  0.0310],\n",
              "                      [-0.0108,  0.0090,  0.0135,  ...,  0.0060, -0.0038,  0.0018],\n",
              "                      [ 0.0165, -0.0094, -0.0241,  ...,  0.0377, -0.0142,  0.0212]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.14.input_layernorm.weight',\n",
              "              tensor([0.3608, 0.3594, 0.3579,  ..., 0.3801, 0.3721, 0.3574],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.14.post_attention_layernorm.weight',\n",
              "              tensor([0.2549, 0.2549, 0.2524,  ..., 0.2479, 0.2588, 0.2421],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.15.self_attn.q_proj.weight',\n",
              "              tensor([[ 0.0097,  0.0277, -0.0158,  ...,  0.0003, -0.0021,  0.0020],\n",
              "                      [ 0.0110, -0.0103, -0.0081,  ..., -0.0042,  0.0003, -0.0150],\n",
              "                      [-0.0088, -0.0091, -0.0360,  ...,  0.0170, -0.0083, -0.0108],\n",
              "                      ...,\n",
              "                      [-0.0105,  0.0254,  0.0028,  ..., -0.0177, -0.0334,  0.0421],\n",
              "                      [ 0.0075,  0.0362,  0.0132,  ...,  0.0220,  0.0067,  0.0115],\n",
              "                      [ 0.0387, -0.0307, -0.0230,  ...,  0.0657,  0.0175,  0.0527]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.15.self_attn.k_proj.weight',\n",
              "              tensor([[-0.0021, -0.0177,  0.0099,  ...,  0.0085,  0.0139, -0.0083],\n",
              "                      [-0.0136,  0.0246,  0.0116,  ..., -0.0012,  0.0187,  0.0111],\n",
              "                      [ 0.0107,  0.0092,  0.0079,  ..., -0.0145, -0.0024,  0.0028],\n",
              "                      ...,\n",
              "                      [-0.0117,  0.0108,  0.0299,  ...,  0.0132, -0.0181,  0.0293],\n",
              "                      [ 0.0269,  0.0141,  0.0002,  ..., -0.0196, -0.0488, -0.0171],\n",
              "                      [-0.0107, -0.0184,  0.0063,  ..., -0.0196, -0.0057,  0.0406]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.15.self_attn.v_proj.weight',\n",
              "              tensor([[-0.0060, -0.0317, -0.0056,  ...,  0.0414, -0.0106, -0.0065],\n",
              "                      [-0.0082,  0.0157,  0.0209,  ..., -0.0339, -0.0105, -0.0048],\n",
              "                      [ 0.0141, -0.0016,  0.0368,  ..., -0.0069, -0.0031, -0.0150],\n",
              "                      ...,\n",
              "                      [ 0.0022, -0.0060,  0.0184,  ...,  0.0013, -0.0169,  0.0376],\n",
              "                      [-0.0110,  0.0239,  0.0148,  ...,  0.0041, -0.0130,  0.0029],\n",
              "                      [ 0.0013, -0.0043,  0.0009,  ...,  0.0210,  0.0081, -0.0058]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.15.self_attn.o_proj.weight',\n",
              "              tensor([[ 0.0216,  0.0162, -0.0094,  ..., -0.0053, -0.0221, -0.0150],\n",
              "                      [ 0.0087,  0.0327, -0.0116,  ...,  0.0002, -0.0028,  0.0122],\n",
              "                      [-0.0159,  0.0183,  0.0327,  ...,  0.0024,  0.0163, -0.0059],\n",
              "                      ...,\n",
              "                      [-0.0197, -0.0025, -0.0081,  ..., -0.0047, -0.0152,  0.0193],\n",
              "                      [ 0.0094, -0.0212,  0.0139,  ...,  0.0209, -0.0003,  0.0189],\n",
              "                      [ 0.0221, -0.0110,  0.0016,  ..., -0.0178,  0.0193,  0.0138]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.15.mlp.gate_proj.weight',\n",
              "              tensor([[ 0.0137,  0.0080,  0.0193,  ...,  0.0128,  0.0060,  0.0025],\n",
              "                      [ 0.0072,  0.0005, -0.0114,  ..., -0.0468, -0.0302, -0.0317],\n",
              "                      [ 0.0481, -0.0151, -0.0127,  ..., -0.0050,  0.0074, -0.0281],\n",
              "                      ...,\n",
              "                      [-0.0005,  0.0021, -0.0132,  ..., -0.0122,  0.0003, -0.0498],\n",
              "                      [-0.0084,  0.0472, -0.0042,  ..., -0.0167, -0.0262,  0.0013],\n",
              "                      [-0.0193,  0.0153, -0.0479,  ...,  0.0141,  0.0010,  0.0039]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.15.mlp.up_proj.weight',\n",
              "              tensor([[-0.0101,  0.0064, -0.0265,  ...,  0.0351,  0.0227, -0.0300],\n",
              "                      [ 0.0089,  0.0255, -0.0255,  ..., -0.0529, -0.0034, -0.0281],\n",
              "                      [ 0.0098, -0.0132,  0.0235,  ..., -0.0388, -0.0065, -0.0140],\n",
              "                      ...,\n",
              "                      [ 0.0060,  0.0073, -0.0007,  ..., -0.0029, -0.0023, -0.0217],\n",
              "                      [ 0.0162,  0.0269,  0.0210,  ..., -0.0270,  0.0149,  0.0107],\n",
              "                      [ 0.0088, -0.0189,  0.0127,  ..., -0.0217, -0.0170,  0.0125]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.15.mlp.down_proj.weight',\n",
              "              tensor([[ 0.0283,  0.0185,  0.0074,  ..., -0.0046,  0.0489, -0.0136],\n",
              "                      [-0.0058,  0.0116, -0.0185,  ..., -0.0069, -0.0030, -0.0181],\n",
              "                      [ 0.0213,  0.0065,  0.0451,  ...,  0.0041,  0.0207, -0.0057],\n",
              "                      ...,\n",
              "                      [-0.0111,  0.0168,  0.0232,  ..., -0.0216,  0.0106, -0.0151],\n",
              "                      [-0.0237,  0.0278, -0.0034,  ...,  0.0191, -0.0044, -0.0137],\n",
              "                      [ 0.0217,  0.0109,  0.0163,  ...,  0.0249, -0.0159, -0.0161]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.15.input_layernorm.weight',\n",
              "              tensor([0.3601, 0.3635, 0.3560,  ..., 0.3748, 0.3765, 0.3691],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.15.post_attention_layernorm.weight',\n",
              "              tensor([0.2622, 0.2668, 0.2598,  ..., 0.2549, 0.2683, 0.2534],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.16.self_attn.q_proj.weight',\n",
              "              tensor([[ 0.0250, -0.0022, -0.0166,  ...,  0.0088, -0.0059,  0.0159],\n",
              "                      [ 0.0272,  0.0135,  0.0185,  ..., -0.0151, -0.0056, -0.0053],\n",
              "                      [-0.0016, -0.0004,  0.0124,  ..., -0.0186,  0.0187,  0.0088],\n",
              "                      ...,\n",
              "                      [ 0.0142, -0.0241,  0.0370,  ..., -0.0757,  0.0145, -0.0268],\n",
              "                      [-0.0551,  0.0396, -0.0224,  ..., -0.0217,  0.0012,  0.0374],\n",
              "                      [-0.0038,  0.0054,  0.0078,  ...,  0.0345,  0.0461,  0.0313]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.16.self_attn.k_proj.weight',\n",
              "              tensor([[ 0.0075, -0.0204,  0.0203,  ...,  0.0071, -0.0021,  0.0016],\n",
              "                      [-0.0047,  0.0188, -0.0155,  ..., -0.0014,  0.0159,  0.0207],\n",
              "                      [-0.0061,  0.0229,  0.0348,  ..., -0.0029, -0.0165,  0.0017],\n",
              "                      ...,\n",
              "                      [ 0.0269, -0.0255,  0.0433,  ..., -0.0357,  0.0182,  0.0231],\n",
              "                      [-0.0207,  0.0344,  0.0371,  ...,  0.0355, -0.0124,  0.0396],\n",
              "                      [-0.0343,  0.0075,  0.0082,  ...,  0.0071,  0.0042, -0.0313]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.16.self_attn.v_proj.weight',\n",
              "              tensor([[ 0.0072, -0.0486, -0.0298,  ..., -0.0271,  0.0129,  0.0377],\n",
              "                      [ 0.0085, -0.0016, -0.0175,  ..., -0.0217, -0.0334, -0.0068],\n",
              "                      [-0.0020,  0.0416,  0.0012,  ...,  0.0340, -0.0076, -0.0081],\n",
              "                      ...,\n",
              "                      [ 0.0035, -0.0152, -0.0077,  ...,  0.0269,  0.0259,  0.0355],\n",
              "                      [-0.0033,  0.0158,  0.0102,  ..., -0.0153,  0.0035,  0.0066],\n",
              "                      [ 0.0171, -0.0141,  0.0032,  ..., -0.0005,  0.0130,  0.0109]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.16.self_attn.o_proj.weight',\n",
              "              tensor([[-0.0193,  0.0124,  0.0292,  ..., -0.0079, -0.0063, -0.0388],\n",
              "                      [-0.0342, -0.0067, -0.0174,  ...,  0.0129, -0.0326, -0.0131],\n",
              "                      [ 0.0101,  0.0097, -0.0063,  ..., -0.0086, -0.0142,  0.0035],\n",
              "                      ...,\n",
              "                      [-0.0239, -0.0072,  0.0522,  ..., -0.0238, -0.0019, -0.0314],\n",
              "                      [ 0.0512, -0.0061,  0.0143,  ..., -0.0052, -0.0286, -0.0078],\n",
              "                      [ 0.0166, -0.0017,  0.0045,  ..., -0.0459, -0.0066, -0.0149]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.16.mlp.gate_proj.weight',\n",
              "              tensor([[-0.0031,  0.0077,  0.0091,  ..., -0.0026,  0.0306, -0.0142],\n",
              "                      [-0.0028,  0.0314,  0.0235,  ...,  0.0082,  0.0443, -0.0094],\n",
              "                      [ 0.0048,  0.0228,  0.0141,  ...,  0.0236,  0.0149, -0.0084],\n",
              "                      ...,\n",
              "                      [-0.0107, -0.0376, -0.0062,  ..., -0.0294,  0.0301, -0.0101],\n",
              "                      [ 0.0066, -0.0311,  0.0213,  ..., -0.0156,  0.0114,  0.0014],\n",
              "                      [-0.0265,  0.0144,  0.0072,  ...,  0.0424, -0.0020,  0.0150]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.16.mlp.up_proj.weight',\n",
              "              tensor([[-0.0236,  0.0232, -0.0365,  ...,  0.0188, -0.0030, -0.0260],\n",
              "                      [-0.0025,  0.0272,  0.0463,  ..., -0.0058, -0.0426,  0.0096],\n",
              "                      [-0.0086, -0.0053, -0.0217,  ...,  0.0088, -0.0018,  0.0696],\n",
              "                      ...,\n",
              "                      [ 0.0512,  0.0510, -0.0338,  ...,  0.0003,  0.0134, -0.0066],\n",
              "                      [-0.0076,  0.0043,  0.0098,  ...,  0.0261, -0.0389,  0.0013],\n",
              "                      [-0.0197,  0.0003, -0.0124,  ..., -0.0287,  0.0394,  0.0222]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.16.mlp.down_proj.weight',\n",
              "              tensor([[ 0.0242,  0.0143, -0.0476,  ...,  0.0057, -0.0163, -0.0261],\n",
              "                      [ 0.0387,  0.0095, -0.0138,  ...,  0.0370, -0.0093,  0.0026],\n",
              "                      [-0.0233,  0.0058,  0.0320,  ..., -0.0044,  0.0255, -0.0187],\n",
              "                      ...,\n",
              "                      [ 0.0309,  0.0053,  0.0133,  ...,  0.0433, -0.0140, -0.0132],\n",
              "                      [ 0.0205,  0.0100, -0.0065,  ...,  0.0366,  0.0086,  0.0096],\n",
              "                      [ 0.0037, -0.0165,  0.0013,  ..., -0.0065, -0.0151, -0.0207]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.16.input_layernorm.weight',\n",
              "              tensor([0.3591, 0.3535, 0.3506,  ..., 0.3672, 0.3691, 0.3650],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.16.post_attention_layernorm.weight',\n",
              "              tensor([0.2766, 0.2820, 0.2834,  ..., 0.2781, 0.2800, 0.2698],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.17.self_attn.q_proj.weight',\n",
              "              tensor([[-0.0195, -0.0028, -0.0006,  ...,  0.0057, -0.0291,  0.0203],\n",
              "                      [ 0.0018,  0.0107,  0.0092,  ...,  0.0134,  0.0276, -0.0083],\n",
              "                      [-0.0301, -0.0005,  0.0098,  ...,  0.0130, -0.0111, -0.0125],\n",
              "                      ...,\n",
              "                      [-0.0037, -0.0056,  0.0506,  ..., -0.0070,  0.0134,  0.0010],\n",
              "                      [ 0.0147,  0.0364,  0.0123,  ...,  0.0358,  0.0183,  0.0753],\n",
              "                      [-0.0053, -0.0140,  0.0211,  ..., -0.0328, -0.0152, -0.0005]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.17.self_attn.k_proj.weight',\n",
              "              tensor([[-2.8290e-02,  1.0963e-02,  3.7079e-03,  ..., -2.6657e-02,\n",
              "                       -2.5299e-02, -1.9348e-02],\n",
              "                      [-7.4997e-03, -1.9470e-02, -6.0320e-05,  ...,  2.0554e-02,\n",
              "                        2.1332e-02, -2.8564e-02],\n",
              "                      [ 7.7095e-03,  1.5190e-02, -3.8940e-02,  ..., -2.8152e-02,\n",
              "                       -9.0933e-04,  1.0391e-02],\n",
              "                      ...,\n",
              "                      [-1.6556e-02,  1.9363e-02,  1.1948e-02,  ..., -1.6937e-02,\n",
              "                       -1.8906e-02, -2.6932e-03],\n",
              "                      [ 2.9564e-03,  1.8600e-02, -7.2784e-03,  ...,  4.4678e-02,\n",
              "                        1.3893e-02,  2.3804e-02],\n",
              "                      [-3.3478e-02, -7.0915e-03,  2.0752e-02,  ..., -7.1526e-03,\n",
              "                       -3.8818e-02, -1.8219e-02]], dtype=torch.float16)),\n",
              "             ('model.model.layers.17.self_attn.v_proj.weight',\n",
              "              tensor([[-0.0030,  0.0149,  0.0686,  ..., -0.0127, -0.0346,  0.0120],\n",
              "                      [-0.0002, -0.0278,  0.0257,  ..., -0.0139, -0.0037, -0.0058],\n",
              "                      [-0.0104, -0.0058, -0.0014,  ...,  0.0054,  0.0028, -0.0075],\n",
              "                      ...,\n",
              "                      [ 0.0052,  0.0660,  0.0281,  ...,  0.0261,  0.0241, -0.0076],\n",
              "                      [ 0.0258,  0.0754, -0.0233,  ...,  0.0224,  0.0338,  0.0023],\n",
              "                      [-0.0046,  0.0062,  0.0135,  ...,  0.0078,  0.0330, -0.0007]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.17.self_attn.o_proj.weight',\n",
              "              tensor([[ 0.0104,  0.0305, -0.0098,  ..., -0.0150, -0.0275, -0.0037],\n",
              "                      [-0.0223,  0.0101,  0.0034,  ...,  0.0215,  0.0155, -0.0001],\n",
              "                      [-0.0243, -0.0207, -0.0120,  ..., -0.0170,  0.0201,  0.0097],\n",
              "                      ...,\n",
              "                      [-0.0038,  0.0085, -0.0010,  ..., -0.0402, -0.0036,  0.0120],\n",
              "                      [ 0.0028, -0.0031,  0.0237,  ...,  0.0137, -0.0282, -0.0011],\n",
              "                      [-0.0160, -0.0274,  0.0182,  ..., -0.0113, -0.0051,  0.0095]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.17.mlp.gate_proj.weight',\n",
              "              tensor([[ 0.0341,  0.0300, -0.0179,  ...,  0.0106,  0.0149, -0.0007],\n",
              "                      [ 0.0006, -0.0062,  0.0122,  ..., -0.0475, -0.0377,  0.0007],\n",
              "                      [ 0.0367,  0.0250,  0.0030,  ..., -0.0057,  0.0370,  0.0336],\n",
              "                      ...,\n",
              "                      [-0.0139,  0.0153, -0.0214,  ..., -0.0054, -0.0445,  0.0098],\n",
              "                      [ 0.0404,  0.0264,  0.0597,  ...,  0.0054, -0.0320,  0.0310],\n",
              "                      [-0.0145, -0.0034,  0.0003,  ..., -0.0089, -0.0067,  0.0280]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.17.mlp.up_proj.weight',\n",
              "              tensor([[-0.0233, -0.0159, -0.0248,  ...,  0.0326,  0.0306, -0.0433],\n",
              "                      [-0.0274, -0.0089, -0.0178,  ...,  0.0025,  0.0042,  0.0169],\n",
              "                      [ 0.0002,  0.0299,  0.0265,  ...,  0.0134, -0.0074,  0.0239],\n",
              "                      ...,\n",
              "                      [-0.0058, -0.0062,  0.0149,  ...,  0.0153, -0.0421, -0.0432],\n",
              "                      [-0.0108, -0.0359, -0.0043,  ...,  0.0120, -0.0479,  0.0178],\n",
              "                      [-0.0006,  0.0048, -0.0095,  ..., -0.0264,  0.0394,  0.0047]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.17.mlp.down_proj.weight',\n",
              "              tensor([[-0.0287, -0.0023,  0.0116,  ...,  0.0012,  0.0104, -0.0213],\n",
              "                      [-0.0401,  0.0250, -0.0021,  ...,  0.0074,  0.0198, -0.0201],\n",
              "                      [-0.0017, -0.0126, -0.0057,  ...,  0.0531,  0.0008, -0.0256],\n",
              "                      ...,\n",
              "                      [ 0.0151,  0.0168,  0.0524,  ..., -0.0181, -0.0128,  0.0585],\n",
              "                      [-0.0005,  0.0016, -0.0551,  ...,  0.0465, -0.0275,  0.0277],\n",
              "                      [-0.0480, -0.0396,  0.0086,  ..., -0.0113,  0.0147,  0.0216]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.17.input_layernorm.weight',\n",
              "              tensor([0.3918, 0.3877, 0.3784,  ..., 0.3838, 0.3928, 0.3865],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.17.post_attention_layernorm.weight',\n",
              "              tensor([0.2910, 0.2957, 0.2983,  ..., 0.2871, 0.2979, 0.2876],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.18.self_attn.q_proj.weight',\n",
              "              tensor([[-5.3215e-04, -3.5400e-03, -4.4594e-03,  ...,  1.0788e-02,\n",
              "                       -1.1330e-02, -1.3725e-02],\n",
              "                      [-7.2098e-03,  2.7039e-02,  5.5199e-03,  ...,  3.2902e-03,\n",
              "                       -7.7426e-05, -9.8190e-03],\n",
              "                      [ 2.4231e-02,  3.1219e-02,  1.1993e-02,  ...,  5.9319e-03,\n",
              "                       -2.9011e-03,  3.0273e-02],\n",
              "                      ...,\n",
              "                      [-7.4097e-02, -1.7197e-02,  9.0714e-03,  ..., -4.0924e-02,\n",
              "                       -1.0185e-03, -2.4780e-02],\n",
              "                      [-2.1637e-02, -1.6613e-03, -2.0493e-02,  ..., -1.8482e-03,\n",
              "                       -3.2776e-02,  1.8158e-02],\n",
              "                      [-5.2643e-02, -4.7913e-02, -1.2274e-01,  ..., -5.2185e-02,\n",
              "                       -1.3023e-02,  2.4261e-03]], dtype=torch.float16)),\n",
              "             ('model.model.layers.18.self_attn.k_proj.weight',\n",
              "              tensor([[-0.0193,  0.0047,  0.0153,  ...,  0.0213,  0.0039,  0.0127],\n",
              "                      [-0.0210, -0.0153,  0.0184,  ..., -0.0021, -0.0075,  0.0403],\n",
              "                      [ 0.0086, -0.0121,  0.0081,  ...,  0.0004,  0.0001,  0.0190],\n",
              "                      ...,\n",
              "                      [-0.0753, -0.0185, -0.0087,  ...,  0.0003, -0.0327,  0.0179],\n",
              "                      [-0.0179, -0.0306, -0.0249,  ..., -0.0229,  0.0124,  0.0203],\n",
              "                      [-0.0660, -0.0360, -0.0382,  ..., -0.0185,  0.0279, -0.0130]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.18.self_attn.v_proj.weight',\n",
              "              tensor([[ 0.0385, -0.0036,  0.0450,  ..., -0.0239,  0.0070,  0.0382],\n",
              "                      [ 0.0375, -0.0285,  0.0428,  ...,  0.0091,  0.0010,  0.0099],\n",
              "                      [ 0.0072,  0.0062,  0.0167,  ..., -0.0280,  0.0171,  0.0001],\n",
              "                      ...,\n",
              "                      [ 0.0266,  0.0014, -0.0236,  ...,  0.0273, -0.0160, -0.0104],\n",
              "                      [ 0.0312,  0.0349, -0.0039,  ..., -0.0172,  0.0110, -0.0472],\n",
              "                      [ 0.0439,  0.0247, -0.0299,  ..., -0.0013, -0.0025,  0.0173]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.18.self_attn.o_proj.weight',\n",
              "              tensor([[ 0.0351,  0.0143,  0.0188,  ...,  0.0139,  0.0160, -0.0008],\n",
              "                      [ 0.0109, -0.0034, -0.0126,  ...,  0.0177, -0.0239, -0.0124],\n",
              "                      [ 0.0433, -0.0034, -0.0207,  ..., -0.0130,  0.0153, -0.0093],\n",
              "                      ...,\n",
              "                      [-0.0257,  0.0185, -0.0299,  ..., -0.0159,  0.0128,  0.0019],\n",
              "                      [-0.0233,  0.0374,  0.0011,  ...,  0.0226, -0.0072, -0.0071],\n",
              "                      [ 0.0235,  0.0071,  0.0134,  ..., -0.0218,  0.0008,  0.0042]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.18.mlp.gate_proj.weight',\n",
              "              tensor([[-0.0170,  0.0314,  0.0340,  ...,  0.0186, -0.0184,  0.0147],\n",
              "                      [-0.0481,  0.0176, -0.0035,  ...,  0.0308, -0.0158,  0.0157],\n",
              "                      [ 0.0024, -0.0030,  0.0515,  ..., -0.0166,  0.0039,  0.0095],\n",
              "                      ...,\n",
              "                      [-0.0089,  0.0090,  0.0098,  ...,  0.0177,  0.0101, -0.0039],\n",
              "                      [ 0.0189,  0.0126,  0.0258,  ...,  0.0066,  0.0158,  0.0163],\n",
              "                      [-0.0074,  0.0166, -0.0306,  ...,  0.0235, -0.0173, -0.0056]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.18.mlp.up_proj.weight',\n",
              "              tensor([[ 4.7485e-02,  1.6052e-02,  8.4152e-03,  ..., -1.9741e-03,\n",
              "                        4.4647e-02, -2.4704e-02],\n",
              "                      [ 1.9028e-02,  2.4902e-02,  1.9608e-02,  ...,  3.2318e-02,\n",
              "                       -7.5378e-03, -2.7344e-02],\n",
              "                      [-4.7874e-03,  1.0468e-02,  1.5717e-02,  ..., -1.0204e-03,\n",
              "                       -8.5602e-03,  3.5309e-02],\n",
              "                      ...,\n",
              "                      [ 4.8462e-02, -1.2703e-02, -1.2955e-02,  ...,  6.3591e-03,\n",
              "                       -3.0251e-03,  6.3057e-03],\n",
              "                      [ 4.1901e-02,  6.4507e-03,  1.1894e-02,  ...,  1.0689e-02,\n",
              "                       -9.4910e-03, -2.0035e-02],\n",
              "                      [-1.4664e-02, -2.0752e-02,  2.3621e-02,  ...,  6.9809e-03,\n",
              "                        5.9738e-03,  6.8963e-05]], dtype=torch.float16)),\n",
              "             ('model.model.layers.18.mlp.down_proj.weight',\n",
              "              tensor([[-0.0038, -0.0143, -0.0087,  ..., -0.0432,  0.0059, -0.0361],\n",
              "                      [-0.0249, -0.0273, -0.0338,  ..., -0.0010, -0.0105,  0.0150],\n",
              "                      [-0.0019,  0.0063,  0.0071,  ..., -0.0048,  0.0278,  0.0044],\n",
              "                      ...,\n",
              "                      [-0.0174, -0.0238,  0.0124,  ...,  0.0026, -0.0081,  0.0086],\n",
              "                      [ 0.0091,  0.0202, -0.0078,  ..., -0.0070, -0.0270, -0.0250],\n",
              "                      [ 0.0031, -0.0071,  0.0511,  ...,  0.0010, -0.0383,  0.0396]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.18.input_layernorm.weight',\n",
              "              tensor([0.3940, 0.3950, 0.3828,  ..., 0.4023, 0.4011, 0.4021],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.18.post_attention_layernorm.weight',\n",
              "              tensor([0.3142, 0.3142, 0.3208,  ..., 0.3044, 0.3169, 0.3044],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.19.self_attn.q_proj.weight',\n",
              "              tensor([[ 0.0033,  0.0113,  0.0181,  ...,  0.0169,  0.0493, -0.0044],\n",
              "                      [ 0.0402,  0.0118, -0.0093,  ..., -0.0080,  0.0076,  0.0020],\n",
              "                      [ 0.0180,  0.0345, -0.0180,  ...,  0.0420, -0.0138,  0.0027],\n",
              "                      ...,\n",
              "                      [ 0.0724,  0.0354,  0.0305,  ...,  0.0015, -0.0057,  0.0445],\n",
              "                      [-0.0130,  0.0177,  0.0003,  ..., -0.0553, -0.0133,  0.0045],\n",
              "                      [ 0.0279,  0.0267, -0.0176,  ..., -0.0047, -0.0453,  0.0402]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.19.self_attn.k_proj.weight',\n",
              "              tensor([[-2.6703e-03, -6.7253e-03, -1.4648e-02,  ..., -3.8574e-02,\n",
              "                        1.1925e-02,  2.0142e-03],\n",
              "                      [ 1.9836e-02,  2.5902e-03,  2.5757e-02,  ...,  1.4442e-02,\n",
              "                        8.9340e-03,  1.8482e-03],\n",
              "                      [ 3.8239e-02, -2.8076e-02,  2.5444e-03,  ...,  2.4834e-03,\n",
              "                       -2.2766e-02,  7.8082e-06],\n",
              "                      ...,\n",
              "                      [ 5.0507e-03,  9.1675e-02,  2.1652e-02,  ...,  1.6327e-02,\n",
              "                       -4.4525e-02, -3.5583e-02],\n",
              "                      [-4.0793e-04,  1.0941e-02, -3.6835e-02,  ..., -6.2317e-02,\n",
              "                        2.7039e-02, -3.5126e-02],\n",
              "                      [ 6.9519e-02,  1.5793e-02,  2.1500e-02,  ...,  6.0150e-02,\n",
              "                       -6.1157e-02, -1.0994e-02]], dtype=torch.float16)),\n",
              "             ('model.model.layers.19.self_attn.v_proj.weight',\n",
              "              tensor([[-0.0034, -0.0197, -0.0392,  ...,  0.0232, -0.0331,  0.0441],\n",
              "                      [ 0.0116,  0.0104, -0.0067,  ...,  0.0333, -0.0032,  0.0052],\n",
              "                      [ 0.0110, -0.0347, -0.0339,  ...,  0.0069, -0.0109, -0.0140],\n",
              "                      ...,\n",
              "                      [ 0.0022, -0.0091,  0.0105,  ...,  0.0055,  0.0047, -0.0069],\n",
              "                      [ 0.0406,  0.0257, -0.0067,  ...,  0.0043, -0.0204, -0.0164],\n",
              "                      [ 0.0285, -0.0015,  0.0356,  ..., -0.0195, -0.0178, -0.0146]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.19.self_attn.o_proj.weight',\n",
              "              tensor([[ 0.0133, -0.0200,  0.0329,  ..., -0.0286,  0.0269,  0.0070],\n",
              "                      [ 0.0363, -0.0039, -0.0298,  ..., -0.0119,  0.0365, -0.0190],\n",
              "                      [-0.0338,  0.0053, -0.0065,  ...,  0.0433, -0.0038,  0.0273],\n",
              "                      ...,\n",
              "                      [ 0.0180,  0.0165, -0.0045,  ...,  0.0055,  0.0093,  0.0076],\n",
              "                      [ 0.0096, -0.0090,  0.0442,  ...,  0.0054, -0.0289,  0.0024],\n",
              "                      [ 0.0263,  0.0108, -0.0011,  ..., -0.0232, -0.0247, -0.0096]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.19.mlp.gate_proj.weight',\n",
              "              tensor([[-0.0075,  0.0183, -0.0153,  ..., -0.0410, -0.0137, -0.0205],\n",
              "                      [-0.0386, -0.0199,  0.0084,  ..., -0.0084,  0.0102,  0.0197],\n",
              "                      [ 0.0004, -0.0115,  0.0355,  ..., -0.0100,  0.0018, -0.0017],\n",
              "                      ...,\n",
              "                      [ 0.0043,  0.0001, -0.0105,  ..., -0.0077,  0.0136, -0.0024],\n",
              "                      [-0.0048, -0.0122,  0.0163,  ..., -0.0085,  0.0354, -0.0029],\n",
              "                      [ 0.0267, -0.0106, -0.0285,  ...,  0.0092,  0.0053,  0.0291]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.19.mlp.up_proj.weight',\n",
              "              tensor([[ 0.0021,  0.0401, -0.0438,  ...,  0.0107, -0.0046,  0.0127],\n",
              "                      [ 0.0075,  0.0150, -0.0126,  ..., -0.0135, -0.0126, -0.0256],\n",
              "                      [ 0.0078, -0.0009,  0.0529,  ...,  0.0091, -0.0177,  0.0211],\n",
              "                      ...,\n",
              "                      [-0.0341, -0.0111, -0.0088,  ..., -0.0236,  0.0144,  0.0089],\n",
              "                      [ 0.0003, -0.0071,  0.0253,  ...,  0.0013,  0.0012, -0.0300],\n",
              "                      [ 0.0192,  0.0446,  0.0041,  ...,  0.0115, -0.0300,  0.0041]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.19.mlp.down_proj.weight',\n",
              "              tensor([[ 0.0224,  0.0209,  0.0166,  ..., -0.0057, -0.0108, -0.0143],\n",
              "                      [ 0.0274,  0.0166, -0.0059,  ..., -0.0127,  0.0089,  0.0021],\n",
              "                      [ 0.0128,  0.0158, -0.0047,  ...,  0.0084, -0.0140,  0.0119],\n",
              "                      ...,\n",
              "                      [ 0.0451,  0.0053, -0.0072,  ...,  0.0105,  0.0172,  0.0049],\n",
              "                      [-0.0107, -0.0246, -0.0278,  ...,  0.0119, -0.0008, -0.0388],\n",
              "                      [-0.0410, -0.0149,  0.0318,  ...,  0.0197,  0.0061,  0.0280]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.19.input_layernorm.weight',\n",
              "              tensor([0.3887, 0.3738, 0.3767,  ..., 0.3843, 0.3945, 0.3994],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.19.post_attention_layernorm.weight',\n",
              "              tensor([0.3259, 0.3276, 0.3311,  ..., 0.3176, 0.3308, 0.3225],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.20.self_attn.q_proj.weight',\n",
              "              tensor([[ 0.0161,  0.0126,  0.0013,  ..., -0.0084,  0.0010,  0.0102],\n",
              "                      [-0.0144,  0.0059, -0.0132,  ..., -0.0128,  0.0123,  0.0157],\n",
              "                      [-0.0094, -0.0052, -0.0035,  ...,  0.0214,  0.0253,  0.0296],\n",
              "                      ...,\n",
              "                      [ 0.0024,  0.0443, -0.0127,  ...,  0.0172,  0.0460, -0.0146],\n",
              "                      [ 0.0327, -0.0250,  0.0624,  ...,  0.0718, -0.0098, -0.0003],\n",
              "                      [ 0.0171, -0.0389,  0.0160,  ...,  0.0105,  0.0106, -0.0048]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.20.self_attn.k_proj.weight',\n",
              "              tensor([[-0.0194,  0.0190,  0.0007,  ...,  0.0019,  0.0040,  0.0135],\n",
              "                      [-0.0065,  0.0110, -0.0146,  ...,  0.0163,  0.0065, -0.0103],\n",
              "                      [-0.0091, -0.0034,  0.0045,  ...,  0.0096,  0.0187,  0.0147],\n",
              "                      ...,\n",
              "                      [ 0.0021, -0.0115,  0.0246,  ..., -0.0226, -0.0340, -0.0060],\n",
              "                      [ 0.0610, -0.0189,  0.0049,  ...,  0.0646,  0.0253,  0.0446],\n",
              "                      [ 0.0202,  0.0289, -0.0496,  ..., -0.0143, -0.0169,  0.0145]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.20.self_attn.v_proj.weight',\n",
              "              tensor([[-0.0104,  0.0406, -0.0216,  ...,  0.0088,  0.0301, -0.0145],\n",
              "                      [ 0.0203, -0.0584,  0.0003,  ..., -0.0251, -0.0151, -0.0042],\n",
              "                      [ 0.0089, -0.0448, -0.0060,  ..., -0.0126,  0.0372,  0.0336],\n",
              "                      ...,\n",
              "                      [ 0.0370,  0.0357,  0.0344,  ..., -0.0053,  0.0028, -0.0420],\n",
              "                      [-0.0117, -0.0449, -0.0260,  ...,  0.0101, -0.0037, -0.0202],\n",
              "                      [ 0.0301,  0.0004, -0.0440,  ..., -0.0079, -0.0166, -0.0103]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.20.self_attn.o_proj.weight',\n",
              "              tensor([[-0.0006,  0.0040, -0.0220,  ...,  0.0653,  0.0163,  0.0034],\n",
              "                      [ 0.0370, -0.0164, -0.0204,  ..., -0.0036,  0.0108,  0.0099],\n",
              "                      [-0.0220, -0.0162,  0.0094,  ...,  0.0261, -0.0061, -0.0143],\n",
              "                      ...,\n",
              "                      [-0.0014,  0.0039, -0.0029,  ..., -0.0179,  0.0135, -0.0544],\n",
              "                      [ 0.0305, -0.0285,  0.0133,  ..., -0.0050, -0.0190, -0.0476],\n",
              "                      [-0.0382,  0.0061,  0.0131,  ..., -0.0111, -0.0325, -0.0016]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.20.mlp.gate_proj.weight',\n",
              "              tensor([[-0.0200,  0.0167,  0.0118,  ...,  0.0268, -0.0301, -0.0012],\n",
              "                      [-0.0220, -0.0158,  0.0170,  ...,  0.0218, -0.0439, -0.0252],\n",
              "                      [-0.0461, -0.0398, -0.0046,  ..., -0.0067, -0.0164, -0.0153],\n",
              "                      ...,\n",
              "                      [ 0.0153, -0.0253, -0.0229,  ...,  0.0112,  0.0147, -0.0206],\n",
              "                      [-0.0151, -0.0006,  0.0249,  ..., -0.0196, -0.0871, -0.0064],\n",
              "                      [ 0.0045, -0.0265, -0.0113,  ...,  0.0015,  0.0007, -0.0067]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.20.mlp.up_proj.weight',\n",
              "              tensor([[ 0.0191, -0.0102, -0.0086,  ...,  0.0079, -0.0270,  0.0109],\n",
              "                      [ 0.0288,  0.0367, -0.0469,  ...,  0.0161, -0.0043,  0.0091],\n",
              "                      [-0.0055, -0.0560, -0.0237,  ...,  0.0117, -0.0153, -0.0102],\n",
              "                      ...,\n",
              "                      [-0.0166, -0.0105, -0.0109,  ...,  0.0008,  0.0418, -0.0445],\n",
              "                      [ 0.0080, -0.0098, -0.0103,  ...,  0.0083,  0.0105, -0.0469],\n",
              "                      [-0.0364,  0.0201,  0.0026,  ...,  0.0151, -0.0137,  0.0158]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.20.mlp.down_proj.weight',\n",
              "              tensor([[ 0.0220,  0.0491, -0.0326,  ...,  0.0008, -0.0025,  0.0426],\n",
              "                      [-0.0122,  0.0149,  0.0222,  ..., -0.0455,  0.0005, -0.0185],\n",
              "                      [-0.0363, -0.0248, -0.0024,  ..., -0.0345, -0.0163, -0.0360],\n",
              "                      ...,\n",
              "                      [-0.0062, -0.0295,  0.0306,  ...,  0.0061, -0.0316,  0.0521],\n",
              "                      [ 0.0396,  0.0486, -0.0384,  ..., -0.0290, -0.0116, -0.0089],\n",
              "                      [ 0.0387, -0.0046,  0.0032,  ...,  0.0237, -0.0229,  0.0025]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.20.input_layernorm.weight',\n",
              "              tensor([0.4072, 0.4175, 0.4014,  ..., 0.4075, 0.4180, 0.4243],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.20.post_attention_layernorm.weight',\n",
              "              tensor([0.3391, 0.3416, 0.3411,  ..., 0.3291, 0.3394, 0.3411],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.21.self_attn.q_proj.weight',\n",
              "              tensor([[-2.7069e-02, -1.7456e-02,  2.6688e-02,  ..., -1.4496e-02,\n",
              "                       -1.5152e-02, -1.9217e-03],\n",
              "                      [ 1.6098e-02, -5.5351e-03, -5.4626e-03,  ..., -1.2344e-02,\n",
              "                       -1.3489e-02, -6.8169e-03],\n",
              "                      [-1.4832e-02, -1.6769e-02, -9.6202e-05,  ..., -6.3324e-03,\n",
              "                       -2.7512e-02, -1.2886e-02],\n",
              "                      ...,\n",
              "                      [-8.7204e-03,  8.4457e-03,  2.3041e-02,  ...,  3.2776e-02,\n",
              "                        2.1362e-02, -1.7258e-02],\n",
              "                      [ 3.3630e-02,  2.7512e-02,  5.5908e-02,  ...,  3.7292e-02,\n",
              "                        2.4780e-02, -6.6414e-03],\n",
              "                      [ 2.6367e-02,  5.1605e-02, -1.4702e-02,  ..., -2.2888e-02,\n",
              "                        1.0028e-01,  1.9470e-02]], dtype=torch.float16)),\n",
              "             ('model.model.layers.21.self_attn.k_proj.weight',\n",
              "              tensor([[ 0.0023, -0.0035,  0.0046,  ...,  0.0008, -0.0078, -0.0191],\n",
              "                      [ 0.0006, -0.0011,  0.0040,  ...,  0.0060,  0.0002, -0.0315],\n",
              "                      [-0.0090, -0.0058, -0.0109,  ..., -0.0035,  0.0123, -0.0110],\n",
              "                      ...,\n",
              "                      [-0.0263, -0.0038, -0.0173,  ...,  0.0308,  0.0335, -0.0089],\n",
              "                      [ 0.0082, -0.0394,  0.0258,  ...,  0.0372,  0.0045, -0.0173],\n",
              "                      [-0.0191,  0.0433,  0.0206,  ...,  0.0093,  0.0189, -0.0130]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.21.self_attn.v_proj.weight',\n",
              "              tensor([[-0.0211,  0.0226, -0.0159,  ...,  0.0038,  0.0213,  0.0545],\n",
              "                      [ 0.0343, -0.0218, -0.0374,  ...,  0.0166,  0.0176, -0.0444],\n",
              "                      [ 0.0409,  0.0148, -0.0360,  ...,  0.0156, -0.0240, -0.0111],\n",
              "                      ...,\n",
              "                      [-0.0089,  0.0278, -0.0226,  ...,  0.0526,  0.0386,  0.0132],\n",
              "                      [ 0.0025,  0.0289, -0.0193,  ..., -0.0043, -0.0012,  0.0122],\n",
              "                      [ 0.0143, -0.0138,  0.0423,  ...,  0.0197,  0.0171, -0.0128]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.21.self_attn.o_proj.weight',\n",
              "              tensor([[-0.0022,  0.0235,  0.0062,  ...,  0.0008,  0.0128,  0.0171],\n",
              "                      [-0.0177, -0.0149,  0.0049,  ...,  0.0092,  0.0199, -0.0307],\n",
              "                      [-0.0093, -0.0312, -0.0515,  ..., -0.0234, -0.0127,  0.0301],\n",
              "                      ...,\n",
              "                      [-0.0109,  0.0241,  0.0015,  ...,  0.0014, -0.0195, -0.0012],\n",
              "                      [ 0.0219,  0.0044, -0.0100,  ...,  0.0043, -0.0122,  0.0188],\n",
              "                      [ 0.0035, -0.0212,  0.0145,  ...,  0.0146,  0.0125, -0.0601]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.21.mlp.gate_proj.weight',\n",
              "              tensor([[-0.0177, -0.0135,  0.0038,  ..., -0.0103, -0.0115, -0.0004],\n",
              "                      [ 0.0437, -0.0317, -0.0085,  ...,  0.0021,  0.0018,  0.0074],\n",
              "                      [ 0.0015, -0.0047,  0.0004,  ...,  0.0002, -0.0391, -0.0003],\n",
              "                      ...,\n",
              "                      [ 0.0366, -0.0025,  0.0032,  ..., -0.0175,  0.0185,  0.0347],\n",
              "                      [-0.0048,  0.0286, -0.0401,  ..., -0.0127, -0.0078, -0.0551],\n",
              "                      [-0.0098, -0.0025, -0.0170,  ..., -0.0312, -0.0067, -0.0225]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.21.mlp.up_proj.weight',\n",
              "              tensor([[-0.0218,  0.0360, -0.0061,  ...,  0.0153,  0.0177,  0.0113],\n",
              "                      [ 0.0191,  0.0212,  0.0586,  ...,  0.0029,  0.0103, -0.0334],\n",
              "                      [-0.0070, -0.0118, -0.0313,  ...,  0.0133,  0.0224, -0.0074],\n",
              "                      ...,\n",
              "                      [ 0.0109,  0.0112, -0.0127,  ...,  0.0192, -0.0041, -0.0171],\n",
              "                      [-0.0048, -0.0105, -0.0091,  ...,  0.0158, -0.0016,  0.0006],\n",
              "                      [ 0.0007, -0.0257, -0.0155,  ..., -0.0362,  0.0046,  0.0136]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.21.mlp.down_proj.weight',\n",
              "              tensor([[-0.0060,  0.0485,  0.0007,  ..., -0.0121,  0.0180, -0.0228],\n",
              "                      [-0.0083, -0.0292, -0.0107,  ...,  0.0258,  0.0142, -0.0231],\n",
              "                      [-0.0132, -0.0362,  0.0057,  ..., -0.0079, -0.0017,  0.0034],\n",
              "                      ...,\n",
              "                      [-0.0093, -0.0172, -0.0089,  ..., -0.0085,  0.0073, -0.0453],\n",
              "                      [ 0.0337, -0.0118, -0.0352,  ..., -0.0202,  0.0007,  0.0216],\n",
              "                      [ 0.0065, -0.0346, -0.0079,  ...,  0.0092,  0.0142,  0.0231]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.21.input_layernorm.weight',\n",
              "              tensor([0.4111, 0.4089, 0.4087,  ..., 0.4141, 0.4106, 0.4272],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.21.post_attention_layernorm.weight',\n",
              "              tensor([0.3523, 0.3547, 0.3604,  ..., 0.3477, 0.3613, 0.3535],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.22.self_attn.q_proj.weight',\n",
              "              tensor([[ 0.0078,  0.0342,  0.0096,  ..., -0.0031, -0.0055,  0.0134],\n",
              "                      [ 0.0156,  0.0020,  0.0177,  ..., -0.0129, -0.0201, -0.0117],\n",
              "                      [-0.0226,  0.0025, -0.0206,  ..., -0.0103, -0.0292,  0.0063],\n",
              "                      ...,\n",
              "                      [ 0.0212,  0.0076,  0.0071,  ..., -0.0280,  0.0064, -0.0208],\n",
              "                      [ 0.0091, -0.0322, -0.0368,  ..., -0.0411,  0.0008,  0.0200],\n",
              "                      [ 0.0188, -0.0181, -0.0086,  ..., -0.0230,  0.0091, -0.0050]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.22.self_attn.k_proj.weight',\n",
              "              tensor([[-0.0005, -0.0040,  0.0179,  ..., -0.0136, -0.0016,  0.0287],\n",
              "                      [ 0.0051, -0.0154, -0.0148,  ..., -0.0365,  0.0034,  0.0027],\n",
              "                      [-0.0292,  0.0363,  0.0298,  ..., -0.0132,  0.0315, -0.0184],\n",
              "                      ...,\n",
              "                      [-0.0333,  0.0557, -0.0113,  ..., -0.0140,  0.0423,  0.0067],\n",
              "                      [ 0.0099, -0.0474, -0.0190,  ..., -0.0474, -0.0491, -0.0477],\n",
              "                      [-0.0115, -0.0156, -0.0269,  ..., -0.0038,  0.0103, -0.0179]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.22.self_attn.v_proj.weight',\n",
              "              tensor([[-0.0369,  0.0082, -0.0026,  ...,  0.0321,  0.0073,  0.0165],\n",
              "                      [-0.0080,  0.0245, -0.0204,  ...,  0.0233, -0.0123,  0.0313],\n",
              "                      [ 0.0365,  0.0011,  0.0394,  ..., -0.0584,  0.0510,  0.0722],\n",
              "                      ...,\n",
              "                      [ 0.0077,  0.0094,  0.0338,  ...,  0.0143, -0.0146,  0.0308],\n",
              "                      [-0.0112,  0.0027,  0.0207,  ...,  0.0051, -0.0256, -0.0203],\n",
              "                      [ 0.0190,  0.0103, -0.0073,  ..., -0.0313, -0.0180,  0.0020]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.22.self_attn.o_proj.weight',\n",
              "              tensor([[-0.0251, -0.0327, -0.0417,  ...,  0.0329, -0.0222, -0.0029],\n",
              "                      [ 0.0116, -0.0135,  0.0051,  ..., -0.0173,  0.0169,  0.0106],\n",
              "                      [ 0.0130,  0.0122, -0.0297,  ..., -0.0304, -0.0195,  0.0343],\n",
              "                      ...,\n",
              "                      [ 0.0034, -0.0185,  0.0102,  ..., -0.0241, -0.0182,  0.0118],\n",
              "                      [-0.0004, -0.0086,  0.0105,  ..., -0.0035, -0.0220, -0.0079],\n",
              "                      [ 0.0253,  0.0464, -0.0146,  ...,  0.0147,  0.0178, -0.0085]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.22.mlp.gate_proj.weight',\n",
              "              tensor([[ 0.0170,  0.0373,  0.0681,  ...,  0.0042, -0.0229, -0.0550],\n",
              "                      [ 0.0356, -0.0065, -0.0046,  ...,  0.0330,  0.0173, -0.0179],\n",
              "                      [-0.0268,  0.0316, -0.0030,  ...,  0.0113,  0.0003, -0.0068],\n",
              "                      ...,\n",
              "                      [ 0.0093,  0.0171, -0.0375,  ...,  0.0243, -0.0213, -0.0187],\n",
              "                      [-0.0086, -0.0174, -0.0056,  ..., -0.0098, -0.0023, -0.0193],\n",
              "                      [ 0.0038, -0.0131,  0.0006,  ...,  0.0257,  0.0135,  0.0142]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.22.mlp.up_proj.weight',\n",
              "              tensor([[-1.9547e-02, -1.6052e-02,  2.4857e-02,  ...,  1.9379e-02,\n",
              "                        1.7366e-03, -4.9472e-06],\n",
              "                      [-3.1281e-02, -3.2104e-02, -6.8436e-03,  ..., -1.3367e-02,\n",
              "                       -3.6392e-03, -2.3590e-02],\n",
              "                      [-2.1381e-03,  2.6108e-02,  6.8321e-03,  ...,  2.4979e-02,\n",
              "                        1.7405e-04,  2.6199e-02],\n",
              "                      ...,\n",
              "                      [-2.8534e-03, -2.7054e-02,  5.1270e-02,  ..., -3.7170e-02,\n",
              "                        2.0981e-02, -1.3741e-02],\n",
              "                      [ 8.4457e-03,  1.1253e-02,  1.8625e-03,  ...,  4.9255e-02,\n",
              "                        1.0040e-02, -2.9831e-03],\n",
              "                      [ 4.8141e-03,  5.2155e-02,  5.3215e-03,  ...,  1.0986e-02,\n",
              "                        3.5645e-02, -9.3307e-03]], dtype=torch.float16)),\n",
              "             ('model.model.layers.22.mlp.down_proj.weight',\n",
              "              tensor([[-0.0056,  0.0248, -0.0085,  ..., -0.0178,  0.0128,  0.0137],\n",
              "                      [-0.0301,  0.0750,  0.0154,  ...,  0.0082,  0.0204, -0.0125],\n",
              "                      [ 0.0144,  0.0256,  0.0195,  ..., -0.0020,  0.0296, -0.0088],\n",
              "                      ...,\n",
              "                      [ 0.0395, -0.0255,  0.0244,  ...,  0.0697,  0.0039,  0.0328],\n",
              "                      [ 0.0093, -0.0088,  0.0197,  ...,  0.0032, -0.0184,  0.0016],\n",
              "                      [ 0.0126,  0.0288,  0.0407,  ...,  0.0165, -0.0442, -0.0257]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.22.input_layernorm.weight',\n",
              "              tensor([0.4119, 0.4312, 0.4282,  ..., 0.4329, 0.4375, 0.4436],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.22.post_attention_layernorm.weight',\n",
              "              tensor([0.3711, 0.3757, 0.3740,  ..., 0.3665, 0.3743, 0.3647],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.23.self_attn.q_proj.weight',\n",
              "              tensor([[ 1.2932e-02, -9.0866e-03, -5.6610e-03,  ...,  5.5580e-03,\n",
              "                       -2.0065e-02,  1.3542e-02],\n",
              "                      [ 2.1687e-03, -9.8419e-03, -2.3666e-02,  ...,  9.5308e-05,\n",
              "                       -2.0477e-02,  1.3990e-03],\n",
              "                      [-2.1271e-02,  2.9449e-02, -1.7319e-02,  ..., -3.6926e-02,\n",
              "                        3.1586e-02, -2.0111e-02],\n",
              "                      ...,\n",
              "                      [ 2.8038e-04, -1.5839e-02, -1.6251e-02,  ..., -2.0111e-02,\n",
              "                       -1.8845e-02, -6.8932e-03],\n",
              "                      [-1.9455e-02, -1.8234e-02, -6.1127e-02,  ..., -1.2985e-02,\n",
              "                        3.6713e-02, -2.6657e-02],\n",
              "                      [ 3.4771e-03,  1.0529e-02,  3.4149e-02,  ..., -3.9139e-03,\n",
              "                       -3.8361e-02,  4.6906e-02]], dtype=torch.float16)),\n",
              "             ('model.model.layers.23.self_attn.k_proj.weight',\n",
              "              tensor([[ 0.0133, -0.0219,  0.0094,  ..., -0.0038,  0.0109,  0.0011],\n",
              "                      [-0.0027, -0.0094, -0.0055,  ..., -0.0006,  0.0119,  0.0300],\n",
              "                      [-0.0048, -0.0009,  0.0018,  ...,  0.0398, -0.0030, -0.0045],\n",
              "                      ...,\n",
              "                      [-0.0484, -0.0159, -0.0077,  ..., -0.0133,  0.0031,  0.0044],\n",
              "                      [ 0.0130, -0.0375,  0.0458,  ...,  0.0308, -0.0076,  0.0269],\n",
              "                      [-0.0443, -0.0486,  0.0200,  ...,  0.0217,  0.0287,  0.0051]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.23.self_attn.v_proj.weight',\n",
              "              tensor([[ 0.0083, -0.0225,  0.0164,  ..., -0.0020, -0.0104, -0.0026],\n",
              "                      [-0.0057, -0.0432, -0.0246,  ..., -0.0114,  0.0181,  0.0223],\n",
              "                      [ 0.0320, -0.0379, -0.0277,  ...,  0.0323,  0.0188,  0.0036],\n",
              "                      ...,\n",
              "                      [ 0.0118,  0.0111, -0.0131,  ..., -0.0003, -0.0225,  0.0248],\n",
              "                      [ 0.0037, -0.0168,  0.0079,  ...,  0.0235, -0.0018,  0.0116],\n",
              "                      [ 0.0306,  0.0008,  0.0021,  ..., -0.0467,  0.0476, -0.0141]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.23.self_attn.o_proj.weight',\n",
              "              tensor([[ 0.0051, -0.0077, -0.0485,  ..., -0.0244,  0.0005, -0.0330],\n",
              "                      [ 0.0460,  0.0302, -0.0091,  ..., -0.0202, -0.0067,  0.0064],\n",
              "                      [-0.0276, -0.0027, -0.0198,  ..., -0.0269, -0.0050,  0.0032],\n",
              "                      ...,\n",
              "                      [-0.0062,  0.0144, -0.0325,  ..., -0.0117,  0.0022,  0.0212],\n",
              "                      [-0.0296,  0.0124, -0.0093,  ...,  0.0054,  0.0218, -0.0153],\n",
              "                      [ 0.0127, -0.0014,  0.0029,  ...,  0.0235, -0.0037, -0.0685]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.23.mlp.gate_proj.weight',\n",
              "              tensor([[ 0.0373,  0.0078, -0.0182,  ...,  0.0381, -0.0103, -0.0169],\n",
              "                      [-0.0507, -0.0048, -0.0287,  ..., -0.0245,  0.0216,  0.0083],\n",
              "                      [ 0.0159, -0.0596,  0.0076,  ..., -0.0124, -0.0214, -0.0058],\n",
              "                      ...,\n",
              "                      [ 0.0122, -0.0446, -0.0485,  ...,  0.0363, -0.0166,  0.0079],\n",
              "                      [-0.0085, -0.0006,  0.0292,  ..., -0.0127,  0.0316, -0.0221],\n",
              "                      [ 0.0190,  0.0027,  0.0239,  ..., -0.0136, -0.0075,  0.0534]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.23.mlp.up_proj.weight',\n",
              "              tensor([[-0.0034,  0.0364,  0.0053,  ...,  0.0066, -0.0111, -0.0223],\n",
              "                      [ 0.0192,  0.0250, -0.0345,  ..., -0.0385,  0.0232, -0.0059],\n",
              "                      [-0.0164,  0.0158, -0.0071,  ...,  0.0132,  0.0484,  0.0026],\n",
              "                      ...,\n",
              "                      [-0.0126,  0.0100, -0.0057,  ..., -0.0059,  0.0259, -0.0246],\n",
              "                      [-0.0117,  0.0055,  0.0044,  ..., -0.0279,  0.0007, -0.0091],\n",
              "                      [ 0.0302,  0.0179, -0.0135,  ..., -0.0005,  0.0196, -0.0395]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.23.mlp.down_proj.weight',\n",
              "              tensor([[ 0.0079, -0.0050,  0.0186,  ..., -0.0300, -0.0155, -0.0120],\n",
              "                      [ 0.0242, -0.0193, -0.0031,  ..., -0.0275, -0.0283,  0.0008],\n",
              "                      [-0.0192,  0.0598, -0.0013,  ..., -0.0022,  0.0126,  0.0119],\n",
              "                      ...,\n",
              "                      [ 0.0027, -0.0090,  0.0166,  ...,  0.0097,  0.0160,  0.0436],\n",
              "                      [ 0.0154,  0.0024, -0.0320,  ..., -0.0257, -0.0219,  0.0564],\n",
              "                      [ 0.0095, -0.0129, -0.0213,  ...,  0.0562, -0.0183, -0.0007]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.23.input_layernorm.weight',\n",
              "              tensor([0.4324, 0.4331, 0.4316,  ..., 0.4260, 0.4304, 0.4451],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.23.post_attention_layernorm.weight',\n",
              "              tensor([0.3860, 0.3953, 0.3879,  ..., 0.3757, 0.3960, 0.3801],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.24.self_attn.q_proj.weight',\n",
              "              tensor([[-0.0028,  0.0025, -0.0088,  ...,  0.0067, -0.0091, -0.0291],\n",
              "                      [-0.0163, -0.0028,  0.0059,  ...,  0.0058, -0.0160, -0.0202],\n",
              "                      [ 0.0052, -0.0040, -0.0116,  ...,  0.0205, -0.0223, -0.0072],\n",
              "                      ...,\n",
              "                      [ 0.0248, -0.0213,  0.0168,  ...,  0.0279, -0.0274,  0.0513],\n",
              "                      [ 0.0202,  0.0206, -0.0255,  ..., -0.0131,  0.0136,  0.0063],\n",
              "                      [-0.0250, -0.0245, -0.0204,  ..., -0.0152, -0.0138,  0.0110]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.24.self_attn.k_proj.weight',\n",
              "              tensor([[-0.0024, -0.0160,  0.0036,  ...,  0.0101, -0.0227, -0.0253],\n",
              "                      [-0.0019, -0.0022,  0.0222,  ...,  0.0082, -0.0101,  0.0209],\n",
              "                      [ 0.0096, -0.0036, -0.0127,  ...,  0.0097, -0.0005, -0.0031],\n",
              "                      ...,\n",
              "                      [ 0.0300, -0.0875,  0.0202,  ..., -0.0105,  0.0138,  0.0387],\n",
              "                      [ 0.0099,  0.0148, -0.0679,  ..., -0.0011,  0.0082, -0.0337],\n",
              "                      [-0.0012, -0.0228, -0.0107,  ..., -0.0025,  0.0204,  0.0088]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.24.self_attn.v_proj.weight',\n",
              "              tensor([[ 0.0137, -0.0126,  0.0286,  ...,  0.0519,  0.0017, -0.0077],\n",
              "                      [-0.0242,  0.0137, -0.0118,  ...,  0.0076, -0.0367,  0.0084],\n",
              "                      [ 0.0096,  0.0193,  0.0123,  ..., -0.0089, -0.0290, -0.0113],\n",
              "                      ...,\n",
              "                      [ 0.0222,  0.0096,  0.0003,  ...,  0.0036,  0.0177, -0.0337],\n",
              "                      [ 0.0271,  0.0090,  0.0419,  ...,  0.0144, -0.0054, -0.0044],\n",
              "                      [-0.0440,  0.0424, -0.0426,  ..., -0.0206, -0.0587, -0.0393]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.24.self_attn.o_proj.weight',\n",
              "              tensor([[ 5.8556e-03, -3.4393e-02,  3.8376e-03,  ..., -1.2360e-02,\n",
              "                       -1.6892e-04, -1.6708e-02],\n",
              "                      [-1.0757e-02,  5.8670e-03, -7.8087e-03,  ...,  1.5495e-02,\n",
              "                        2.8820e-03,  1.5087e-03],\n",
              "                      [ 3.6346e-02,  4.4975e-03, -1.8433e-02,  ...,  1.9608e-03,\n",
              "                       -2.6550e-02, -6.5956e-03],\n",
              "                      ...,\n",
              "                      [ 2.3376e-02,  2.0462e-02,  2.4139e-02,  ...,  1.6541e-02,\n",
              "                       -5.4321e-03, -1.4832e-02],\n",
              "                      [ 3.7231e-03, -1.3893e-02, -2.5360e-02,  ...,  2.0828e-02,\n",
              "                        5.4382e-02, -1.6495e-02],\n",
              "                      [-6.0425e-03,  2.7008e-02,  2.2614e-02,  ..., -3.7551e-06,\n",
              "                        3.9856e-02, -1.2688e-02]], dtype=torch.float16)),\n",
              "             ('model.model.layers.24.mlp.gate_proj.weight',\n",
              "              tensor([[ 4.8492e-02,  2.5177e-02, -1.2375e-02,  ...,  5.2521e-02,\n",
              "                       -2.7115e-02, -1.0252e-03],\n",
              "                      [-1.1131e-02, -4.5654e-02,  2.9755e-02,  ..., -1.9180e-02,\n",
              "                        4.2206e-02,  8.0643e-03],\n",
              "                      [ 1.3840e-02, -7.7438e-03, -6.9857e-05,  ..., -1.4429e-03,\n",
              "                       -1.2947e-02, -1.8311e-02],\n",
              "                      ...,\n",
              "                      [-1.4858e-03, -8.9188e-03, -6.5727e-03,  ..., -1.4938e-02,\n",
              "                        4.1656e-03,  5.9433e-03],\n",
              "                      [ 2.2430e-02, -2.0691e-02, -3.5248e-03,  ..., -1.7639e-02,\n",
              "                        2.4887e-02, -3.0029e-02],\n",
              "                      [-4.8943e-03, -1.3180e-03,  7.2174e-03,  ..., -3.9978e-03,\n",
              "                       -1.5762e-02,  1.2489e-02]], dtype=torch.float16)),\n",
              "             ('model.model.layers.24.mlp.up_proj.weight',\n",
              "              tensor([[ 0.0074, -0.0274, -0.0349,  ...,  0.0158,  0.0175, -0.0469],\n",
              "                      [ 0.0098,  0.0044,  0.0372,  ..., -0.0199,  0.0431, -0.0046],\n",
              "                      [ 0.0290,  0.0292, -0.0252,  ..., -0.0073, -0.0137, -0.0284],\n",
              "                      ...,\n",
              "                      [ 0.0303,  0.0202, -0.0176,  ..., -0.0276, -0.0108, -0.0075],\n",
              "                      [-0.0309,  0.0157, -0.0047,  ...,  0.0238,  0.0145, -0.0084],\n",
              "                      [ 0.0095,  0.0008,  0.0302,  ..., -0.0355, -0.0142,  0.0101]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.24.mlp.down_proj.weight',\n",
              "              tensor([[ 0.0096,  0.0160, -0.0144,  ..., -0.0171, -0.0042,  0.0141],\n",
              "                      [-0.0408, -0.0372,  0.0106,  ...,  0.0473, -0.0007, -0.0200],\n",
              "                      [ 0.0060, -0.0125,  0.0072,  ..., -0.0333,  0.0169,  0.0379],\n",
              "                      ...,\n",
              "                      [ 0.0541,  0.0129, -0.0009,  ..., -0.0335, -0.0102,  0.0059],\n",
              "                      [-0.0106,  0.0057, -0.0078,  ..., -0.0345,  0.0083,  0.0143],\n",
              "                      [-0.0240,  0.0067, -0.0133,  ..., -0.0031, -0.0147,  0.0290]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.24.input_layernorm.weight',\n",
              "              tensor([0.4473, 0.4685, 0.4607,  ..., 0.4626, 0.4604, 0.4775],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.24.post_attention_layernorm.weight',\n",
              "              tensor([0.3994, 0.4014, 0.4011,  ..., 0.3931, 0.3967, 0.4009],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.25.self_attn.q_proj.weight',\n",
              "              tensor([[-0.0186,  0.0012,  0.0044,  ...,  0.0080,  0.0039,  0.0116],\n",
              "                      [-0.0228, -0.0128, -0.0138,  ..., -0.0041, -0.0303, -0.0051],\n",
              "                      [ 0.0147,  0.0115, -0.0051,  ...,  0.0040,  0.0197, -0.0227],\n",
              "                      ...,\n",
              "                      [ 0.0238, -0.0317, -0.0233,  ..., -0.0302, -0.0168, -0.0070],\n",
              "                      [-0.0272, -0.0510,  0.0224,  ..., -0.0292, -0.0088,  0.0123],\n",
              "                      [-0.0100, -0.0062, -0.0082,  ...,  0.0056,  0.0466,  0.0789]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.25.self_attn.k_proj.weight',\n",
              "              tensor([[-0.0138,  0.0233,  0.0021,  ...,  0.0166,  0.0213, -0.0166],\n",
              "                      [-0.0136, -0.0191, -0.0121,  ..., -0.0195,  0.0030,  0.0068],\n",
              "                      [-0.0017, -0.0110,  0.0252,  ...,  0.0067, -0.0110, -0.0214],\n",
              "                      ...,\n",
              "                      [ 0.0330, -0.0065,  0.0203,  ...,  0.0027,  0.0448, -0.0193],\n",
              "                      [-0.0060, -0.0377, -0.0040,  ...,  0.0125, -0.0719,  0.0028],\n",
              "                      [-0.0359, -0.0145,  0.0015,  ..., -0.0085, -0.0422,  0.0233]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.25.self_attn.v_proj.weight',\n",
              "              tensor([[ 0.0026, -0.0095,  0.0182,  ...,  0.0075,  0.0541,  0.0182],\n",
              "                      [ 0.0069,  0.0064, -0.0626,  ...,  0.0126,  0.0128, -0.0154],\n",
              "                      [-0.0206, -0.0004,  0.0109,  ...,  0.0185, -0.0197, -0.0032],\n",
              "                      ...,\n",
              "                      [-0.0005,  0.0034, -0.0327,  ...,  0.0138, -0.0286,  0.0095],\n",
              "                      [ 0.0266, -0.0005,  0.0037,  ...,  0.0127,  0.0225,  0.0392],\n",
              "                      [ 0.0188, -0.0212, -0.0020,  ..., -0.0013, -0.0180, -0.0097]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.25.self_attn.o_proj.weight',\n",
              "              tensor([[-0.0188, -0.0464, -0.0379,  ..., -0.0273, -0.0131,  0.0092],\n",
              "                      [ 0.0080,  0.0433, -0.0102,  ...,  0.0098, -0.0373,  0.0158],\n",
              "                      [-0.0285,  0.0239, -0.0158,  ..., -0.0020, -0.0116, -0.0548],\n",
              "                      ...,\n",
              "                      [ 0.0187, -0.0290, -0.0039,  ...,  0.0015,  0.0003, -0.0233],\n",
              "                      [ 0.0114,  0.0222, -0.0276,  ..., -0.0159,  0.0067,  0.0124],\n",
              "                      [-0.0288, -0.0101, -0.0257,  ...,  0.0344, -0.0063,  0.0130]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.25.mlp.gate_proj.weight',\n",
              "              tensor([[ 0.0043, -0.0276, -0.0002,  ...,  0.0218, -0.0092,  0.0339],\n",
              "                      [ 0.0360,  0.0467,  0.0219,  ..., -0.0150,  0.0242,  0.0070],\n",
              "                      [ 0.0358,  0.0420,  0.0205,  ..., -0.0004,  0.0358,  0.0149],\n",
              "                      ...,\n",
              "                      [-0.0128,  0.0182, -0.0266,  ..., -0.0002,  0.0233, -0.0210],\n",
              "                      [-0.0085, -0.0139,  0.0545,  ...,  0.0077,  0.0025,  0.0285],\n",
              "                      [ 0.0004,  0.0044,  0.0281,  ..., -0.0037, -0.0411,  0.0344]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.25.mlp.up_proj.weight',\n",
              "              tensor([[-0.0019,  0.0039, -0.0256,  ..., -0.0488, -0.0309,  0.0033],\n",
              "                      [ 0.0061,  0.0261,  0.0075,  ..., -0.0046,  0.0110,  0.0372],\n",
              "                      [-0.0109,  0.0192, -0.0396,  ..., -0.0381, -0.0277,  0.0666],\n",
              "                      ...,\n",
              "                      [-0.0008,  0.0015, -0.0107,  ...,  0.0576,  0.0124,  0.0241],\n",
              "                      [ 0.0219,  0.0034,  0.0151,  ...,  0.0346, -0.0126, -0.0237],\n",
              "                      [-0.0024, -0.0644,  0.0446,  ...,  0.0058,  0.0045,  0.0534]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.25.mlp.down_proj.weight',\n",
              "              tensor([[ 0.0381, -0.0141,  0.0442,  ...,  0.0409, -0.0159,  0.0250],\n",
              "                      [ 0.0239, -0.0111,  0.0161,  ...,  0.0147,  0.0396, -0.0173],\n",
              "                      [ 0.0126,  0.0255, -0.0148,  ...,  0.0014,  0.0084,  0.0067],\n",
              "                      ...,\n",
              "                      [-0.0253, -0.0665, -0.0266,  ..., -0.0314,  0.0363, -0.0082],\n",
              "                      [ 0.0247, -0.0548, -0.0097,  ..., -0.0091,  0.0307,  0.0075],\n",
              "                      [ 0.0005, -0.0178, -0.0098,  ...,  0.0154,  0.0067, -0.0259]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.25.input_layernorm.weight',\n",
              "              tensor([0.4810, 0.4741, 0.4688,  ..., 0.4880, 0.4834, 0.4802],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.25.post_attention_layernorm.weight',\n",
              "              tensor([0.4077, 0.4087, 0.4121,  ..., 0.4033, 0.4163, 0.4141],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.26.self_attn.q_proj.weight',\n",
              "              tensor([[-0.0219, -0.0256,  0.0617,  ...,  0.0155,  0.0160, -0.0029],\n",
              "                      [-0.0274, -0.0102, -0.0086,  ...,  0.0338, -0.0266, -0.0325],\n",
              "                      [ 0.0068,  0.0075, -0.0488,  ...,  0.0212,  0.0071, -0.0133],\n",
              "                      ...,\n",
              "                      [ 0.0048,  0.0227,  0.0103,  ..., -0.0272,  0.0693, -0.0292],\n",
              "                      [ 0.0041,  0.0010,  0.0478,  ..., -0.0359,  0.0290,  0.0423],\n",
              "                      [ 0.0295, -0.0045,  0.0490,  ..., -0.0245, -0.0167, -0.0109]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.26.self_attn.k_proj.weight',\n",
              "              tensor([[-2.3956e-02, -2.9335e-03,  3.8696e-02,  ...,  7.8064e-02,\n",
              "                        1.4130e-02, -1.6449e-02],\n",
              "                      [-5.7869e-03,  1.8799e-02, -9.4986e-03,  ...,  4.0070e-02,\n",
              "                       -3.3016e-03, -1.2245e-02],\n",
              "                      [ 4.7112e-03, -4.2000e-03, -8.2275e-02,  ..., -1.7593e-02,\n",
              "                        4.0985e-02, -1.7639e-02],\n",
              "                      ...,\n",
              "                      [-9.4299e-03,  2.6443e-02,  2.4994e-02,  ...,  5.9662e-03,\n",
              "                        2.3758e-02,  2.0554e-02],\n",
              "                      [ 4.3221e-03,  2.0737e-02, -2.9160e-02,  ..., -3.5248e-02,\n",
              "                       -1.3857e-03, -2.5570e-05],\n",
              "                      [ 1.2993e-02, -1.2751e-03, -6.6490e-03,  ..., -8.6899e-03,\n",
              "                        1.7338e-03,  1.5068e-02]], dtype=torch.float16)),\n",
              "             ('model.model.layers.26.self_attn.v_proj.weight',\n",
              "              tensor([[ 0.0085, -0.0016, -0.0075,  ...,  0.0064,  0.0016, -0.0309],\n",
              "                      [-0.0109, -0.0141,  0.0098,  ...,  0.0303, -0.0344, -0.0042],\n",
              "                      [ 0.0690, -0.0304, -0.0170,  ...,  0.0268,  0.0490,  0.0387],\n",
              "                      ...,\n",
              "                      [ 0.0182,  0.0240,  0.0006,  ...,  0.0060,  0.0431,  0.0094],\n",
              "                      [ 0.0049, -0.0120, -0.0369,  ...,  0.0049, -0.0068,  0.0424],\n",
              "                      [ 0.0075, -0.0115, -0.0003,  ..., -0.0336,  0.0621, -0.0291]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.26.self_attn.o_proj.weight',\n",
              "              tensor([[-0.0285, -0.0131,  0.0180,  ...,  0.0037,  0.0227,  0.0320],\n",
              "                      [ 0.0087, -0.0015, -0.0057,  ...,  0.0028,  0.0176, -0.0141],\n",
              "                      [ 0.0106,  0.0231, -0.0407,  ...,  0.0021, -0.0114,  0.0410],\n",
              "                      ...,\n",
              "                      [-0.0016, -0.0535, -0.0024,  ...,  0.0401, -0.0014,  0.0002],\n",
              "                      [-0.0221, -0.0079, -0.0420,  ..., -0.0373, -0.0476, -0.0107],\n",
              "                      [ 0.0440, -0.0098,  0.0197,  ..., -0.0114, -0.0018, -0.0495]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.26.mlp.gate_proj.weight',\n",
              "              tensor([[-0.0263,  0.0263,  0.0062,  ..., -0.0320,  0.0089,  0.0165],\n",
              "                      [-0.0047, -0.0137, -0.0186,  ...,  0.0119, -0.0106, -0.0208],\n",
              "                      [ 0.0058,  0.0072, -0.0172,  ..., -0.0024,  0.0039,  0.0021],\n",
              "                      ...,\n",
              "                      [ 0.0109, -0.0384, -0.0053,  ...,  0.0079, -0.0400,  0.0232],\n",
              "                      [ 0.0163,  0.0399,  0.0438,  ...,  0.0020,  0.0064,  0.0097],\n",
              "                      [-0.0109,  0.0520,  0.0112,  ..., -0.0416,  0.0302, -0.0024]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.26.mlp.up_proj.weight',\n",
              "              tensor([[ 0.0255, -0.0292, -0.0341,  ...,  0.0050, -0.0262, -0.0081],\n",
              "                      [ 0.0176, -0.0134,  0.0573,  ..., -0.0161, -0.0161, -0.0277],\n",
              "                      [-0.0353, -0.0202, -0.0017,  ...,  0.0216, -0.0022, -0.0073],\n",
              "                      ...,\n",
              "                      [ 0.0099, -0.0118,  0.0185,  ...,  0.0133, -0.0052, -0.0024],\n",
              "                      [ 0.0065, -0.0282, -0.0213,  ...,  0.0029, -0.0407,  0.0160],\n",
              "                      [-0.0540, -0.0129, -0.0033,  ...,  0.0271,  0.0063,  0.0075]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.26.mlp.down_proj.weight',\n",
              "              tensor([[-0.0165, -0.0157,  0.0235,  ...,  0.0103,  0.0039, -0.0022],\n",
              "                      [-0.0142,  0.0096, -0.0142,  ..., -0.0230,  0.0051,  0.0117],\n",
              "                      [ 0.0173, -0.0343,  0.0288,  ...,  0.0162, -0.0214,  0.0061],\n",
              "                      ...,\n",
              "                      [ 0.0208, -0.0285,  0.0098,  ...,  0.0082,  0.0087, -0.0108],\n",
              "                      [ 0.0018, -0.0037,  0.0125,  ...,  0.0244,  0.0075,  0.0030],\n",
              "                      [-0.0148,  0.0148, -0.0138,  ..., -0.0322,  0.0190,  0.0041]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.26.input_layernorm.weight',\n",
              "              tensor([0.4985, 0.5044, 0.5078,  ..., 0.5117, 0.5122, 0.5093],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.26.post_attention_layernorm.weight',\n",
              "              tensor([0.4187, 0.4241, 0.4253,  ..., 0.4148, 0.4280, 0.4255],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.27.self_attn.q_proj.weight',\n",
              "              tensor([[-0.0061,  0.0060,  0.0030,  ..., -0.0038,  0.0106, -0.0240],\n",
              "                      [ 0.0220, -0.0073,  0.0008,  ..., -0.0073,  0.0145, -0.0115],\n",
              "                      [-0.0106, -0.0318,  0.0374,  ..., -0.0069,  0.0414, -0.0130],\n",
              "                      ...,\n",
              "                      [ 0.0019, -0.0091,  0.0005,  ..., -0.0151, -0.0118,  0.0020],\n",
              "                      [ 0.0296,  0.0845,  0.0569,  ..., -0.0031,  0.0174,  0.0331],\n",
              "                      [ 0.0304,  0.0208, -0.0213,  ...,  0.0221,  0.0235, -0.0372]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.27.self_attn.k_proj.weight',\n",
              "              tensor([[ 0.0025,  0.0133,  0.0107,  ...,  0.0091,  0.0079,  0.0112],\n",
              "                      [-0.0176, -0.0044,  0.0078,  ...,  0.0123, -0.0240, -0.0077],\n",
              "                      [ 0.0216,  0.0030,  0.0088,  ..., -0.0171,  0.0319,  0.0068],\n",
              "                      ...,\n",
              "                      [ 0.0130, -0.0507, -0.0194,  ..., -0.0059,  0.0100, -0.0092],\n",
              "                      [ 0.0232, -0.0190,  0.0135,  ..., -0.0064, -0.0025,  0.0058],\n",
              "                      [-0.0441,  0.0020,  0.0367,  ..., -0.0172,  0.0130,  0.0017]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.27.self_attn.v_proj.weight',\n",
              "              tensor([[ 0.0053, -0.0570, -0.0210,  ..., -0.0018,  0.0101,  0.0424],\n",
              "                      [ 0.0191,  0.0111,  0.0040,  ...,  0.0053, -0.0325,  0.0170],\n",
              "                      [-0.0208,  0.0312,  0.0292,  ..., -0.0187, -0.0164, -0.0320],\n",
              "                      ...,\n",
              "                      [ 0.0124, -0.0130, -0.0201,  ..., -0.0107,  0.0053, -0.0185],\n",
              "                      [ 0.0259,  0.0146, -0.0062,  ...,  0.0018,  0.0289,  0.0058],\n",
              "                      [ 0.0214, -0.0172, -0.0088,  ...,  0.0116, -0.0102, -0.0048]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.27.self_attn.o_proj.weight',\n",
              "              tensor([[-0.0111, -0.0156,  0.0044,  ..., -0.0253, -0.0105, -0.0089],\n",
              "                      [-0.0055, -0.0223,  0.0298,  ..., -0.0332,  0.0062,  0.0033],\n",
              "                      [ 0.0105, -0.0036, -0.0087,  ..., -0.0166, -0.0233,  0.0015],\n",
              "                      ...,\n",
              "                      [ 0.0302, -0.0171,  0.0303,  ..., -0.0122,  0.0189,  0.0046],\n",
              "                      [ 0.0002,  0.0041,  0.0107,  ..., -0.0096, -0.0208,  0.0344],\n",
              "                      [-0.0096, -0.0226, -0.0142,  ..., -0.0283,  0.0196, -0.0164]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.27.mlp.gate_proj.weight',\n",
              "              tensor([[-0.0132, -0.0141, -0.0542,  ..., -0.0343,  0.0102, -0.0021],\n",
              "                      [-0.0247,  0.0442,  0.0063,  ...,  0.0448,  0.0115,  0.0392],\n",
              "                      [ 0.0084,  0.0306, -0.0065,  ...,  0.0192,  0.0512,  0.0030],\n",
              "                      ...,\n",
              "                      [-0.0113, -0.0061,  0.0368,  ...,  0.0131, -0.0124, -0.0021],\n",
              "                      [ 0.0231,  0.0145,  0.0125,  ...,  0.0076, -0.0025, -0.0054],\n",
              "                      [ 0.0383, -0.0074,  0.0520,  ...,  0.0020,  0.0127, -0.0349]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.27.mlp.up_proj.weight',\n",
              "              tensor([[ 0.0183, -0.0012,  0.0333,  ..., -0.0098, -0.0151,  0.0292],\n",
              "                      [ 0.0326, -0.0260,  0.0051,  ..., -0.0294, -0.0154,  0.0308],\n",
              "                      [ 0.0287,  0.0249, -0.0197,  ..., -0.0351,  0.0124, -0.0168],\n",
              "                      ...,\n",
              "                      [-0.0045,  0.0013,  0.0306,  ...,  0.0078,  0.0137, -0.0259],\n",
              "                      [-0.0071, -0.0037,  0.0154,  ..., -0.0046, -0.0105,  0.0237],\n",
              "                      [-0.0012,  0.0087,  0.0032,  ..., -0.0374, -0.0140, -0.0253]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.27.mlp.down_proj.weight',\n",
              "              tensor([[ 0.0013, -0.0288, -0.0025,  ...,  0.0082, -0.0159,  0.0200],\n",
              "                      [ 0.0190, -0.0246,  0.0267,  ..., -0.0003, -0.0264,  0.0452],\n",
              "                      [-0.0242,  0.0277,  0.0128,  ..., -0.0176,  0.0029, -0.0133],\n",
              "                      ...,\n",
              "                      [ 0.0056,  0.0453, -0.0042,  ...,  0.0158,  0.0012,  0.0398],\n",
              "                      [ 0.0337,  0.0013,  0.0242,  ..., -0.0101,  0.0078, -0.0027],\n",
              "                      [ 0.0576, -0.0073,  0.0006,  ..., -0.0206,  0.0316,  0.0163]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.27.input_layernorm.weight',\n",
              "              tensor([0.4631, 0.5186, 0.5190,  ..., 0.5137, 0.5220, 0.5093],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.27.post_attention_layernorm.weight',\n",
              "              tensor([0.4395, 0.4363, 0.4355,  ..., 0.4285, 0.4412, 0.4314],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.28.self_attn.q_proj.weight',\n",
              "              tensor([[ 0.0255,  0.0275, -0.0144,  ..., -0.0238, -0.0124, -0.0230],\n",
              "                      [ 0.0186,  0.0137, -0.0068,  ...,  0.0322, -0.0203, -0.0046],\n",
              "                      [-0.0046,  0.0117,  0.0073,  ..., -0.0060,  0.0169,  0.0191],\n",
              "                      ...,\n",
              "                      [ 0.0194, -0.0136,  0.0086,  ..., -0.0226,  0.0148, -0.0369],\n",
              "                      [-0.0280,  0.0346, -0.0309,  ..., -0.0268,  0.0231, -0.0013],\n",
              "                      [-0.0061,  0.0294, -0.0050,  ..., -0.0763,  0.0057, -0.0093]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.28.self_attn.k_proj.weight',\n",
              "              tensor([[ 0.0051, -0.0129,  0.0069,  ..., -0.0042,  0.0101,  0.0090],\n",
              "                      [ 0.0107,  0.0010,  0.0070,  ...,  0.0255, -0.0008, -0.0159],\n",
              "                      [-0.0063, -0.0053,  0.0054,  ...,  0.0143,  0.0091,  0.0195],\n",
              "                      ...,\n",
              "                      [-0.0110, -0.0188, -0.0263,  ...,  0.0195,  0.0048, -0.0259],\n",
              "                      [ 0.0139, -0.0256, -0.0311,  ..., -0.0105, -0.0351, -0.0098],\n",
              "                      [ 0.0108, -0.0184, -0.0097,  ..., -0.0144, -0.0165,  0.0129]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.28.self_attn.v_proj.weight',\n",
              "              tensor([[ 0.0004,  0.0014, -0.0076,  ...,  0.0116, -0.0090,  0.0327],\n",
              "                      [ 0.0154, -0.0127, -0.0118,  ..., -0.0481,  0.0062, -0.0598],\n",
              "                      [ 0.0271,  0.0187, -0.0089,  ..., -0.0139,  0.0382, -0.0242],\n",
              "                      ...,\n",
              "                      [ 0.0058,  0.0021, -0.0286,  ..., -0.0080,  0.0255,  0.0093],\n",
              "                      [ 0.0017,  0.0065,  0.0343,  ...,  0.0147,  0.0128, -0.0082],\n",
              "                      [-0.0163, -0.0172, -0.0004,  ..., -0.0065,  0.0288,  0.0133]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.28.self_attn.o_proj.weight',\n",
              "              tensor([[-0.0248,  0.0240,  0.0299,  ...,  0.0220, -0.0188,  0.0063],\n",
              "                      [-0.0014, -0.0065,  0.0120,  ...,  0.0067,  0.0225, -0.0192],\n",
              "                      [-0.0141,  0.0037, -0.0640,  ...,  0.0398, -0.0243,  0.0039],\n",
              "                      ...,\n",
              "                      [ 0.0421,  0.0029,  0.0043,  ...,  0.0061, -0.0295,  0.0247],\n",
              "                      [ 0.0185,  0.0147,  0.0075,  ..., -0.0136,  0.0395,  0.0032],\n",
              "                      [ 0.0063, -0.0063,  0.0148,  ..., -0.0259, -0.0006, -0.0113]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.28.mlp.gate_proj.weight',\n",
              "              tensor([[-0.0260, -0.0061,  0.0078,  ..., -0.0018, -0.0546,  0.0186],\n",
              "                      [ 0.0357, -0.0109,  0.0085,  ..., -0.0318,  0.0109, -0.0055],\n",
              "                      [ 0.0219, -0.0051,  0.0223,  ...,  0.0272, -0.0091, -0.0053],\n",
              "                      ...,\n",
              "                      [ 0.0251, -0.0069,  0.0082,  ...,  0.0232, -0.0223,  0.0013],\n",
              "                      [-0.0197, -0.0408, -0.0553,  ..., -0.0264, -0.0258,  0.0291],\n",
              "                      [-0.0055, -0.0278,  0.0124,  ...,  0.0327,  0.0023, -0.0312]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.28.mlp.up_proj.weight',\n",
              "              tensor([[ 0.0136, -0.0225,  0.0054,  ...,  0.0416,  0.0417,  0.0146],\n",
              "                      [-0.0011,  0.0136,  0.0161,  ..., -0.0037,  0.0654, -0.0154],\n",
              "                      [ 0.0229,  0.0112,  0.0180,  ..., -0.0145,  0.0205,  0.0292],\n",
              "                      ...,\n",
              "                      [-0.0056, -0.0162,  0.0219,  ..., -0.0059, -0.0052, -0.0019],\n",
              "                      [ 0.0150, -0.0103,  0.0176,  ...,  0.0101,  0.0075, -0.0080],\n",
              "                      [-0.0125,  0.0115, -0.0202,  ...,  0.0372, -0.0210, -0.0303]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.28.mlp.down_proj.weight',\n",
              "              tensor([[ 7.7581e-04,  2.1088e-02, -1.7715e-02,  ..., -4.0131e-02,\n",
              "                       -3.8055e-02,  3.6316e-02],\n",
              "                      [-2.9945e-03, -2.1942e-02, -2.7420e-02,  ...,  2.7008e-02,\n",
              "                       -9.7046e-03, -3.7109e-02],\n",
              "                      [-2.3880e-02,  9.0866e-03, -2.9602e-03,  ...,  3.4241e-02,\n",
              "                        1.5045e-02,  1.1162e-02],\n",
              "                      ...,\n",
              "                      [-3.7781e-02,  1.0406e-02, -2.5501e-03,  ...,  4.6692e-02,\n",
              "                        1.3596e-02,  3.2196e-03],\n",
              "                      [-1.7872e-03, -3.4943e-02, -8.0490e-03,  ..., -6.4514e-02,\n",
              "                       -3.1647e-02,  2.7130e-02],\n",
              "                      [ 1.0925e-02,  1.6928e-05, -3.3997e-02,  ...,  2.4963e-02,\n",
              "                        1.4893e-02,  2.9144e-02]], dtype=torch.float16)),\n",
              "             ('model.model.layers.28.input_layernorm.weight',\n",
              "              tensor([0.4612, 0.5083, 0.4880,  ..., 0.4963, 0.5054, 0.5068],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.28.post_attention_layernorm.weight',\n",
              "              tensor([0.4451, 0.4419, 0.4556,  ..., 0.4375, 0.4583, 0.4409],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.29.self_attn.q_proj.weight',\n",
              "              tensor([[-0.0077, -0.0192,  0.0093,  ...,  0.0072,  0.0007, -0.0010],\n",
              "                      [-0.0071, -0.0019,  0.0001,  ..., -0.0019, -0.0089,  0.0095],\n",
              "                      [ 0.0042, -0.0013, -0.0277,  ..., -0.0164,  0.0204,  0.0053],\n",
              "                      ...,\n",
              "                      [ 0.0002,  0.0338,  0.0118,  ...,  0.0363,  0.0486, -0.0199],\n",
              "                      [-0.0021, -0.0548, -0.0518,  ...,  0.0180, -0.0522, -0.0299],\n",
              "                      [ 0.0257,  0.0182, -0.0176,  ..., -0.0317,  0.0033,  0.0104]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.29.self_attn.k_proj.weight',\n",
              "              tensor([[ 0.0084,  0.0087,  0.0123,  ...,  0.0157, -0.0196,  0.0034],\n",
              "                      [-0.0055, -0.0085, -0.0434,  ..., -0.0026, -0.0142,  0.0148],\n",
              "                      [-0.0223, -0.0008, -0.0074,  ...,  0.0082,  0.0013,  0.0019],\n",
              "                      ...,\n",
              "                      [-0.0059, -0.0323,  0.0447,  ...,  0.0098,  0.0291,  0.0114],\n",
              "                      [-0.0095, -0.0229, -0.0698,  ..., -0.0184, -0.0047,  0.0310],\n",
              "                      [ 0.0080,  0.0045, -0.0105,  ...,  0.0047,  0.0326, -0.0316]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.29.self_attn.v_proj.weight',\n",
              "              tensor([[ 0.0101,  0.0045, -0.0117,  ..., -0.0054,  0.0249,  0.0077],\n",
              "                      [-0.0242, -0.0314,  0.0167,  ...,  0.0424, -0.0091, -0.0558],\n",
              "                      [ 0.0173, -0.0115,  0.0163,  ...,  0.0242,  0.0241,  0.0028],\n",
              "                      ...,\n",
              "                      [-0.0112, -0.0045,  0.0064,  ..., -0.0295,  0.0191, -0.0057],\n",
              "                      [ 0.0197, -0.0262, -0.0083,  ...,  0.0065, -0.0344, -0.0187],\n",
              "                      [ 0.0122,  0.0116, -0.0383,  ..., -0.0237, -0.0503, -0.0526]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.29.self_attn.o_proj.weight',\n",
              "              tensor([[-0.0337,  0.0111,  0.0146,  ...,  0.0110,  0.0206, -0.0275],\n",
              "                      [ 0.0665, -0.0248, -0.0398,  ..., -0.0039, -0.0389,  0.0094],\n",
              "                      [ 0.0178,  0.0174,  0.0279,  ..., -0.0151,  0.0013, -0.0085],\n",
              "                      ...,\n",
              "                      [-0.0078,  0.0459,  0.0170,  ...,  0.0079,  0.0086,  0.0399],\n",
              "                      [ 0.0022, -0.0190,  0.0105,  ...,  0.0468, -0.0463, -0.0110],\n",
              "                      [ 0.0120, -0.0108, -0.0487,  ...,  0.0219,  0.0021, -0.0089]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.29.mlp.gate_proj.weight',\n",
              "              tensor([[ 0.0076, -0.0256, -0.0197,  ..., -0.0015, -0.0064, -0.0300],\n",
              "                      [ 0.0106,  0.0180, -0.0149,  ...,  0.0074, -0.0113, -0.0196],\n",
              "                      [-0.0246, -0.0128, -0.0148,  ...,  0.0150, -0.0085, -0.0280],\n",
              "                      ...,\n",
              "                      [-0.0250,  0.0215,  0.0237,  ...,  0.0562, -0.0334, -0.0429],\n",
              "                      [-0.0211, -0.0063,  0.0168,  ..., -0.0052, -0.0370, -0.0075],\n",
              "                      [-0.0176, -0.0228, -0.0047,  ...,  0.0072,  0.0253,  0.0055]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.29.mlp.up_proj.weight',\n",
              "              tensor([[-0.0251, -0.0194, -0.0249,  ...,  0.0360, -0.0092,  0.0046],\n",
              "                      [-0.0061,  0.0208,  0.0204,  ...,  0.0438, -0.0061,  0.0426],\n",
              "                      [ 0.0131, -0.0255,  0.0100,  ...,  0.0097,  0.0214, -0.0150],\n",
              "                      ...,\n",
              "                      [ 0.0322, -0.0326, -0.0206,  ..., -0.0381,  0.0001,  0.0542],\n",
              "                      [ 0.0160, -0.0150, -0.0471,  ..., -0.0087, -0.0323,  0.0112],\n",
              "                      [-0.0047,  0.0333,  0.0467,  ..., -0.0041,  0.0063, -0.0167]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.29.mlp.down_proj.weight',\n",
              "              tensor([[ 0.0211, -0.0011, -0.0243,  ...,  0.0342,  0.0017,  0.0080],\n",
              "                      [-0.0318, -0.0110, -0.0238,  ...,  0.0016,  0.0387,  0.0271],\n",
              "                      [ 0.0288,  0.0032,  0.0139,  ...,  0.0010,  0.0296, -0.0030],\n",
              "                      ...,\n",
              "                      [-0.0357,  0.0056, -0.0186,  ...,  0.0524, -0.0025, -0.0126],\n",
              "                      [ 0.0382, -0.0217, -0.0101,  ..., -0.0361, -0.0019,  0.0111],\n",
              "                      [-0.0003, -0.0196, -0.0383,  ..., -0.0115,  0.0028, -0.0588]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.29.input_layernorm.weight',\n",
              "              tensor([0.4717, 0.5142, 0.5107,  ..., 0.5117, 0.5244, 0.5254],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.29.post_attention_layernorm.weight',\n",
              "              tensor([0.4668, 0.4617, 0.4543,  ..., 0.4434, 0.4609, 0.4556],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.30.self_attn.q_proj.weight',\n",
              "              tensor([[ 0.0124,  0.0133,  0.0015,  ...,  0.0047,  0.0023,  0.0011],\n",
              "                      [ 0.0116,  0.0178, -0.0082,  ...,  0.0066, -0.0159, -0.0023],\n",
              "                      [ 0.0210,  0.0039,  0.0075,  ..., -0.0008,  0.0201, -0.0034],\n",
              "                      ...,\n",
              "                      [-0.0259,  0.0104,  0.0250,  ..., -0.0140,  0.0166, -0.0018],\n",
              "                      [ 0.0151, -0.0167,  0.0043,  ..., -0.0256, -0.0444,  0.0354],\n",
              "                      [ 0.0315,  0.0137, -0.0027,  ..., -0.0220,  0.0165, -0.0332]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.30.self_attn.k_proj.weight',\n",
              "              tensor([[-2.8549e-02,  1.5625e-02,  9.2888e-04,  ..., -8.9979e-04,\n",
              "                        1.1909e-02, -7.1869e-03],\n",
              "                      [-2.9922e-02,  1.2291e-02, -9.2864e-05,  ...,  4.0054e-03,\n",
              "                        9.5673e-03, -6.9580e-03],\n",
              "                      [-4.6654e-03, -1.0025e-02,  6.6566e-03,  ..., -2.3987e-02,\n",
              "                       -2.9430e-03, -9.3765e-03],\n",
              "                      ...,\n",
              "                      [ 5.1155e-03, -2.4048e-02,  2.4834e-03,  ..., -3.3783e-02,\n",
              "                        1.1833e-02,  1.5358e-02],\n",
              "                      [-2.9251e-02,  1.8478e-02, -9.0942e-03,  ..., -7.4097e-02,\n",
              "                       -1.7746e-02, -1.8326e-02],\n",
              "                      [ 4.3945e-02,  2.7252e-02,  2.5146e-02,  ...,  2.8014e-04,\n",
              "                        7.4539e-03, -1.6098e-02]], dtype=torch.float16)),\n",
              "             ('model.model.layers.30.self_attn.v_proj.weight',\n",
              "              tensor([[-0.0165, -0.0529, -0.0378,  ...,  0.0077, -0.0169,  0.0039],\n",
              "                      [ 0.0173, -0.0143, -0.0332,  ..., -0.0264, -0.0161,  0.0206],\n",
              "                      [-0.0035, -0.0159,  0.0371,  ...,  0.0083,  0.0309,  0.0064],\n",
              "                      ...,\n",
              "                      [ 0.0322, -0.0435,  0.0185,  ..., -0.0160, -0.0194,  0.0032],\n",
              "                      [-0.0106, -0.0179, -0.0383,  ..., -0.0151,  0.0042,  0.0255],\n",
              "                      [-0.0641, -0.0450,  0.0262,  ..., -0.0131,  0.0051,  0.0112]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.30.self_attn.o_proj.weight',\n",
              "              tensor([[-0.0255,  0.0050, -0.0067,  ..., -0.0096, -0.0067,  0.0016],\n",
              "                      [ 0.0031,  0.0009, -0.0264,  ..., -0.0323, -0.0107,  0.0039],\n",
              "                      [ 0.0462,  0.0705, -0.0111,  ..., -0.0081,  0.0358,  0.0036],\n",
              "                      ...,\n",
              "                      [-0.0029,  0.0212, -0.0017,  ..., -0.0122,  0.0114, -0.0179],\n",
              "                      [-0.0199, -0.0230,  0.0158,  ...,  0.0215, -0.0073, -0.0163],\n",
              "                      [-0.0194, -0.0618, -0.0006,  ..., -0.0350, -0.0132,  0.0005]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.30.mlp.gate_proj.weight',\n",
              "              tensor([[ 6.8247e-05, -1.6525e-02, -1.9970e-03,  ..., -1.5869e-02,\n",
              "                       -1.8494e-02,  1.2962e-02],\n",
              "                      [ 8.2703e-03,  3.2898e-02,  6.9580e-03,  ..., -3.5400e-02,\n",
              "                       -3.2837e-02,  2.5818e-02],\n",
              "                      [ 1.1055e-02, -2.0905e-02, -2.6001e-02,  ...,  2.6062e-02,\n",
              "                       -2.8534e-03,  1.4648e-02],\n",
              "                      ...,\n",
              "                      [-3.0075e-02,  8.5211e-04, -1.9653e-02,  ...,  3.1021e-02,\n",
              "                       -2.0813e-02,  2.6398e-02],\n",
              "                      [-7.8869e-04, -4.5715e-02,  1.9928e-02,  ..., -6.4240e-03,\n",
              "                       -3.2501e-02,  1.7578e-02],\n",
              "                      [-6.3248e-03, -8.2245e-03, -3.3054e-03,  ...,  1.3939e-02,\n",
              "                       -4.5288e-02, -1.2390e-02]], dtype=torch.float16)),\n",
              "             ('model.model.layers.30.mlp.up_proj.weight',\n",
              "              tensor([[-1.2398e-02,  1.4048e-03, -3.6865e-02,  ..., -9.3994e-03,\n",
              "                       -3.0716e-02, -5.8319e-02],\n",
              "                      [-2.9785e-02, -2.2339e-02,  8.7433e-03,  ..., -7.1220e-03,\n",
              "                       -6.8235e-04, -3.9459e-02],\n",
              "                      [-3.5191e-03,  1.1223e-02, -2.2079e-02,  ...,  5.8289e-03,\n",
              "                       -3.6041e-02,  5.2643e-03],\n",
              "                      ...,\n",
              "                      [-3.3478e-02,  1.5793e-02,  1.1284e-02,  ..., -2.3239e-02,\n",
              "                        3.3752e-02,  7.2327e-02],\n",
              "                      [ 1.4381e-02,  7.7605e-05, -2.2644e-02,  ..., -3.6865e-02,\n",
              "                       -2.7420e-02, -5.6744e-04],\n",
              "                      [ 4.9744e-03, -9.6130e-03,  2.9449e-02,  ...,  3.5217e-02,\n",
              "                       -3.2013e-02, -9.6359e-03]], dtype=torch.float16)),\n",
              "             ('model.model.layers.30.mlp.down_proj.weight',\n",
              "              tensor([[-0.0171,  0.0200,  0.0255,  ..., -0.0605,  0.0015, -0.0146],\n",
              "                      [ 0.0393,  0.0204,  0.0126,  ...,  0.0061, -0.0199,  0.0143],\n",
              "                      [ 0.0234,  0.0088,  0.0105,  ...,  0.0109,  0.0404,  0.0093],\n",
              "                      ...,\n",
              "                      [-0.0382,  0.0058,  0.0683,  ..., -0.0175, -0.0113,  0.0323],\n",
              "                      [ 0.0098, -0.0150,  0.0034,  ...,  0.0464, -0.0005, -0.0014],\n",
              "                      [ 0.0355,  0.0113, -0.0050,  ..., -0.0109,  0.0044, -0.0044]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.30.input_layernorm.weight',\n",
              "              tensor([0.4712, 0.5015, 0.5034,  ..., 0.5059, 0.5176, 0.5127],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.30.post_attention_layernorm.weight',\n",
              "              tensor([0.4561, 0.4519, 0.4543,  ..., 0.4417, 0.4548, 0.4502],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.31.self_attn.q_proj.weight',\n",
              "              tensor([[-0.0019,  0.0049, -0.0112,  ...,  0.0484,  0.0052, -0.0126],\n",
              "                      [ 0.0151,  0.0270, -0.0292,  ...,  0.0205, -0.0056, -0.0260],\n",
              "                      [-0.0327,  0.0025, -0.0121,  ...,  0.0050, -0.0057, -0.0219],\n",
              "                      ...,\n",
              "                      [ 0.0332, -0.0170,  0.0083,  ...,  0.0045,  0.0233, -0.0072],\n",
              "                      [ 0.0030,  0.0216, -0.0384,  ...,  0.0254,  0.0234, -0.0382],\n",
              "                      [-0.0800, -0.0216, -0.0187,  ..., -0.0005,  0.0180,  0.0054]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.31.self_attn.k_proj.weight',\n",
              "              tensor([[-0.0050, -0.0195, -0.0022,  ...,  0.0230, -0.0012, -0.0129],\n",
              "                      [-0.0217, -0.0200, -0.0245,  ..., -0.0142, -0.0167,  0.0215],\n",
              "                      [-0.0182,  0.0143,  0.0014,  ...,  0.0019,  0.0098, -0.0038],\n",
              "                      ...,\n",
              "                      [ 0.0174,  0.0386, -0.0381,  ...,  0.0039, -0.0033, -0.0196],\n",
              "                      [-0.0129,  0.0085, -0.0301,  ..., -0.0467,  0.0090, -0.0130],\n",
              "                      [-0.0176,  0.0037, -0.0076,  ..., -0.0160, -0.0024,  0.0006]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.31.self_attn.v_proj.weight',\n",
              "              tensor([[-0.0284, -0.0171, -0.0228,  ...,  0.0144, -0.0158, -0.0248],\n",
              "                      [ 0.0082, -0.0333,  0.0047,  ...,  0.0132, -0.0030,  0.0234],\n",
              "                      [-0.0375,  0.0057,  0.0115,  ..., -0.0021,  0.0285, -0.0058],\n",
              "                      ...,\n",
              "                      [ 0.0005,  0.0147, -0.0134,  ..., -0.0013,  0.0274,  0.0057],\n",
              "                      [-0.0064, -0.0029,  0.0523,  ...,  0.0090, -0.0053, -0.0022],\n",
              "                      [-0.0110, -0.0061, -0.0257,  ..., -0.0153,  0.0372, -0.0043]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.31.self_attn.o_proj.weight',\n",
              "              tensor([[ 0.0031,  0.0171,  0.0186,  ..., -0.0090, -0.0159, -0.0199],\n",
              "                      [-0.0106,  0.0112, -0.0198,  ...,  0.0176, -0.0058,  0.0134],\n",
              "                      [-0.0171,  0.0130,  0.0053,  ..., -0.0065,  0.0091, -0.0042],\n",
              "                      ...,\n",
              "                      [-0.0078,  0.0269,  0.0261,  ..., -0.0228,  0.0081, -0.0109],\n",
              "                      [ 0.0216,  0.0123,  0.0030,  ...,  0.0103, -0.0203,  0.0330],\n",
              "                      [ 0.0471,  0.0093,  0.0031,  ..., -0.0004,  0.0419,  0.0514]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.31.mlp.gate_proj.weight',\n",
              "              tensor([[-0.0488,  0.0225, -0.0016,  ...,  0.0008, -0.0265, -0.0079],\n",
              "                      [-0.0172, -0.0430, -0.0052,  ..., -0.0313,  0.0133, -0.0057],\n",
              "                      [-0.0177,  0.0352, -0.0045,  ...,  0.0030, -0.0225,  0.0090],\n",
              "                      ...,\n",
              "                      [-0.0306,  0.0301,  0.0041,  ..., -0.0081, -0.0097,  0.0271],\n",
              "                      [-0.0067, -0.0357,  0.0142,  ..., -0.0015, -0.0104, -0.0190],\n",
              "                      [-0.0089,  0.0563, -0.0251,  ...,  0.0412,  0.0063, -0.0020]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.31.mlp.up_proj.weight',\n",
              "              tensor([[ 0.0410,  0.0150, -0.0090,  ..., -0.0011, -0.0002,  0.0688],\n",
              "                      [ 0.0112, -0.0121,  0.0412,  ...,  0.0012, -0.0503, -0.0060],\n",
              "                      [-0.0210, -0.0415,  0.0234,  ...,  0.0174,  0.0297,  0.0161],\n",
              "                      ...,\n",
              "                      [-0.0324,  0.0477, -0.0042,  ...,  0.0203, -0.0135,  0.0256],\n",
              "                      [ 0.0073,  0.0362, -0.0131,  ...,  0.0043, -0.0029,  0.0094],\n",
              "                      [ 0.0114,  0.0261,  0.0363,  ..., -0.0134, -0.0132,  0.0157]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.31.mlp.down_proj.weight',\n",
              "              tensor([[ 0.0307, -0.0110,  0.0506,  ...,  0.0424,  0.0244,  0.0448],\n",
              "                      [-0.0076, -0.0290, -0.0024,  ..., -0.0015,  0.0143,  0.0526],\n",
              "                      [ 0.0518, -0.0100, -0.0296,  ..., -0.0124,  0.0134, -0.0310],\n",
              "                      ...,\n",
              "                      [-0.0007,  0.0127,  0.0298,  ...,  0.0065,  0.0157,  0.0044],\n",
              "                      [-0.0129,  0.0381,  0.0064,  ..., -0.0445,  0.0087,  0.0371],\n",
              "                      [ 0.0369, -0.0142,  0.0257,  ...,  0.0100, -0.0355,  0.0216]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.31.input_layernorm.weight',\n",
              "              tensor([0.3289, 0.4109, 0.4275,  ..., 0.4087, 0.4031, 0.4290],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.layers.31.post_attention_layernorm.weight',\n",
              "              tensor([0.3950, 0.3918, 0.4197,  ..., 0.4011, 0.4187, 0.4082],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.model.norm.weight',\n",
              "              tensor([1.8828, 1.5605, 1.6426,  ..., 1.7158, 1.6602, 1.5781],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.lm_head.weight',\n",
              "              tensor([[-0.0150,  0.0009,  0.0042,  ..., -0.0017, -0.0145, -0.0085],\n",
              "                      [ 0.0191, -0.0437,  0.0180,  ..., -0.0190, -0.0642,  0.0194],\n",
              "                      [ 0.0218,  0.0147,  0.0292,  ...,  0.0344,  0.0157,  0.0244],\n",
              "                      ...,\n",
              "                      [-0.0236, -0.0140,  0.0330,  ..., -0.0258,  0.0103, -0.0037],\n",
              "                      [-0.0274,  0.0042, -0.0012,  ...,  0.0203, -0.0111, -0.0093],\n",
              "                      [-0.0031, -0.0197, -0.0013,  ...,  0.0135,  0.0029, -0.0267]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('medusa_head1.linear1.weight',\n",
              "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "                      ...,\n",
              "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)),\n",
              "             ('medusa_head1.linear1.bias',\n",
              "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "             ('medusa_head1.linear2.weight',\n",
              "              tensor([[-0.0150,  0.0009,  0.0042,  ..., -0.0017, -0.0145, -0.0085],\n",
              "                      [ 0.0191, -0.0437,  0.0180,  ..., -0.0190, -0.0642,  0.0194],\n",
              "                      [ 0.0218,  0.0147,  0.0292,  ...,  0.0344,  0.0157,  0.0244],\n",
              "                      ...,\n",
              "                      [-0.0236, -0.0140,  0.0330,  ..., -0.0258,  0.0103, -0.0037],\n",
              "                      [-0.0274,  0.0042, -0.0012,  ...,  0.0203, -0.0111, -0.0093],\n",
              "                      [-0.0031, -0.0197, -0.0013,  ...,  0.0135,  0.0029, -0.0267]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('medusa_head1.linear2.bias',\n",
              "              tensor([-0.0125,  0.0131, -0.0104,  ...,  0.0028, -0.0007, -0.0068],\n",
              "                     dtype=torch.float16)),\n",
              "             ('medusa_head2.linear1.weight',\n",
              "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "                      ...,\n",
              "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)),\n",
              "             ('medusa_head2.linear1.bias',\n",
              "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "             ('medusa_head2.linear2.weight',\n",
              "              tensor([[-0.0150,  0.0009,  0.0042,  ..., -0.0017, -0.0145, -0.0085],\n",
              "                      [ 0.0191, -0.0437,  0.0180,  ..., -0.0190, -0.0642,  0.0194],\n",
              "                      [ 0.0218,  0.0147,  0.0292,  ...,  0.0344,  0.0157,  0.0244],\n",
              "                      ...,\n",
              "                      [-0.0236, -0.0140,  0.0330,  ..., -0.0258,  0.0103, -0.0037],\n",
              "                      [-0.0274,  0.0042, -0.0012,  ...,  0.0203, -0.0111, -0.0093],\n",
              "                      [-0.0031, -0.0197, -0.0013,  ...,  0.0135,  0.0029, -0.0267]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('medusa_head2.linear2.bias',\n",
              "              tensor([-0.0012,  0.0069,  0.0100,  ...,  0.0102,  0.0018, -0.0069],\n",
              "                     dtype=torch.float16)),\n",
              "             ('medusa_head3.linear1.weight',\n",
              "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "                      ...,\n",
              "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)),\n",
              "             ('medusa_head3.linear1.bias',\n",
              "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "             ('medusa_head3.linear2.weight',\n",
              "              tensor([[-0.0150,  0.0009,  0.0042,  ..., -0.0017, -0.0145, -0.0085],\n",
              "                      [ 0.0191, -0.0437,  0.0180,  ..., -0.0190, -0.0642,  0.0194],\n",
              "                      [ 0.0218,  0.0147,  0.0292,  ...,  0.0344,  0.0157,  0.0244],\n",
              "                      ...,\n",
              "                      [-0.0236, -0.0140,  0.0330,  ..., -0.0258,  0.0103, -0.0037],\n",
              "                      [-0.0274,  0.0042, -0.0012,  ...,  0.0203, -0.0111, -0.0093],\n",
              "                      [-0.0031, -0.0197, -0.0013,  ...,  0.0135,  0.0029, -0.0267]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('medusa_head3.linear2.bias',\n",
              "              tensor([ 0.0104, -0.0059,  0.0110,  ...,  0.0135,  0.0003, -0.0005],\n",
              "                     dtype=torch.float16)),\n",
              "             ('medusa_head4.linear1.weight',\n",
              "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "                      ...,\n",
              "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)),\n",
              "             ('medusa_head4.linear1.bias',\n",
              "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "             ('medusa_head4.linear2.weight',\n",
              "              tensor([[-0.0150,  0.0009,  0.0042,  ..., -0.0017, -0.0145, -0.0085],\n",
              "                      [ 0.0191, -0.0437,  0.0180,  ..., -0.0190, -0.0642,  0.0194],\n",
              "                      [ 0.0218,  0.0147,  0.0292,  ...,  0.0344,  0.0157,  0.0244],\n",
              "                      ...,\n",
              "                      [-0.0236, -0.0140,  0.0330,  ..., -0.0258,  0.0103, -0.0037],\n",
              "                      [-0.0274,  0.0042, -0.0012,  ...,  0.0203, -0.0111, -0.0093],\n",
              "                      [-0.0031, -0.0197, -0.0013,  ...,  0.0135,  0.0029, -0.0267]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('medusa_head4.linear2.bias',\n",
              "              tensor([-0.0141,  0.0019, -0.0017,  ...,  0.0114, -0.0020, -0.0003],\n",
              "                     dtype=torch.float16)),\n",
              "             ('medusa_head5.linear1.weight',\n",
              "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "                      ...,\n",
              "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)),\n",
              "             ('medusa_head5.linear1.bias',\n",
              "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "             ('medusa_head5.linear2.weight',\n",
              "              tensor([[-0.0150,  0.0009,  0.0042,  ..., -0.0017, -0.0145, -0.0085],\n",
              "                      [ 0.0191, -0.0437,  0.0180,  ..., -0.0190, -0.0642,  0.0194],\n",
              "                      [ 0.0218,  0.0147,  0.0292,  ...,  0.0344,  0.0157,  0.0244],\n",
              "                      ...,\n",
              "                      [-0.0236, -0.0140,  0.0330,  ..., -0.0258,  0.0103, -0.0037],\n",
              "                      [-0.0274,  0.0042, -0.0012,  ...,  0.0203, -0.0111, -0.0093],\n",
              "                      [-0.0031, -0.0197, -0.0013,  ...,  0.0135,  0.0029, -0.0267]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('medusa_head5.linear2.bias',\n",
              "              tensor([-0.0042,  0.0013, -0.0154,  ...,  0.0151, -0.0043,  0.0091],\n",
              "                     dtype=torch.float16))])"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.state_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3Qnv6kQ5iXP",
        "outputId": "6a6dfc24-28db-40e1-e850-967ca4e32055"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16,\n",
              "       requires_grad=True)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.medusa_head1.linear1.weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBej5ELg4L2K"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def prepare_model_for_gguf_conversion(model, output_dir):\n",
        "    \"\"\"\n",
        "    Prepare a custom model for GGUF conversion by restructuring its state dict\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): Custom model to prepare\n",
        "        output_dir (str): Directory to save prepared model\n",
        "    \"\"\"\n",
        "    # Ensure output directory exists\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Get the state dict\n",
        "    state_dict = model.state_dict()\n",
        "\n",
        "    # Create a new state dict with standard HuggingFace naming\n",
        "    new_state_dict = {}\n",
        "\n",
        "    # Map custom model layers to standard HuggingFace naming\n",
        "    layer_mapping = {\n",
        "        'model.embed_tokens.weight': 'model.model.embed_tokens.weight',\n",
        "        'model.layers': 'model.model.layers',\n",
        "        'model.norm.weight': 'model.model.norm.weight',\n",
        "        'model.lm_head.weight': 'model.lm_head.weight'\n",
        "    }\n",
        "\n",
        "    # Custom head mapping (optional, modify as needed)\n",
        "    for i in range(1, 6):\n",
        "        head_key = f'medusa_head{i}.linear1'\n",
        "        new_state_dict[f'medusa_head{i}.linear1.weight'] = state_dict[f'{head_key}.weight']\n",
        "\n",
        "        if f'{head_key}.bias' in state_dict:\n",
        "            new_state_dict[f'medusa_head{i}.linear1.bias'] = state_dict[f'{head_key}.bias']\n",
        "\n",
        "        head_key = f'medusa_head{i}.linear2'\n",
        "        new_state_dict[f'medusa_head{i}.linear2.weight'] = state_dict[f'{head_key}.weight']\n",
        "\n",
        "        if f'{head_key}.bias' in state_dict:\n",
        "            new_state_dict[f'medusa_head{i}.linear2.bias'] = state_dict[f'{head_key}.bias']\n",
        "\n",
        "\n",
        "    # Manually map base model layers\n",
        "    for key, value in state_dict.items():\n",
        "        # Look for keys that match standard HuggingFace model structure\n",
        "        for prefix, replacement in layer_mapping.items():\n",
        "            if key.startswith(prefix):\n",
        "                new_key = key.replace(prefix, replacement)\n",
        "                new_state_dict[new_key] = value\n",
        "\n",
        "    # Save the restructured model\n",
        "    pytorch_model_path = os.path.join(output_dir, 'pytorch_model.bin')\n",
        "    torch.save(new_state_dict, pytorch_model_path)\n",
        "\n",
        "    # Copy or recreate the config\n",
        "    config = model.model.config.to_dict()\n",
        "    config['custom_heads'] = [f'medusa_head{i}' for i in range(1, 6)]\n",
        "\n",
        "    config_path = os.path.join(output_dir, 'config.json')\n",
        "    with open(config_path, 'w') as f:\n",
        "        json.dump(config, f, indent=2)\n",
        "\n",
        "    # Copy tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model.model.config._name_or_path)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    return output_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYxeU_AQ3H-o"
      },
      "outputs": [],
      "source": [
        "!mkdir vicuna-7b-with-medusa\n",
        "!mkdir vicuna-7b-with-medusa2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NIp4Er0E46_B",
        "outputId": "586e7c53-d742-4a80-dfa8-0324f9d2b01f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'vicuna-7b-with-medusa'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prepare_model_for_gguf_conversion(model, 'vicuna-7b-with-medusa')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "X5p5UlmVA-BM",
        "outputId": "dfa7a06e-2a83-4fcb-9d7f-fa7e49ce041a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'vicuna-7b-with-medusa2'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prepare_model_for_gguf_conversion(model, 'vicuna-7b-with-medusa2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llT0InwbKhdx",
        "outputId": "29545936-3c93-471a-9486-a634f87b6923"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to vicuna-7b-with-medusa\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('vicuna-7b-with-medusa/tokenizer_config.json',\n",
              " 'vicuna-7b-with-medusa/special_tokens_map.json',\n",
              " 'vicuna-7b-with-medusa/tokenizer.model',\n",
              " 'vicuna-7b-with-medusa/added_tokens.json',\n",
              " 'vicuna-7b-with-medusa/tokenizer.json')"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save as GGML format using llama.cpp compatibility\n",
        "model.save_pretrained(\"vicuna-7b-with-medusa\")\n",
        "tokenizer.save_pretrained(\"vicuna-7b-with-medusa\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zblyP6pOLfei"
      },
      "outputs": [],
      "source": [
        "!mkdir quantized_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Suudz9p9K2C2",
        "outputId": "f3682ab0-541f-48ee-f9cf-b169c951eef9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:hf-to-gguf:Loading model: vicuna-7b-with-medusa2\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model.bin'\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/llama.cpp/convert_hf_to_gguf.py\", line 4436, in <module>\n",
            "    main()\n",
            "  File \"/content/llama.cpp/convert_hf_to_gguf.py\", line 4430, in main\n",
            "    model_instance.write()\n",
            "  File \"/content/llama.cpp/convert_hf_to_gguf.py\", line 434, in write\n",
            "    self.prepare_tensors()\n",
            "  File \"/content/llama.cpp/convert_hf_to_gguf.py\", line 1657, in prepare_tensors\n",
            "    super().prepare_tensors()\n",
            "  File \"/content/llama.cpp/convert_hf_to_gguf.py\", line 298, in prepare_tensors\n",
            "    for new_name, data_torch in (self.modify_tensors(data_torch, name, bid)):\n",
            "  File \"/content/llama.cpp/convert_hf_to_gguf.py\", line 1625, in modify_tensors\n",
            "    return [(self.map_tensor_name(name), data_torch)]\n",
            "  File \"/content/llama.cpp/convert_hf_to_gguf.py\", line 214, in map_tensor_name\n",
            "    raise ValueError(f\"Can not map tensor {name!r}\")\n",
            "ValueError: Can not map tensor 'model.model.embed_tokens.weight'\n"
          ]
        }
      ],
      "source": [
        "!python3 convert_hf_to_gguf.py /content/llama.cpp/vicuna-7b-with-medusa2 --model-name vicuna-7b-v1.3-medusa --outfile /content/llama.cpp/quantized_models/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scZyYtGpzmrr",
        "outputId": "cb125933-2c59-4f99-f904-ac09aeade2be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_with_medusa(\n",
            "  (base_model): LlamaForCausalLM(\n",
            "    (model): LlamaModel(\n",
            "      (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
            "      (layers): ModuleList(\n",
            "        (0-31): 32 x LlamaDecoderLayer(\n",
            "          (self_attn): LlamaSdpaAttention(\n",
            "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "            (rotary_emb): LlamaRotaryEmbedding()\n",
            "          )\n",
            "          (mlp): LlamaMLP(\n",
            "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
            "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
            "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
            "            (act_fn): SiLU()\n",
            "          )\n",
            "          (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
            "          (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
            "        )\n",
            "      )\n",
            "      (norm): LlamaRMSNorm((4096,), eps=1e-06)\n",
            "      (rotary_emb): LlamaRotaryEmbedding()\n",
            "    )\n",
            "    (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
            "  )\n",
            "  (medusa_head1): medusahead(\n",
            "    (linear1): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (linear2): Linear(in_features=4096, out_features=32000, bias=True)\n",
            "    (silu): SiLU()\n",
            "  )\n",
            "  (medusa_head2): medusahead(\n",
            "    (linear1): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (linear2): Linear(in_features=4096, out_features=32000, bias=True)\n",
            "    (silu): SiLU()\n",
            "  )\n",
            "  (medusa_head3): medusahead(\n",
            "    (linear1): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (linear2): Linear(in_features=4096, out_features=32000, bias=True)\n",
            "    (silu): SiLU()\n",
            "  )\n",
            "  (medusa_head4): medusahead(\n",
            "    (linear1): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (linear2): Linear(in_features=4096, out_features=32000, bias=True)\n",
            "    (silu): SiLU()\n",
            "  )\n",
            "  (medusa_head5): medusahead(\n",
            "    (linear1): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (linear2): Linear(in_features=4096, out_features=32000, bias=True)\n",
            "    (silu): SiLU()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation from Model with Medusa"
      ],
      "metadata": {
        "id": "VUouycDVW5sG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvQjIsmt44bu"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "prompt = \"The meaning of life is\"\n",
        "input_ids = base_tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "max_length = 50  # Set maximum sequence length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDWiXiaj5iob",
        "outputId": "3d04db83-1a47-4a87-cf2c-e649be453a50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 6])\n"
          ]
        }
      ],
      "source": [
        "print(input_ids.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TqbvhdZeTfn",
        "outputId": "00e75cfb-975f-4b22-9338-cdd2d55e30ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 6])\n"
          ]
        }
      ],
      "source": [
        "print(input_ids.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7K0BGQ99DsR",
        "outputId": "6135f1fd-5649-477d-e458-71206633fe7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(450)\n"
          ]
        }
      ],
      "source": [
        "print(input_ids[0][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQGFEM3e5-DR",
        "outputId": "8bd98f9e-552a-4096-e403-af5af1285239"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hidden state data type: torch.float16\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "  outputs = model(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7F_FvM8_fQcU"
      },
      "outputs": [],
      "source": [
        "def decoding(tokenizer, output):\n",
        "  # If logits are present, convert them to token IDs (e.g., argmax for greedy decoding)\n",
        "  token_ids= []\n",
        "  for k in output:\n",
        "    # print(k)\n",
        "    token_ids.append((torch.argmax(output[k], dim=-1)))\n",
        "\n",
        "  print(f'length of token ids: {len(token_ids)}')\n",
        "  print(f'shape of one element in token ids: {(token_ids[0].shape)}')\n",
        "  for tokens in token_ids:\n",
        "    print(f'token shape: {tokens.shape}')\n",
        "    print(tokens)\n",
        "    print(tokens[0][-1])\n",
        "    print(tokens[0][-1].shape)\n",
        "\n",
        "  x= torch.tensor([tokens[0][-1] for tokens in token_ids]).unsqueeze(0)\n",
        "  print(x)\n",
        "  print(x.shape)\n",
        "\n",
        "  # Decode token IDs into text\n",
        "  decoded_texts= []\n",
        "  for tkn_id in token_ids:\n",
        "    decoded_texts.append(tokenizer.batch_decode(tkn_id, skip_special_tokens=True))\n",
        "\n",
        "  # Print the results\n",
        "  for idx, text in enumerate(decoded_texts):\n",
        "      print(f\"Output {idx + 1}: {text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNzbI8KWfQGw",
        "outputId": "51fb5be7-a50d-4a03-9f52-ce1ffa2aaeee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of token ids: 6\n",
            "shape of one element in token ids: torch.Size([1, 6])\n",
            "token shape: torch.Size([1, 6])\n",
            "tensor([[29906, 29871,   310,   278,   338,   304]])\n",
            "tensor(304)\n",
            "torch.Size([])\n",
            "token shape: torch.Size([1, 6])\n",
            "tensor([[29906, 29871,   310,   278,   338,   304]])\n",
            "tensor(304)\n",
            "torch.Size([])\n",
            "token shape: torch.Size([1, 6])\n",
            "tensor([[29906,   937,   310,   278,   338,   304]])\n",
            "tensor(304)\n",
            "torch.Size([])\n",
            "token shape: torch.Size([1, 6])\n",
            "tensor([[29906, 29871,   310,   278,   338,   304]])\n",
            "tensor(304)\n",
            "torch.Size([])\n",
            "token shape: torch.Size([1, 6])\n",
            "tensor([[29906, 29871,   310,   278,   338,   304]])\n",
            "tensor(304)\n",
            "torch.Size([])\n",
            "token shape: torch.Size([1, 6])\n",
            "tensor([[29906, 29871,   310,   278,   338,   304]])\n",
            "tensor(304)\n",
            "torch.Size([])\n",
            "tensor([[304, 304, 304, 304, 304, 304]])\n",
            "torch.Size([1, 6])\n",
            "Output 1: ['2  of the is to']\n",
            "Output 2: ['2  of the is to']\n",
            "Output 3: ['2 first of the is to']\n",
            "Output 4: ['2  of the is to']\n",
            "Output 5: ['2  of the is to']\n",
            "Output 6: ['2  of the is to']\n"
          ]
        }
      ],
      "source": [
        "decoding(base_tokenizer, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "actsTKtOdWad",
        "outputId": "83c1aced-bc37-4b1a-e398-cbabdacba878"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 6, 32000])"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs['logits'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZzWrYRSdslX",
        "outputId": "50fe0f9a-4f11-414a-ab94-f00ce125b867"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(304)\n"
          ]
        }
      ],
      "source": [
        "print(torch.argmax(outputs['logits'][0][5], dim=-1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fj-H-nfJkJ13"
      },
      "source": [
        "### Speculative Decoding Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XieuGQ9X3MaF"
      },
      "outputs": [],
      "source": [
        "class SpeculativeDecoder:\n",
        "    def __init__(self, model, tokenizer, max_length=128):\n",
        "        \"\"\"\n",
        "        Initialize the speculative decoder.\n",
        "\n",
        "        Args:\n",
        "        - model: The `model_with_medusa` instance.\n",
        "        - tokenizer: Tokenizer corresponding to the base model.\n",
        "        - max_length: Maximum length for generated tokens.\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def decode(self, prompt, alpha=1):\n",
        "        \"\"\"\n",
        "        Perform speculative decoding.\n",
        "\n",
        "        Args:\n",
        "        - prompt: The input text prompt.\n",
        "        - alpha: The speculative decoding factor (higher values mean more aggressive speculative decoding).\n",
        "\n",
        "        Returns:\n",
        "        - Generated text.\n",
        "        \"\"\"\n",
        "        # Tokenize the input prompt\n",
        "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "        # Initialize the sequence and attention mask\n",
        "        sequence = input_ids\n",
        "        attention_mask = torch.ones_like(sequence)\n",
        "        token= 0\n",
        "        medusa_pred_tokens= []\n",
        "\n",
        "        while token < (self.max_length):\n",
        "\n",
        "\n",
        "          # 1. Use Medusa heads for faster predictions\n",
        "          outputs = self.model(input_ids=sequence)\n",
        "\n",
        "          token_ids= []\n",
        "\n",
        "          for k in outputs:\n",
        "            token_ids.append((torch.argmax(outputs[k], dim=-1)))\n",
        "\n",
        "          print(f'token_ids ka length: {len(token_ids)}')\n",
        "          print(f'token_ids ke element ka length: {(token_ids[0].shape)}')\n",
        "          if medusa_pred_tokens is None:\n",
        "            print('none ke andar')\n",
        "\n",
        "          if len(medusa_pred_tokens) == 0:\n",
        "            medusa_pred_tokens.extend(token_ids[1:6])\n",
        "            print(f'sequence ka shape: {sequence.shape}')\n",
        "            print(f'medusa pred tokens length: {len(medusa_pred_tokens)}')\n",
        "            print(f'medusa pred tokens shape: {medusa_pred_tokens[0].shape}')\n",
        "            print(f'token ids[0] shape: {token_ids[0].shape}')\n",
        "            print(f'token ids[0][-1] shape: {token_ids[0][-1].shape}')\n",
        "            print(f'token ids[0][:][-1] shape: {token_ids[0][:][-1].shape}')\n",
        "            print(f'token ids[0][0][-1].unsqueeze(0).unsqueeze(1) shape: {(token_ids[0][0][-1].unsqueeze(0).unsqueeze(1)).shape}')\n",
        "            print(f'token ids[0][0]: {token_ids[0][0]}')\n",
        "            print(f'token ids[0][0][-1]: {token_ids[0][0][-1]}')\n",
        "\n",
        "            # Append accepted tokens to the sequence\n",
        "            sequence = torch.cat([sequence, token_ids[0][0][-1].unsqueeze(0).unsqueeze(1)], dim=-1)\n",
        "\n",
        "            token += 1\n",
        "\n",
        "          else:\n",
        "            token += 1\n",
        "\n",
        "            if medusa_pred_tokens[0][0][-1] == token_ids[0][0][-6]:\n",
        "              token += 1\n",
        "\n",
        "              if medusa_pred_tokens[1][0][-1] == token_ids[0][0][-5]:\n",
        "                token += 1\n",
        "\n",
        "                if medusa_pred_tokens[2][0][-1] == token_ids[0][0][-4]:\n",
        "                  token += 1\n",
        "\n",
        "                  if medusa_pred_tokens[3][0][-1] == token_ids[0][0][-3]:\n",
        "                    token += 1\n",
        "\n",
        "                    if medusa_pred_tokens[4][0][-1] == token_ids[0][0][-2]:\n",
        "                      token += 1\n",
        "                      medusa_correct= torch.tensor([tokens[0][-1] for tokens in medusa_pred_tokens]).unsqueeze(0)\n",
        "                      print(f'medusa_correct shape: {medusa_correct.shape}')\n",
        "                      sequence = torch.cat([sequence , medusa_correct , token_ids[0][0][-1].unsqueeze(0).unsqueeze(1)], dim=-1)\n",
        "                      print(f'token: {token}')\n",
        "                      print(sequence)\n",
        "                      medusa_pred_tokens.extend(token_ids[1:6])\n",
        "\n",
        "\n",
        "                    else:\n",
        "                      medusa_correct= torch.tensor([tokens[0][-1] for tokens in medusa_pred_tokens[:4]]).unsqueeze(0)\n",
        "                      sequence = torch.cat([sequence , medusa_correct , token_ids[0][0][-2].unsqueeze(0).unsqueeze(1)], dim=-1)\n",
        "                      print(f'token: {token}')\n",
        "                      print(sequence)\n",
        "                      medusa_pred_tokens= []\n",
        "\n",
        "\n",
        "                  else:\n",
        "                    medusa_correct= torch.tensor([tokens[0][-1] for tokens in medusa_pred_tokens[:3]]).unsqueeze(0)\n",
        "                    sequence = torch.cat([sequence , medusa_correct , token_ids[0][0][-3].unsqueeze(0).unsqueeze(1)], dim=-1)\n",
        "                    print(f'token: {token}')\n",
        "                    print(sequence)\n",
        "                    medusa_pred_tokens= []\n",
        "\n",
        "\n",
        "                else:\n",
        "                  medusa_correct= torch.tensor([tokens[0][-1] for tokens in medusa_pred_tokens[:2]]).unsqueeze(0)\n",
        "                  sequence = torch.cat([sequence , medusa_correct , token_ids[0][0][-4].unsqueeze(0).unsqueeze(1)], dim=-1)\n",
        "                  print(f'token: {token}')\n",
        "                  print(sequence)\n",
        "                  medusa_pred_tokens= []\n",
        "\n",
        "\n",
        "              else:\n",
        "                medusa_correct= torch.tensor([tokens[0][-1] for tokens in medusa_pred_tokens[:1]]).unsqueeze(0)\n",
        "                sequence = torch.cat([sequence , medusa_correct , token_ids[0][0][-5].unsqueeze(0).unsqueeze(1)], dim=-1)\n",
        "                print(f'token: {token}')\n",
        "                print(sequence)\n",
        "                medusa_pred_tokens= []\n",
        "\n",
        "\n",
        "            else:\n",
        "              sequence = torch.cat([sequence , token_ids[0][0][-6].unsqueeze(0).unsqueeze(1)], dim=-1)\n",
        "              print(f'token: {token}')\n",
        "              print(sequence)\n",
        "              medusa_pred_tokens= []\n",
        "\n",
        "\n",
        "          # Stop if all tokens are accepted or EOS token is generated\n",
        "          if (sequence == self.tokenizer.eos_token_id).any():\n",
        "              break\n",
        "\n",
        "        # Decode the generated token IDs to text\n",
        "        generated_text = self.tokenizer.decode(sequence[0], skip_special_tokens=True)\n",
        "        return generated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ioXhKXJW1SC9",
        "outputId": "c72b17f4-5748-4ffd-b376-a3406c37904f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hidden state data type: torch.float16\n",
            "token_ids ka length: 6\n",
            "token_ids ke element ka length: torch.Size([1, 13])\n",
            "sequence ka shape: torch.Size([1, 13])\n",
            "medusa pred tokens length: 5\n",
            "medusa pred tokens shape: torch.Size([1, 13])\n",
            "token ids[0] shape: torch.Size([1, 13])\n",
            "token ids[0][-1] shape: torch.Size([13])\n",
            "token ids[0][:][-1] shape: torch.Size([13])\n",
            "token ids[0][0][-1].unsqueeze(0).unsqueeze(1) shape: torch.Size([1, 1])\n",
            "token ids[0][0]: tensor([29906,   366,   263,   931, 29892,   263,  2319, 29891,  2215, 29892,\n",
            "         2215,  3448, 29892])\n",
            "token ids[0][0][-1]: 29892\n",
            "hidden state data type: torch.float16\n",
            "token_ids ka length: 6\n",
            "token_ids ke element ka length: torch.Size([1, 14])\n",
            "token: 2\n",
            "tensor([[    1,  9038,  2501,   263,   931,   297,   263, 15400, 29891,  2215,\n",
            "         29892,  2215,  3448, 29892,  2215]])\n",
            "hidden state data type: torch.float16\n",
            "token_ids ka length: 6\n",
            "token_ids ke element ka length: torch.Size([1, 15])\n",
            "sequence ka shape: torch.Size([1, 15])\n",
            "medusa pred tokens length: 5\n",
            "medusa pred tokens shape: torch.Size([1, 15])\n",
            "token ids[0] shape: torch.Size([1, 15])\n",
            "token ids[0][-1] shape: torch.Size([15])\n",
            "token ids[0][:][-1] shape: torch.Size([15])\n",
            "token ids[0][0][-1].unsqueeze(0).unsqueeze(1) shape: torch.Size([1, 1])\n",
            "token ids[0][0]: tensor([29906,   366,   263,   931, 29892,   263,  2319, 29891,  2215, 29892,\n",
            "         2215,  3448, 29892,   727, 13269])\n",
            "token ids[0][0][-1]: 13269\n",
            "hidden state data type: torch.float16\n",
            "token_ids ka length: 6\n",
            "token_ids ke element ka length: torch.Size([1, 16])\n",
            "token: 4\n",
            "tensor([[    1,  9038,  2501,   263,   931,   297,   263, 15400, 29891,  2215,\n",
            "         29892,  2215,  3448, 29892,  2215, 13269,  2215]])\n",
            "hidden state data type: torch.float16\n",
            "token_ids ka length: 6\n",
            "token_ids ke element ka length: torch.Size([1, 17])\n",
            "sequence ka shape: torch.Size([1, 17])\n",
            "medusa pred tokens length: 5\n",
            "medusa pred tokens shape: torch.Size([1, 17])\n",
            "token ids[0] shape: torch.Size([1, 17])\n",
            "token ids[0][-1] shape: torch.Size([17])\n",
            "token ids[0][:][-1] shape: torch.Size([17])\n",
            "token ids[0][0][-1].unsqueeze(0).unsqueeze(1) shape: torch.Size([1, 1])\n",
            "token ids[0][0]: tensor([29906,   366,   263,   931, 29892,   263,  2319, 29891,  2215, 29892,\n",
            "         2215,  3448, 29892,   727, 13269,   297,  2168])\n",
            "token ids[0][0][-1]: 2168\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-46a8b6b1f797>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Perform speculative decoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Once upon a time in a galaxy far, far away\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mgenerated_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generated Text:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-82-8cb684aafe1f>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, prompt, alpha)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m           \u001b[0;31m# 1. Use Medusa heads for faster predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m           \u001b[0mtoken_ids\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-7fbeffc0fbfd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0;31m# Forward pass through the base model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0;31m# Extract the final hidden states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1191\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    943\u001b[0m                 )\n\u001b[1;32m    944\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 945\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    946\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n\u001b[0m\u001b[1;32m    677\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0mquery_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Example Usage\n",
        "base_model_name = \"lmsys/vicuna-7b-v1.3\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "\n",
        "# Initialize your model_with_medusa\n",
        "model = model_with_medusa(base_model_name)\n",
        "\n",
        "# Initialize the speculative decoder\n",
        "decoder = SpeculativeDecoder(model=model, tokenizer=base_tokenizer)\n",
        "\n",
        "# Perform speculative decoding\n",
        "prompt = \"Once upon a time in a galaxy far, far away\"\n",
        "generated_text = decoder.decode(prompt, alpha=1)\n",
        "\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8-1iB5p45UV"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Generate tokens using speculative decoding\n",
        "output_ids = speculative_decode(base_model, proxy_model, input_ids, max_length)\n",
        "output_text = base_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated text:\", output_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj1JC25uSHgv"
      },
      "source": [
        "### Dynamic Batching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQgoAAse45Ie"
      },
      "outputs": [],
      "source": [
        "import queue\n",
        "import time\n",
        "\n",
        "class DynamicBatching:\n",
        "    def __init__(self, model_name, max_batch_size=8, timeout=1.0):\n",
        "        self.max_batch_size = max_batch_size\n",
        "        self.timeout = timeout\n",
        "        self.request_queue = queue.Queue()\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def add_request(self, request):\n",
        "        \"\"\"Add a new request to the queue.\"\"\"\n",
        "        self.request_queue.put(request)\n",
        "\n",
        "    def process_batch(self, model):\n",
        "        \"\"\"Collect and process a batch of requests.\"\"\"\n",
        "        batch = []\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Collect requests until timeout or max batch size\n",
        "        while len(batch) < self.max_batch_size and (time.time() - start_time) < self.timeout:\n",
        "            if not self.request_queue.empty():\n",
        "                batch.append(self.request_queue.get())\n",
        "\n",
        "        # If a batch has been collected, process it\n",
        "        if batch:\n",
        "            # Perform inference (simulated here)\n",
        "            output = self.infer(batch, model)\n",
        "            return output\n",
        "        return None\n",
        "\n",
        "    def infer(self, batch, model):\n",
        "        \"\"\"Simulate inference on a batch.\"\"\"\n",
        "        print(f\"Processing batch of size {len(batch)}\")\n",
        "\n",
        "        # Initialize the speculative decoder\n",
        "        decoder = SpeculativeDecoder(model=model, tokenizer=base_tokenizer)\n",
        "\n",
        "        generated_text = decoder.decode(batch, alpha=1)\n",
        "\n",
        "        return generated_text  # Return the processed batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gboJtASCSOxh"
      },
      "outputs": [],
      "source": [
        "# testing\n",
        "\n",
        "# Initialize your model_with_medusa\n",
        "model = model_with_medusa(base_model_name)\n",
        "\n",
        "dynamic_batcher = DynamicBatching(max_batch_size=5, timeout=2)\n",
        "\n",
        "# Simulating incoming requests\n",
        "for _ in range(10):\n",
        "    dynamic_batcher.add_request(\"I love doing\")\n",
        "\n",
        "# Process and get the result\n",
        "output = dynamic_batcher.process_batch(model)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bj4nMkA6TI1a"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Appendix"
      ],
      "metadata": {
        "id": "36hyI16ziY7k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLJp86uN6fu1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class medusahead(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim, weights_data):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(input_dim, input_dim)\n",
        "    self.linear2 = nn.Linear(input_dim, output_dim)\n",
        "    self.silu   = nn.SiLU()\n",
        "\n",
        "    self._initialize_weights_to_zero(self.linear1)\n",
        "    # Ensure the weights of linear1 are in float16\n",
        "    self.linear1.weight.data = self.linear1.weight.data.half()\n",
        "    if self.linear1.bias is not None:\n",
        "        self.linear1.bias.data = self.linear1.bias.data.half()\n",
        "\n",
        "\n",
        "    self.linear2.weight.data.copy_(weights_data)\n",
        "    self.linear2.weight.data = self.linear2.weight.data.half()\n",
        "    if self.linear2.bias is not None:\n",
        "        self.linear2.bias.data = self.linear2.bias.data.half()\n",
        "\n",
        "\n",
        "  def _initialize_weights_to_zero(self, layer):\n",
        "    if hasattr(layer, \"weight\") and layer.weight is not None:\n",
        "      nn.init.constant_(layer.weight, 0.0)  # Set weights to zero\n",
        "    if hasattr(layer, \"bias\") and layer.bias is not None:\n",
        "      nn.init.constant_(layer.bias, 0.0)  # Set biases to zero\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual_x = x\n",
        "    x= self.linear1(x)\n",
        "    x= self.silu(x)\n",
        "    x= x + residual_x\n",
        "    output= self.linear2(x)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxQD0S-F8tGQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ParallelOutputHead(nn.Module):\n",
        "    def __init__(self, original_lm_head, num_parallel_layers, hidden_dim, output_dim, weights_data, base_model_name=\"lmsys/vicuna-7b-v1.3\"):\n",
        "        super().__init__()\n",
        "        self.output = original_lm_head  # Keep the original layer\n",
        "        self.parallel_layers = nn.ModuleList(\n",
        "            [medusahead(hidden_dim, output_dim, weights_data) for _ in range(num_parallel_layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass through the original lm_head\n",
        "        original_output = self.output(x)\n",
        "\n",
        "        # Pass through parallel layers\n",
        "        parallel_outputs = [layer(x) for layer in self.parallel_layers]\n",
        "\n",
        "\n",
        "        # Final output transformation\n",
        "        return {\n",
        "            \"logits\": original_output.logits,  # Original language modeling logits\n",
        "            \"medusa_output\": parallel_outputs,  # Output from the new head\n",
        "                }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjAPP-x1__aQ"
      },
      "outputs": [],
      "source": [
        "# Define dimensions and number of parallel layers\n",
        "hidden_dim   = model.config.hidden_size  # 4096 for LLaMA\n",
        "output_dim   = model.config.vocab_size  # 32000 for LLaMA\n",
        "weights_data = model.lm_head.weight.data\n",
        "num_parallel_layers = 3  # Number of parallel linear layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctw46jdh8wMH"
      },
      "outputs": [],
      "source": [
        "# Create the parallel output head\n",
        "parallel_lm_head = ParallelOutputHead(\n",
        "    original_lm_head=model.lm_head,\n",
        "    num_parallel_layers=num_parallel_layers,\n",
        "    hidden_dim=hidden_dim,\n",
        "    output_dim=output_dim,\n",
        "    weights_data=weights_data\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0iRiFRu_kz5"
      },
      "outputs": [],
      "source": [
        "# Replace the model's lm_head\n",
        "model.lm_head = parallel_lm_head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwJWc8EpAdxL",
        "outputId": "1200bd5a-f4f3-49a1-b887-1d0c7ad7dfd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaSdpaAttention(\n",
            "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
            "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
            "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((4096,), eps=1e-06)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): ParallelOutputHead(\n",
            "    (output): Linear(in_features=4096, out_features=32000, bias=False)\n",
            "    (parallel_layers): ModuleList(\n",
            "      (0-2): 3 x medusahead(\n",
            "        (linear1): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "        (linear2): Linear(in_features=4096, out_features=32000, bias=True)\n",
            "        (silu): SiLU()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Verify the modified model architecture\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DpZuIQ774zn",
        "outputId": "e60dd732-a1f5-4138-ee7b-2e5323fb1e93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keys in the model's parameters:\n",
            "model.embed_tokens.weight\n",
            "model.layers.0.self_attn.q_proj.weight\n",
            "model.layers.0.self_attn.k_proj.weight\n",
            "model.layers.0.self_attn.v_proj.weight\n",
            "model.layers.0.self_attn.o_proj.weight\n",
            "model.layers.0.mlp.gate_proj.weight\n",
            "model.layers.0.mlp.up_proj.weight\n",
            "model.layers.0.mlp.down_proj.weight\n",
            "model.layers.0.input_layernorm.weight\n",
            "model.layers.0.post_attention_layernorm.weight\n",
            "model.layers.1.self_attn.q_proj.weight\n",
            "model.layers.1.self_attn.k_proj.weight\n",
            "model.layers.1.self_attn.v_proj.weight\n",
            "model.layers.1.self_attn.o_proj.weight\n",
            "model.layers.1.mlp.gate_proj.weight\n",
            "model.layers.1.mlp.up_proj.weight\n",
            "model.layers.1.mlp.down_proj.weight\n",
            "model.layers.1.input_layernorm.weight\n",
            "model.layers.1.post_attention_layernorm.weight\n",
            "model.layers.2.self_attn.q_proj.weight\n",
            "model.layers.2.self_attn.k_proj.weight\n",
            "model.layers.2.self_attn.v_proj.weight\n",
            "model.layers.2.self_attn.o_proj.weight\n",
            "model.layers.2.mlp.gate_proj.weight\n",
            "model.layers.2.mlp.up_proj.weight\n",
            "model.layers.2.mlp.down_proj.weight\n",
            "model.layers.2.input_layernorm.weight\n",
            "model.layers.2.post_attention_layernorm.weight\n",
            "model.layers.3.self_attn.q_proj.weight\n",
            "model.layers.3.self_attn.k_proj.weight\n",
            "model.layers.3.self_attn.v_proj.weight\n",
            "model.layers.3.self_attn.o_proj.weight\n",
            "model.layers.3.mlp.gate_proj.weight\n",
            "model.layers.3.mlp.up_proj.weight\n",
            "model.layers.3.mlp.down_proj.weight\n",
            "model.layers.3.input_layernorm.weight\n",
            "model.layers.3.post_attention_layernorm.weight\n",
            "model.layers.4.self_attn.q_proj.weight\n",
            "model.layers.4.self_attn.k_proj.weight\n",
            "model.layers.4.self_attn.v_proj.weight\n",
            "model.layers.4.self_attn.o_proj.weight\n",
            "model.layers.4.mlp.gate_proj.weight\n",
            "model.layers.4.mlp.up_proj.weight\n",
            "model.layers.4.mlp.down_proj.weight\n",
            "model.layers.4.input_layernorm.weight\n",
            "model.layers.4.post_attention_layernorm.weight\n",
            "model.layers.5.self_attn.q_proj.weight\n",
            "model.layers.5.self_attn.k_proj.weight\n",
            "model.layers.5.self_attn.v_proj.weight\n",
            "model.layers.5.self_attn.o_proj.weight\n",
            "model.layers.5.mlp.gate_proj.weight\n",
            "model.layers.5.mlp.up_proj.weight\n",
            "model.layers.5.mlp.down_proj.weight\n",
            "model.layers.5.input_layernorm.weight\n",
            "model.layers.5.post_attention_layernorm.weight\n",
            "model.layers.6.self_attn.q_proj.weight\n",
            "model.layers.6.self_attn.k_proj.weight\n",
            "model.layers.6.self_attn.v_proj.weight\n",
            "model.layers.6.self_attn.o_proj.weight\n",
            "model.layers.6.mlp.gate_proj.weight\n",
            "model.layers.6.mlp.up_proj.weight\n",
            "model.layers.6.mlp.down_proj.weight\n",
            "model.layers.6.input_layernorm.weight\n",
            "model.layers.6.post_attention_layernorm.weight\n",
            "model.layers.7.self_attn.q_proj.weight\n",
            "model.layers.7.self_attn.k_proj.weight\n",
            "model.layers.7.self_attn.v_proj.weight\n",
            "model.layers.7.self_attn.o_proj.weight\n",
            "model.layers.7.mlp.gate_proj.weight\n",
            "model.layers.7.mlp.up_proj.weight\n",
            "model.layers.7.mlp.down_proj.weight\n",
            "model.layers.7.input_layernorm.weight\n",
            "model.layers.7.post_attention_layernorm.weight\n",
            "model.layers.8.self_attn.q_proj.weight\n",
            "model.layers.8.self_attn.k_proj.weight\n",
            "model.layers.8.self_attn.v_proj.weight\n",
            "model.layers.8.self_attn.o_proj.weight\n",
            "model.layers.8.mlp.gate_proj.weight\n",
            "model.layers.8.mlp.up_proj.weight\n",
            "model.layers.8.mlp.down_proj.weight\n",
            "model.layers.8.input_layernorm.weight\n",
            "model.layers.8.post_attention_layernorm.weight\n",
            "model.layers.9.self_attn.q_proj.weight\n",
            "model.layers.9.self_attn.k_proj.weight\n",
            "model.layers.9.self_attn.v_proj.weight\n",
            "model.layers.9.self_attn.o_proj.weight\n",
            "model.layers.9.mlp.gate_proj.weight\n",
            "model.layers.9.mlp.up_proj.weight\n",
            "model.layers.9.mlp.down_proj.weight\n",
            "model.layers.9.input_layernorm.weight\n",
            "model.layers.9.post_attention_layernorm.weight\n",
            "model.layers.10.self_attn.q_proj.weight\n",
            "model.layers.10.self_attn.k_proj.weight\n",
            "model.layers.10.self_attn.v_proj.weight\n",
            "model.layers.10.self_attn.o_proj.weight\n",
            "model.layers.10.mlp.gate_proj.weight\n",
            "model.layers.10.mlp.up_proj.weight\n",
            "model.layers.10.mlp.down_proj.weight\n",
            "model.layers.10.input_layernorm.weight\n",
            "model.layers.10.post_attention_layernorm.weight\n",
            "model.layers.11.self_attn.q_proj.weight\n",
            "model.layers.11.self_attn.k_proj.weight\n",
            "model.layers.11.self_attn.v_proj.weight\n",
            "model.layers.11.self_attn.o_proj.weight\n",
            "model.layers.11.mlp.gate_proj.weight\n",
            "model.layers.11.mlp.up_proj.weight\n",
            "model.layers.11.mlp.down_proj.weight\n",
            "model.layers.11.input_layernorm.weight\n",
            "model.layers.11.post_attention_layernorm.weight\n",
            "model.layers.12.self_attn.q_proj.weight\n",
            "model.layers.12.self_attn.k_proj.weight\n",
            "model.layers.12.self_attn.v_proj.weight\n",
            "model.layers.12.self_attn.o_proj.weight\n",
            "model.layers.12.mlp.gate_proj.weight\n",
            "model.layers.12.mlp.up_proj.weight\n",
            "model.layers.12.mlp.down_proj.weight\n",
            "model.layers.12.input_layernorm.weight\n",
            "model.layers.12.post_attention_layernorm.weight\n",
            "model.layers.13.self_attn.q_proj.weight\n",
            "model.layers.13.self_attn.k_proj.weight\n",
            "model.layers.13.self_attn.v_proj.weight\n",
            "model.layers.13.self_attn.o_proj.weight\n",
            "model.layers.13.mlp.gate_proj.weight\n",
            "model.layers.13.mlp.up_proj.weight\n",
            "model.layers.13.mlp.down_proj.weight\n",
            "model.layers.13.input_layernorm.weight\n",
            "model.layers.13.post_attention_layernorm.weight\n",
            "model.layers.14.self_attn.q_proj.weight\n",
            "model.layers.14.self_attn.k_proj.weight\n",
            "model.layers.14.self_attn.v_proj.weight\n",
            "model.layers.14.self_attn.o_proj.weight\n",
            "model.layers.14.mlp.gate_proj.weight\n",
            "model.layers.14.mlp.up_proj.weight\n",
            "model.layers.14.mlp.down_proj.weight\n",
            "model.layers.14.input_layernorm.weight\n",
            "model.layers.14.post_attention_layernorm.weight\n",
            "model.layers.15.self_attn.q_proj.weight\n",
            "model.layers.15.self_attn.k_proj.weight\n",
            "model.layers.15.self_attn.v_proj.weight\n",
            "model.layers.15.self_attn.o_proj.weight\n",
            "model.layers.15.mlp.gate_proj.weight\n",
            "model.layers.15.mlp.up_proj.weight\n",
            "model.layers.15.mlp.down_proj.weight\n",
            "model.layers.15.input_layernorm.weight\n",
            "model.layers.15.post_attention_layernorm.weight\n",
            "model.layers.16.self_attn.q_proj.weight\n",
            "model.layers.16.self_attn.k_proj.weight\n",
            "model.layers.16.self_attn.v_proj.weight\n",
            "model.layers.16.self_attn.o_proj.weight\n",
            "model.layers.16.mlp.gate_proj.weight\n",
            "model.layers.16.mlp.up_proj.weight\n",
            "model.layers.16.mlp.down_proj.weight\n",
            "model.layers.16.input_layernorm.weight\n",
            "model.layers.16.post_attention_layernorm.weight\n",
            "model.layers.17.self_attn.q_proj.weight\n",
            "model.layers.17.self_attn.k_proj.weight\n",
            "model.layers.17.self_attn.v_proj.weight\n",
            "model.layers.17.self_attn.o_proj.weight\n",
            "model.layers.17.mlp.gate_proj.weight\n",
            "model.layers.17.mlp.up_proj.weight\n",
            "model.layers.17.mlp.down_proj.weight\n",
            "model.layers.17.input_layernorm.weight\n",
            "model.layers.17.post_attention_layernorm.weight\n",
            "model.layers.18.self_attn.q_proj.weight\n",
            "model.layers.18.self_attn.k_proj.weight\n",
            "model.layers.18.self_attn.v_proj.weight\n",
            "model.layers.18.self_attn.o_proj.weight\n",
            "model.layers.18.mlp.gate_proj.weight\n",
            "model.layers.18.mlp.up_proj.weight\n",
            "model.layers.18.mlp.down_proj.weight\n",
            "model.layers.18.input_layernorm.weight\n",
            "model.layers.18.post_attention_layernorm.weight\n",
            "model.layers.19.self_attn.q_proj.weight\n",
            "model.layers.19.self_attn.k_proj.weight\n",
            "model.layers.19.self_attn.v_proj.weight\n",
            "model.layers.19.self_attn.o_proj.weight\n",
            "model.layers.19.mlp.gate_proj.weight\n",
            "model.layers.19.mlp.up_proj.weight\n",
            "model.layers.19.mlp.down_proj.weight\n",
            "model.layers.19.input_layernorm.weight\n",
            "model.layers.19.post_attention_layernorm.weight\n",
            "model.layers.20.self_attn.q_proj.weight\n",
            "model.layers.20.self_attn.k_proj.weight\n",
            "model.layers.20.self_attn.v_proj.weight\n",
            "model.layers.20.self_attn.o_proj.weight\n",
            "model.layers.20.mlp.gate_proj.weight\n",
            "model.layers.20.mlp.up_proj.weight\n",
            "model.layers.20.mlp.down_proj.weight\n",
            "model.layers.20.input_layernorm.weight\n",
            "model.layers.20.post_attention_layernorm.weight\n",
            "model.layers.21.self_attn.q_proj.weight\n",
            "model.layers.21.self_attn.k_proj.weight\n",
            "model.layers.21.self_attn.v_proj.weight\n",
            "model.layers.21.self_attn.o_proj.weight\n",
            "model.layers.21.mlp.gate_proj.weight\n",
            "model.layers.21.mlp.up_proj.weight\n",
            "model.layers.21.mlp.down_proj.weight\n",
            "model.layers.21.input_layernorm.weight\n",
            "model.layers.21.post_attention_layernorm.weight\n",
            "model.layers.22.self_attn.q_proj.weight\n",
            "model.layers.22.self_attn.k_proj.weight\n",
            "model.layers.22.self_attn.v_proj.weight\n",
            "model.layers.22.self_attn.o_proj.weight\n",
            "model.layers.22.mlp.gate_proj.weight\n",
            "model.layers.22.mlp.up_proj.weight\n",
            "model.layers.22.mlp.down_proj.weight\n",
            "model.layers.22.input_layernorm.weight\n",
            "model.layers.22.post_attention_layernorm.weight\n",
            "model.layers.23.self_attn.q_proj.weight\n",
            "model.layers.23.self_attn.k_proj.weight\n",
            "model.layers.23.self_attn.v_proj.weight\n",
            "model.layers.23.self_attn.o_proj.weight\n",
            "model.layers.23.mlp.gate_proj.weight\n",
            "model.layers.23.mlp.up_proj.weight\n",
            "model.layers.23.mlp.down_proj.weight\n",
            "model.layers.23.input_layernorm.weight\n",
            "model.layers.23.post_attention_layernorm.weight\n",
            "model.layers.24.self_attn.q_proj.weight\n",
            "model.layers.24.self_attn.k_proj.weight\n",
            "model.layers.24.self_attn.v_proj.weight\n",
            "model.layers.24.self_attn.o_proj.weight\n",
            "model.layers.24.mlp.gate_proj.weight\n",
            "model.layers.24.mlp.up_proj.weight\n",
            "model.layers.24.mlp.down_proj.weight\n",
            "model.layers.24.input_layernorm.weight\n",
            "model.layers.24.post_attention_layernorm.weight\n",
            "model.layers.25.self_attn.q_proj.weight\n",
            "model.layers.25.self_attn.k_proj.weight\n",
            "model.layers.25.self_attn.v_proj.weight\n",
            "model.layers.25.self_attn.o_proj.weight\n",
            "model.layers.25.mlp.gate_proj.weight\n",
            "model.layers.25.mlp.up_proj.weight\n",
            "model.layers.25.mlp.down_proj.weight\n",
            "model.layers.25.input_layernorm.weight\n",
            "model.layers.25.post_attention_layernorm.weight\n",
            "model.layers.26.self_attn.q_proj.weight\n",
            "model.layers.26.self_attn.k_proj.weight\n",
            "model.layers.26.self_attn.v_proj.weight\n",
            "model.layers.26.self_attn.o_proj.weight\n",
            "model.layers.26.mlp.gate_proj.weight\n",
            "model.layers.26.mlp.up_proj.weight\n",
            "model.layers.26.mlp.down_proj.weight\n",
            "model.layers.26.input_layernorm.weight\n",
            "model.layers.26.post_attention_layernorm.weight\n",
            "model.layers.27.self_attn.q_proj.weight\n",
            "model.layers.27.self_attn.k_proj.weight\n",
            "model.layers.27.self_attn.v_proj.weight\n",
            "model.layers.27.self_attn.o_proj.weight\n",
            "model.layers.27.mlp.gate_proj.weight\n",
            "model.layers.27.mlp.up_proj.weight\n",
            "model.layers.27.mlp.down_proj.weight\n",
            "model.layers.27.input_layernorm.weight\n",
            "model.layers.27.post_attention_layernorm.weight\n",
            "model.layers.28.self_attn.q_proj.weight\n",
            "model.layers.28.self_attn.k_proj.weight\n",
            "model.layers.28.self_attn.v_proj.weight\n",
            "model.layers.28.self_attn.o_proj.weight\n",
            "model.layers.28.mlp.gate_proj.weight\n",
            "model.layers.28.mlp.up_proj.weight\n",
            "model.layers.28.mlp.down_proj.weight\n",
            "model.layers.28.input_layernorm.weight\n",
            "model.layers.28.post_attention_layernorm.weight\n",
            "model.layers.29.self_attn.q_proj.weight\n",
            "model.layers.29.self_attn.k_proj.weight\n",
            "model.layers.29.self_attn.v_proj.weight\n",
            "model.layers.29.self_attn.o_proj.weight\n",
            "model.layers.29.mlp.gate_proj.weight\n",
            "model.layers.29.mlp.up_proj.weight\n",
            "model.layers.29.mlp.down_proj.weight\n",
            "model.layers.29.input_layernorm.weight\n",
            "model.layers.29.post_attention_layernorm.weight\n",
            "model.layers.30.self_attn.q_proj.weight\n",
            "model.layers.30.self_attn.k_proj.weight\n",
            "model.layers.30.self_attn.v_proj.weight\n",
            "model.layers.30.self_attn.o_proj.weight\n",
            "model.layers.30.mlp.gate_proj.weight\n",
            "model.layers.30.mlp.up_proj.weight\n",
            "model.layers.30.mlp.down_proj.weight\n",
            "model.layers.30.input_layernorm.weight\n",
            "model.layers.30.post_attention_layernorm.weight\n",
            "model.layers.31.self_attn.q_proj.weight\n",
            "model.layers.31.self_attn.k_proj.weight\n",
            "model.layers.31.self_attn.v_proj.weight\n",
            "model.layers.31.self_attn.o_proj.weight\n",
            "model.layers.31.mlp.gate_proj.weight\n",
            "model.layers.31.mlp.up_proj.weight\n",
            "model.layers.31.mlp.down_proj.weight\n",
            "model.layers.31.input_layernorm.weight\n",
            "model.layers.31.post_attention_layernorm.weight\n",
            "model.norm.weight\n",
            "lm_head.output.weight\n",
            "lm_head.parallel_layers.0.linear1.weight\n",
            "lm_head.parallel_layers.0.linear1.bias\n",
            "lm_head.parallel_layers.0.linear2.weight\n",
            "lm_head.parallel_layers.0.linear2.bias\n",
            "lm_head.parallel_layers.1.linear1.weight\n",
            "lm_head.parallel_layers.1.linear1.bias\n",
            "lm_head.parallel_layers.1.linear2.weight\n",
            "lm_head.parallel_layers.1.linear2.bias\n",
            "lm_head.parallel_layers.2.linear1.weight\n",
            "lm_head.parallel_layers.2.linear1.bias\n",
            "lm_head.parallel_layers.2.linear2.weight\n",
            "lm_head.parallel_layers.2.linear2.bias\n"
          ]
        }
      ],
      "source": [
        "# Print all keys (parameters and their names)\n",
        "print(\"Keys in the model's parameters:\")\n",
        "for name, param in model.named_parameters():\n",
        "    print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBo2jQ5SAehc",
        "outputId": "7c3b4e53-2843-4364-cd4a-2e599edaf794"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('./modified_vicuna_parallel/tokenizer_config.json',\n",
              " './modified_vicuna_parallel/special_tokens_map.json',\n",
              " './modified_vicuna_parallel/tokenizer.model',\n",
              " './modified_vicuna_parallel/added_tokens.json',\n",
              " './modified_vicuna_parallel/tokenizer.json')"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained(\"./modified_vicuna_parallel\")\n",
        "tokenizer.save_pretrained(\"./modified_vicuna_parallel\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMNeEfFOFfap",
        "outputId": "c82c4271-3f44-4ff3-938c-bf92dcdafcd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:hf-to-gguf:Loading model: modified_vicuna_parallel\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00003.safetensors'\n",
            "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> F16, shape = {4096, 32000}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00003.safetensors'\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00003.safetensors'\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/llama.cpp/convert_hf_to_gguf.py\", line 4436, in <module>\n",
            "    main()\n",
            "  File \"/content/llama.cpp/convert_hf_to_gguf.py\", line 4430, in main\n",
            "    model_instance.write()\n",
            "  File \"/content/llama.cpp/convert_hf_to_gguf.py\", line 434, in write\n",
            "    self.prepare_tensors()\n",
            "  File \"/content/llama.cpp/convert_hf_to_gguf.py\", line 1657, in prepare_tensors\n",
            "    super().prepare_tensors()\n",
            "  File \"/content/llama.cpp/convert_hf_to_gguf.py\", line 298, in prepare_tensors\n",
            "    for new_name, data_torch in (self.modify_tensors(data_torch, name, bid)):\n",
            "  File \"/content/llama.cpp/convert_hf_to_gguf.py\", line 1625, in modify_tensors\n",
            "    return [(self.map_tensor_name(name), data_torch)]\n",
            "  File \"/content/llama.cpp/convert_hf_to_gguf.py\", line 214, in map_tensor_name\n",
            "    raise ValueError(f\"Can not map tensor {name!r}\")\n",
            "ValueError: Can not map tensor 'lm_head.output.weight'\n"
          ]
        }
      ],
      "source": [
        "!python3 convert_hf_to_gguf.py /content/llama.cpp/modified_vicuna_parallel --model-name vicuna-7b-v1.3-medusa --outfile /content/llama.cpp/quantized_models/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTDKabdmFsva"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0232f8cfad6a4751a89b83e0788e2372": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "034326c2cc954e0695be3d3bf2a54938": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05916bcd5e124ca1ac77a9656bbcbdbf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0612c8ea7cd54dab8d440cd5b55e1bb2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "062c4b9e0b0e44b7b641a56f0a25511f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_237f220a84674a5098c59cbf37ec8051",
            "max": 727,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_48836409a0cc4b9a98b04e50fd681b4a",
            "value": 727
          }
        },
        "06b94e77ea3c4e06975de15051bcc0d7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0810e015fded44d8a658e758f58660c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27598e42d9c5435a85e60a5a09e4600c",
            "placeholder": "",
            "style": "IPY_MODEL_78b0c915d4f9401d9137e6009570432d",
            "value": "727/727[00:00&lt;00:00,21.9kB/s]"
          }
        },
        "090c5fcfd66c441c87cd7c87e7fca1b8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b20f818873847f994226ada22639acd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41968ef504fe4ffdabc08f90df8c1b77",
            "placeholder": "",
            "style": "IPY_MODEL_8350886080994024b04008e14e8da30b",
            "value": "2/2[00:01&lt;00:00,2.09it/s]"
          }
        },
        "0caf8032f77e40c09446a9c286209984": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_308126d94173419a908df9a07547583f",
            "placeholder": "",
            "style": "IPY_MODEL_576f8640862e4d11bb70898c7ed48511",
            "value": "435/435[00:00&lt;00:00,16.2kB/s]"
          }
        },
        "0ce6a77ac385450e868b6d2acb49fe12": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0ec43020a81d44a2ae48b9a33f18cb4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65dfaa007eea4f27ae4880a00135fee6",
            "placeholder": "",
            "style": "IPY_MODEL_1ee7836b82aa4f3788eaa62d9d334312",
            "value": "pytorch_model-00001-of-00002.bin:100%"
          }
        },
        "120eb0ec30df4ae3a5907ab5b903cf36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1a29a6452d54d5e9af8afd2bdd1c8ec",
            "placeholder": "",
            "style": "IPY_MODEL_1b5125d083714f9b87d8835490261cd5",
            "value": "26.8k/26.8k[00:00&lt;00:00,1.72MB/s]"
          }
        },
        "145218bc4cb248a2ba7eb74d2cb653aa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15908b6d39bd46d69920ad0ba0ed3d80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1ab9d9ec535145779a1c7227ddeba0fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b5125d083714f9b87d8835490261cd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1dd69b66c3b94bf68baca39342849915": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d21d2300ab7c4967a22ba945d1408f3e",
            "placeholder": "",
            "style": "IPY_MODEL_8e6619d407914d5e980ea78e8ccf140b",
            "value": "Downloadingshards:100%"
          }
        },
        "1ee7836b82aa4f3788eaa62d9d334312": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f6b1aa67142475ea4789da20ee9973c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1fbe8adced5f453ba34097769a7390c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8ee854311844698b0f00a08c421af78",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9de84eba817446c9ac3a5a87d3770c11",
            "value": 2
          }
        },
        "22bf260c22414b6f852f611b9ecdcbc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "22e950a7254f49c79ca370aab8a55952": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "237f220a84674a5098c59cbf37ec8051": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2649c4029b764eb6b2c9fac6216e0b00": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "27598e42d9c5435a85e60a5a09e4600c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27601138e49e42258cf46d430e8f62ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28dc1388ad6541088b5619dc4e0c42c1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b08a26f935443778276b96c3139f767": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b878d4cfa514cba919a816479ceccca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c1a290037b746d3bf0918277f2d747a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b9a589eab0514cbdb8a559df6ca959a9",
              "IPY_MODEL_7ac586ca67da4d7eb4278bd7ef2f4178",
              "IPY_MODEL_a4ba36d6eec34ffa8a1a712d5945188a"
            ],
            "layout": "IPY_MODEL_f292cfc21b2047d2b728d4a815f7d29b"
          }
        },
        "308126d94173419a908df9a07547583f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "333cb64bdc64429cafa80b8f4a8f0f04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3605c399a39342ae9268279e26cac924": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a195136c21c4f1cb767328ecc5fe6f5",
            "placeholder": "",
            "style": "IPY_MODEL_6bbcc022b1fe486491d89cec64a7dff5",
            "value": "2/2[01:10&lt;00:00,32.19s/it]"
          }
        },
        "381c7d3eeac74234823ab424003e1b7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bf9068d17ce4b198c26f740afda9b77",
            "placeholder": "",
            "style": "IPY_MODEL_feea812abca34574991da2ab9f48b0f1",
            "value": "pytorch_model.bin.index.json:100%"
          }
        },
        "3adc6c676f1744659b0cd12df1912bbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b62392368bd4c2dbb374e3e67d4cd21": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d4efc726a9849398a359f5a50b56939": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4774e70eab0a4afeb1d3374238e7443d",
              "IPY_MODEL_a33179dd44d94803ae0ca4e42cb7c27f",
              "IPY_MODEL_4068dd95d0434c83bc90f19622a9a250"
            ],
            "layout": "IPY_MODEL_57c2ed7f03c4471fbb09d7de54d6e1b0"
          }
        },
        "4068dd95d0434c83bc90f19622a9a250": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_779cdc03441a492f9b8e5ac23edcc556",
            "placeholder": "",
            "style": "IPY_MODEL_f7c522bd85ab43b39fb81f122c616f10",
            "value": "3.50G/3.50G[00:16&lt;00:00,247MB/s]"
          }
        },
        "40b050919c2748f4b4e99e185130d475": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d7f19988e354e0ea5452e160ee168de",
            "placeholder": "",
            "style": "IPY_MODEL_2b08a26f935443778276b96c3139f767",
            "value": "500k/500k[00:00&lt;00:00,4.31MB/s]"
          }
        },
        "41968ef504fe4ffdabc08f90df8c1b77": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "464024664de140c882fb81e239650848": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4774e70eab0a4afeb1d3374238e7443d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2fcbae0f9c44b71bb7bd90bc68fa236",
            "placeholder": "",
            "style": "IPY_MODEL_dc5557a5e060491093ab28ec5a4e798e",
            "value": "pytorch_model-00002-of-00002.bin:100%"
          }
        },
        "48836409a0cc4b9a98b04e50fd681b4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4bf9068d17ce4b198c26f740afda9b77": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d7f19988e354e0ea5452e160ee168de": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "515d1e5f1adf44c8a8c8f9b75e218a44": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "549b574902e64d46a214bbc9498e987b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "562160f2776642d5a7f77888c5b2ca57": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "576f8640862e4d11bb70898c7ed48511": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57c2ed7f03c4471fbb09d7de54d6e1b0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a0e293ae81e4cc396cae305e03da53d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61b021d7287c4b18997d345c1f6f21d4",
            "placeholder": "",
            "style": "IPY_MODEL_8c5e75898c8e47789244733bcd542184",
            "value": "tokenizer_config.json:100%"
          }
        },
        "5ee928322e3940908e31e40893ebd669": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_67221f3660bd4b418deb0cbcfe224dd1",
              "IPY_MODEL_61a6b4e762b3446c81362b8f5b64f087",
              "IPY_MODEL_40b050919c2748f4b4e99e185130d475"
            ],
            "layout": "IPY_MODEL_1ab9d9ec535145779a1c7227ddeba0fe"
          }
        },
        "617da13d7cbf48578c92a3713bec6729": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa8281c267de4d61996e28163729279d",
              "IPY_MODEL_c6057f63294349ddb02cbd529b03dafd",
              "IPY_MODEL_cad96e9bd8934f53bfa889951c9bc424"
            ],
            "layout": "IPY_MODEL_515d1e5f1adf44c8a8c8f9b75e218a44"
          }
        },
        "61a6b4e762b3446c81362b8f5b64f087": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b878d4cfa514cba919a816479ceccca",
            "max": 499723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ce6a77ac385450e868b6d2acb49fe12",
            "value": 499723
          }
        },
        "61b021d7287c4b18997d345c1f6f21d4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6528386896dc4f9bb0b079cb54349f2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1dd69b66c3b94bf68baca39342849915",
              "IPY_MODEL_1fbe8adced5f453ba34097769a7390c4",
              "IPY_MODEL_3605c399a39342ae9268279e26cac924"
            ],
            "layout": "IPY_MODEL_7e4ea410128d49fca59e1a6bbcd72713"
          }
        },
        "65dfaa007eea4f27ae4880a00135fee6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6624bc1fec094e90b1c2dc2bda4953ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "67221f3660bd4b418deb0cbcfe224dd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eee423fb7b5c49d68a83190e080300ed",
            "placeholder": "",
            "style": "IPY_MODEL_bf1f783678054990b126dd23904ce12b",
            "value": "tokenizer.model:100%"
          }
        },
        "6a195136c21c4f1cb767328ecc5fe6f5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bbcc022b1fe486491d89cec64a7dff5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ce835a154314fbe994f74c94622b07e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f096c7e4091448ca0af5d0ae4b2a960": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "779cdc03441a492f9b8e5ac23edcc556": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77d165540dad40bfb9b4cb9dbb39d0c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28dc1388ad6541088b5619dc4e0c42c1",
            "placeholder": "",
            "style": "IPY_MODEL_562160f2776642d5a7f77888c5b2ca57",
            "value": "Loadingcheckpointshards:100%"
          }
        },
        "78b0c915d4f9401d9137e6009570432d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ac586ca67da4d7eb4278bd7ef2f4178": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad1d0b8cf9db4f429c0a0dc94986ec10",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_15908b6d39bd46d69920ad0ba0ed3d80",
            "value": 2
          }
        },
        "7df7616123e6410283f641b713014beb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_77d165540dad40bfb9b4cb9dbb39d0c5",
              "IPY_MODEL_83c145bfb3be418d81297b7c662f291d",
              "IPY_MODEL_0b20f818873847f994226ada22639acd"
            ],
            "layout": "IPY_MODEL_9fa3b81e61b04a82895d045693c73b58"
          }
        },
        "7e4ea410128d49fca59e1a6bbcd72713": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8350886080994024b04008e14e8da30b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83c145bfb3be418d81297b7c662f291d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06b94e77ea3c4e06975de15051bcc0d7",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1f6b1aa67142475ea4789da20ee9973c",
            "value": 2
          }
        },
        "868194f48b7b4cbc9ee3e7eaef7cc147": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87a29ca98b664d678a989c795f86f895": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8838563bbeb94aee94ee61354d4b1250": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89e03b7e3705408ea157b8d4a8d4304d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a96ac47828b54fa4a1d0e99782fe4475",
            "max": 132,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_464024664de140c882fb81e239650848",
            "value": 132
          }
        },
        "8c5e75898c8e47789244733bcd542184": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e6619d407914d5e980ea78e8ccf140b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9833fa4ca8364dada389fd8ef3928b5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9843dfa7b5e7490984f2651ef5e75089": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9a407a9a6ab743c1b7c53042868fbb2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0612c8ea7cd54dab8d440cd5b55e1bb2",
            "placeholder": "",
            "style": "IPY_MODEL_87a29ca98b664d678a989c795f86f895",
            "value": "generation_config.json:100%"
          }
        },
        "9de5a1c92448493f94dfa3297096b5ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af35a094d69e45c1ba8018e8247408fb",
            "max": 435,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2649c4029b764eb6b2c9fac6216e0b00",
            "value": 435
          }
        },
        "9de84eba817446c9ac3a5a87d3770c11": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9fa3b81e61b04a82895d045693c73b58": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a040492de74d432e922ef534da4dcf73": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a10c93d91da741b3a17ef3a354e75e44": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a33179dd44d94803ae0ca4e42cb7c27f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8838563bbeb94aee94ee61354d4b1250",
            "max": 3500315539,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9833fa4ca8364dada389fd8ef3928b5b",
            "value": 3500315539
          }
        },
        "a4ba36d6eec34ffa8a1a712d5945188a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddbcbb97d4804b3b84db6641b8257b1a",
            "placeholder": "",
            "style": "IPY_MODEL_3b62392368bd4c2dbb374e3e67d4cd21",
            "value": "2/2[00:01&lt;00:00,1.48it/s]"
          }
        },
        "a64e50d2fb844e9d834c4445c3e8628f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a917b4272f164a608940bc40a1fa6c5e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a96ac47828b54fa4a1d0e99782fe4475": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa8281c267de4d61996e28163729279d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27601138e49e42258cf46d430e8f62ba",
            "placeholder": "",
            "style": "IPY_MODEL_3adc6c676f1744659b0cd12df1912bbb",
            "value": "config.json:100%"
          }
        },
        "ad1d0b8cf9db4f429c0a0dc94986ec10": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af35a094d69e45c1ba8018e8247408fb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af5975bc529d4fc2a3b653dd163675a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bf3a9ca4adeb46c8ba71f14566be638f",
              "IPY_MODEL_9de5a1c92448493f94dfa3297096b5ee",
              "IPY_MODEL_0caf8032f77e40c09446a9c286209984"
            ],
            "layout": "IPY_MODEL_145218bc4cb248a2ba7eb74d2cb653aa"
          }
        },
        "b8523549bc2f477eb0caa125d5eec9c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9a589eab0514cbdb8a559df6ca959a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_034326c2cc954e0695be3d3bf2a54938",
            "placeholder": "",
            "style": "IPY_MODEL_b8523549bc2f477eb0caa125d5eec9c4",
            "value": "Loadingcheckpointshards:100%"
          }
        },
        "ba9d9445939549edad0b966b0134a2eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5a0e293ae81e4cc396cae305e03da53d",
              "IPY_MODEL_062c4b9e0b0e44b7b641a56f0a25511f",
              "IPY_MODEL_0810e015fded44d8a658e758f58660c5"
            ],
            "layout": "IPY_MODEL_e67ef75d4db94903925369798b31ef19"
          }
        },
        "be393ebf21204cba8984d8440dd2c278": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0ec43020a81d44a2ae48b9a33f18cb4d",
              "IPY_MODEL_d70befb903544a8595d518453f90f37e",
              "IPY_MODEL_fff7d5783fcf452e9c3d52340eda7837"
            ],
            "layout": "IPY_MODEL_05916bcd5e124ca1ac77a9656bbcbdbf"
          }
        },
        "bf1f783678054990b126dd23904ce12b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf3a9ca4adeb46c8ba71f14566be638f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_090c5fcfd66c441c87cd7c87e7fca1b8",
            "placeholder": "",
            "style": "IPY_MODEL_549b574902e64d46a214bbc9498e987b",
            "value": "special_tokens_map.json:100%"
          }
        },
        "c0f0ad400f9d4593b3569d3498e650ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f096c7e4091448ca0af5d0ae4b2a960",
            "placeholder": "",
            "style": "IPY_MODEL_22bf260c22414b6f852f611b9ecdcbc0",
            "value": "132/132[00:00&lt;00:00,7.95kB/s]"
          }
        },
        "c2fcbae0f9c44b71bb7bd90bc68fa236": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6057f63294349ddb02cbd529b03dafd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce12870d1132474baeeaf4b996e532b4",
            "max": 566,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6624bc1fec094e90b1c2dc2bda4953ce",
            "value": 566
          }
        },
        "cad96e9bd8934f53bfa889951c9bc424": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a917b4272f164a608940bc40a1fa6c5e",
            "placeholder": "",
            "style": "IPY_MODEL_333cb64bdc64429cafa80b8f4a8f0f04",
            "value": "566/566[00:00&lt;00:00,37.8kB/s]"
          }
        },
        "ce12870d1132474baeeaf4b996e532b4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1a29a6452d54d5e9af8afd2bdd1c8ec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d21d2300ab7c4967a22ba945d1408f3e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2ee8fc079a94e4dbff6e07a7c2adf60": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a64e50d2fb844e9d834c4445c3e8628f",
            "max": 26788,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a040492de74d432e922ef534da4dcf73",
            "value": 26788
          }
        },
        "d70befb903544a8595d518453f90f37e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_868194f48b7b4cbc9ee3e7eaef7cc147",
            "max": 9976634558,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9843dfa7b5e7490984f2651ef5e75089",
            "value": 9976634558
          }
        },
        "dc5557a5e060491093ab28ec5a4e798e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ddbcbb97d4804b3b84db6641b8257b1a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e637082f7a8a482383a4180277a69e35": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a407a9a6ab743c1b7c53042868fbb2f",
              "IPY_MODEL_89e03b7e3705408ea157b8d4a8d4304d",
              "IPY_MODEL_c0f0ad400f9d4593b3569d3498e650ad"
            ],
            "layout": "IPY_MODEL_0232f8cfad6a4751a89b83e0788e2372"
          }
        },
        "e67ef75d4db94903925369798b31ef19": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eee423fb7b5c49d68a83190e080300ed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f292cfc21b2047d2b728d4a815f7d29b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7c522bd85ab43b39fb81f122c616f10": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8ee854311844698b0f00a08c421af78": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "feddde061f6e4444bd480c0f4379b8bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_381c7d3eeac74234823ab424003e1b7c",
              "IPY_MODEL_d2ee8fc079a94e4dbff6e07a7c2adf60",
              "IPY_MODEL_120eb0ec30df4ae3a5907ab5b903cf36"
            ],
            "layout": "IPY_MODEL_a10c93d91da741b3a17ef3a354e75e44"
          }
        },
        "feea812abca34574991da2ab9f48b0f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fff7d5783fcf452e9c3d52340eda7837": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ce835a154314fbe994f74c94622b07e",
            "placeholder": "",
            "style": "IPY_MODEL_22e950a7254f49c79ca370aab8a55952",
            "value": "9.98G/9.98G[00:53&lt;00:00,195MB/s]"
          }
        },
        "d9adcfec44c0496bb0c0faef565191b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_078b5b0c593a409199f8cd2e29d7b873",
              "IPY_MODEL_ad4e9b9a4171462cb46ffe7eeb5e0e79",
              "IPY_MODEL_140f5e3ae1244f7a8feb04a190aad5eb"
            ],
            "layout": "IPY_MODEL_18ee58a9fbed479a968d2eeaf83d6807"
          }
        },
        "078b5b0c593a409199f8cd2e29d7b873": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9cd2d3d705a48378c44e70cfaa6d824",
            "placeholder": "",
            "style": "IPY_MODEL_f5f4814728ac46779cec4ada224fdb28",
            "value": "Loadingcheckpointshards:100%"
          }
        },
        "ad4e9b9a4171462cb46ffe7eeb5e0e79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae975cf9f0954c43baa477d9957e65fa",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_88cb0b3a67d24c5686646f384a54bd1b",
            "value": 2
          }
        },
        "140f5e3ae1244f7a8feb04a190aad5eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db5a75a4f0f043c3b041272bc6c14a56",
            "placeholder": "",
            "style": "IPY_MODEL_a60c1fd202cc4d7ba89c526b9a1c3a06",
            "value": "2/2[00:02&lt;00:00,1.17s/it]"
          }
        },
        "18ee58a9fbed479a968d2eeaf83d6807": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9cd2d3d705a48378c44e70cfaa6d824": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5f4814728ac46779cec4ada224fdb28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae975cf9f0954c43baa477d9957e65fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88cb0b3a67d24c5686646f384a54bd1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "db5a75a4f0f043c3b041272bc6c14a56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a60c1fd202cc4d7ba89c526b9a1c3a06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}